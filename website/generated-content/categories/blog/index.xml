<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Apache Beam – blog</title><link>/categories/blog/</link><description>Recent content in blog on Apache Beam</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 20 Aug 2023 09:00:00 -0400</lastBuildDate><atom:link href="/categories/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Apache Beam 2.50.0</title><link>/blog/beam-2.50.0/</link><pubDate>Sun, 20 Aug 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.50.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.50.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2500-2023-08-20">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.50.0, check out the &lt;a href="https://github.com/apache/beam/milestone/14">detailed release notes&lt;/a>.&lt;/p>
&lt;h1 id="2500---unreleased">[2.50.0] - Unreleased&lt;/h1>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Spark 3.2.2 is used as default version for Spark runner (&lt;a href="https://github.com/apache/beam/issues/23804">#23804&lt;/a>).&lt;/li>
&lt;li>The Go SDK has a new default local runner, called Prism (&lt;a href="https://github.com/apache/beam/issues/24789">#24789&lt;/a>).&lt;/li>
&lt;li>All Beam released container images are now &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/build-multi-arch-for-arm#what_is_a_multi-arch_image">multi-arch images&lt;/a> that support both x86 and ARM CPU architectures.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Java KafkaIO now supports picking up topics via topicPattern (&lt;a href="https://github.com/apache/beam/pull/26948">#26948&lt;/a>)&lt;/li>
&lt;li>Support for read from Cosmos DB Core SQL API (&lt;a href="https://github.com/apache/beam/issues/23604">#23604&lt;/a>)&lt;/li>
&lt;li>Upgraded to HBase 2.5.5 for HBaseIO. (Java) (&lt;a href="https://github.com/apache/beam/issues/19554">#27711&lt;/a>)&lt;/li>
&lt;li>Added support for GoogleAdsIO source (Java) (&lt;a href="https://github.com/apache/beam/pull/27681">#27681&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>The Go SDK now requires Go 1.20 to build. (&lt;a href="https://github.com/apache/beam/issues/27558">#27558&lt;/a>)&lt;/li>
&lt;li>The Go SDK has a new default local runner, Prism. (&lt;a href="https://github.com/apache/beam/issues/24789">#24789&lt;/a>).
&lt;ul>
&lt;li>Prism is a portable runner that executes each transform independantly, ensuring coders.&lt;/li>
&lt;li>At this point it supercedes the Go direct runner in functionality. The Go direct runner is now deprecated.&lt;/li>
&lt;li>See &lt;a href="https://github.com/apache/beam/blob/master/sdks/go/pkg/beam/runners/prism/README.md">https://github.com/apache/beam/blob/master/sdks/go/pkg/beam/runners/prism/README.md&lt;/a> for the goals and features of Prism.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Hugging Face Model Handler for RunInference added to Python SDK. (&lt;a href="https://github.com/apache/beam/pull/26632">#26632&lt;/a>)&lt;/li>
&lt;li>Hugging Face Pipelines support for RunInference added to Python SDK. (&lt;a href="https://github.com/apache/beam/pull/27399">#27399&lt;/a>)&lt;/li>
&lt;li>Vertex AI Model Handler for RunInference now supports private endpoints (&lt;a href="https://github.com/apache/beam/pull/27696">#27696&lt;/a>)&lt;/li>
&lt;li>MLTransform transform added with support for common ML pre/postprocessing operations (&lt;a href="https://github.com/apache/beam/pull/26795">#26795&lt;/a>)&lt;/li>
&lt;li>Upgraded the Kryo extension for the Java SDK to Kryo 5.5.0. This brings in bug fixes, performance improvements, and serialization of Java 14 records. (&lt;a href="https://github.com/apache/beam/issues/27635">#27635&lt;/a>)&lt;/li>
&lt;li>All Beam released container images are now &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/build-multi-arch-for-arm#what_is_a_multi-arch_image">multi-arch images&lt;/a> that support both x86 and ARM CPU architectures. (&lt;a href="https://github.com/apache/beam/issues/27674">#27674&lt;/a>). The multi-arch container images include:
&lt;ul>
&lt;li>All versions of Go, Python, Java and Typescript SDK containers.&lt;/li>
&lt;li>All versions of Flink job server containers.&lt;/li>
&lt;li>Java and Python expansion service containers.&lt;/li>
&lt;li>Transform service controller container.&lt;/li>
&lt;li>Spark3 job server container.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Added support for batched writes to AWS SQS for improved throughput (Java, AWS 2).(&lt;a href="https://github.com/apache/beam/issues/21429">#21429&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Python SDK: Legacy runner support removed from Dataflow, all pipelines must use runner v2.&lt;/li>
&lt;li>Python SDK: Dataflow Runner will no longer stage Beam SDK from PyPI in the &lt;code>--staging_location&lt;/code> at pipeline submission. Custom container images that are not based on Beam&amp;rsquo;s default image must include Apache Beam installation.(&lt;a href="https://github.com/apache/beam/issues/26996">#26996&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>The Go Direct Runner is now Deprecated. It remains available to reduce migration churn.
&lt;ul>
&lt;li>Tests can be set back to the direct runner by overriding TestMain: &lt;code>func TestMain(m *testing.M) { ptest.MainWithDefault(m, &amp;quot;direct&amp;quot;) }&lt;/code>&lt;/li>
&lt;li>It&amp;rsquo;s recommended to fix issues seen in tests using Prism, as they can also happen on any portable runner.&lt;/li>
&lt;li>Use the generic register package for your pipeline DoFns to ensure pipelines function on portable runners, like prism.&lt;/li>
&lt;li>Do not rely on closures or using package globals for DoFn configuration. They don&amp;rsquo;t function on portable runners.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed DirectRunner bug in Python SDK where GroupByKey gets empty PCollection and fails when pipeline option &lt;code>direct_num_workers!=1&lt;/code>.(&lt;a href="https://github.com/apache/beam/pull/27373">#27373&lt;/a>)&lt;/li>
&lt;li>Fixed BigQuery I/O bug when estimating size on queries that utilize row-level security (&lt;a href="https://github.com/apache/beam/pull/27474">#27474&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>TBD&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.40.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abacn&lt;/p>
&lt;p>acejune&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>ahmedabu98&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>al97&lt;/p>
&lt;p>Aleksandr Dudko&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Anton Shalkovich&lt;/p>
&lt;p>ArjunGHUB&lt;/p>
&lt;p>Bjorn Pedersen&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Brett Morgan&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Buqian Zheng&lt;/p>
&lt;p>Burke Davison&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>case-k&lt;/p>
&lt;p>Celeste Zeng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Connor Brett&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Damon Douglas&lt;/p>
&lt;p>Dan Hansen&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmytro Sadovnychyi&lt;/p>
&lt;p>Florent Biville&lt;/p>
&lt;p>Gabriel Lacroix&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Hong Liang Teoh&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>James Fricker&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeff Zhang&lt;/p>
&lt;p>Jing&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>jon esperanza&lt;/p>
&lt;p>Josef Šimánek&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Laksh&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>Mahmud Ridwan&lt;/p>
&lt;p>Manav Garg&lt;/p>
&lt;p>Marco Vela&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>mosche&lt;/p>
&lt;p>Peter Sobot&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>RyuSA&lt;/p>
&lt;p>Saba Sathya&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Steven Niemitz&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yichi Zhang&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zechen Jiang&lt;/p></description></item><item><title>Blog: Apache Beam 2.49.0</title><link>/blog/beam-2.49.0/</link><pubDate>Mon, 17 Jul 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.49.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.49.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2490-2023-07-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.49.0, check out the &lt;a href="https://github.com/apache/beam/milestone/13">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for Bigtable Change Streams added in Java &lt;code>BigtableIO.ReadChangeStream&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/27183">#27183&lt;/a>).&lt;/li>
&lt;li>Added Bigtable Read and Write cross-language transforms to Python SDK ((&lt;a href="https://github.com/apache/beam/issues/26593">#26593&lt;/a>), (&lt;a href="https://github.com/apache/beam/issues/27146">#27146&lt;/a>)).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Allow prebuilding large images when using &lt;code>--prebuild_sdk_container_engine=cloud_build&lt;/code>, like images depending on &lt;code>tensorflow&lt;/code> or &lt;code>torch&lt;/code> (&lt;a href="https://github.com/apache/beam/pull/27023">#27023&lt;/a>).&lt;/li>
&lt;li>Disabled &lt;code>pip&lt;/code> cache when installing packages on the workers. This reduces the size of prebuilt Python container images (&lt;a href="https://github.com/apache/beam/pull/27035">#27035&lt;/a>).&lt;/li>
&lt;li>Select dedicated avro datum reader and writer (Java) (&lt;a href="https://github.com/apache/beam/issues/18874">#18874&lt;/a>).&lt;/li>
&lt;li>Timer API for the Go SDK (Go) (&lt;a href="https://github.com/apache/beam/issues/22737">#22737&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Remove Python 3.7 support. (&lt;a href="https://github.com/apache/beam/issues/26447">#26447&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed KinesisIO &lt;code>NullPointerException&lt;/code> when a progress check is made before the reader is started (IO) (&lt;a href="https://github.com/apache/beam/issues/23868">#23868&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.49.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abzal Tuganbay&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alan Zhang&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Arwin Tio&lt;/p>
&lt;p>Bartosz Zablocki&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Burke Davison&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Charles Rothrock&lt;/p>
&lt;p>Chris Gavin&lt;/p>
&lt;p>Claire McGinty&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Daniel Dopierała&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>David Cavazos&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Gavin McDonald&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>James Fricker&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Jasper Van den Bossche&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>John Gill&lt;/p>
&lt;p>Joseph Crowley&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Katie Liu&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kyle Galloway&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Masato Nakamura&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Naireen Hussain&lt;/p>
&lt;p>Nathaniel Young&lt;/p>
&lt;p>Nelson Osacky&lt;/p>
&lt;p>Nick Li&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rouslan&lt;/p>
&lt;p>Saadat Su&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sanil Jain&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Smeet nagda&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>WuA&lt;/p>
&lt;p>XQ Hu&lt;/p>
&lt;p>Xianhua Liu&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zachary Houfek&lt;/p>
&lt;p>alexeyinkin&lt;/p>
&lt;p>bigduu&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>jonathan-lemos&lt;/p>
&lt;p>jubebo&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>ruslan-ikhsan&lt;/p>
&lt;p>sultanalieva-s&lt;/p>
&lt;p>vitaly.terentyev&lt;/p></description></item><item><title>Blog: Managing Beam dependencies in Java</title><link>/blog/managing-beam-dependencies-in-java/</link><pubDate>Fri, 23 Jun 2023 09:00:00 -0700</pubDate><guid>/blog/managing-beam-dependencies-in-java/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Managing your Java dependencies can be challenging, and if not done correctly,
it may cause a variety of problems, as incompatibilities may arise when using
specific and previously untested combinations.&lt;/p>
&lt;p>To make that process easier, Beam now
provides &lt;a href="https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html#bill-of-materials-bom-poms">Bill of Materials (BOM)&lt;/a>
artifacts that will help dependency management tools to select compatible
combinations.&lt;/p>
&lt;p>We hope this will make it easier for you to use Apache Beam, and have a simpler
transition when upgrading to newer versions.&lt;/p>
&lt;p>When bringing incompatible classes and libraries, the code is susceptible to
errors such
as &lt;code>NoClassDefFoundError&lt;/code>, &lt;code>NoSuchMethodError&lt;/code>, &lt;code>NoSuchFieldError&lt;/code>, &lt;code>FATAL ERROR in native method&lt;/code>.&lt;/p>
&lt;p>When importing Apache Beam, the recommended way is to use Bill of Materials
(BOMs). The way BOMs work is by providing hints to the dependency management
resolution tool, so when a project imports unspecified or ambiguous dependencies,
it will know what version to use.&lt;/p>
&lt;p>There are currently two BOMs provided by Beam:&lt;/p>
&lt;ul>
&lt;li>&lt;code>beam-sdks-java-bom&lt;/code>, which manages what dependencies of Beam will be used, so
you can specify the version only once.&lt;/li>
&lt;li>&lt;code>beam-sdks-java-io-google-cloud-platform-bom&lt;/code>, a more comprehensive list,
which manages Beam, along with GCP client and third-party dependencies.&lt;/li>
&lt;/ul>
&lt;p>Since errors are more likely to arise when using third-party dependencies,
that&amp;rsquo;s the one that is recommended to use to minimize any conflicts.&lt;/p>
&lt;p>In order to use BOM, the artifact has to be imported to your Maven or Gradle
dependency configurations. For example, to
use &lt;code>beam-sdks-java-io-google-cloud-platform-bom&lt;/code>,
the following changes have to be done (and make sure that &lt;em>BEAM_VERSION&lt;/em> is
replaced by a valid version):&lt;/p>
&lt;p>&lt;strong>Maven&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;dependencyManagement&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;dependencies&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;groupId&amp;gt;&lt;/span>org.apache.beam&lt;span class="nt">&amp;lt;/groupId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;artifactId&amp;gt;&lt;/span>beam-sdks-java-google-cloud-platform-bom&lt;span class="nt">&amp;lt;/artifactId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;version&amp;gt;&lt;/span>BEAM_VERSION&lt;span class="nt">&amp;lt;/version&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;type&amp;gt;&lt;/span>pom&lt;span class="nt">&amp;lt;/type&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;scope&amp;gt;&lt;/span>import&lt;span class="nt">&amp;lt;/scope&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;/dependencies&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/dependencyManagement&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Gradle&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code>dependencies {
implementation(platform(&amp;#34;org.apache.beam:beam-sdks-java-google-cloud-platform-bom:BEAM_VERSION&amp;#34;))
}
&lt;/code>&lt;/pre>&lt;p>After importing the BOM, specific version pinning of dependencies, for example,
anything for &lt;code>org.apache.beam&lt;/code>, &lt;code>io.grpc&lt;/code>, &lt;code>com.google.cloud&lt;/code> (
including &lt;code>libraries-bom&lt;/code>) may be removed.&lt;/p>
&lt;p>Do not entirely remove the dependencies, as they are not automatically imported
by the BOM. It is important to keep the dependency without specifying a version.
For example, in Maven:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;groupId&amp;gt;&lt;/span>org.apache.beam&lt;span class="nt">&amp;lt;/groupId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;lt;artifactId&amp;gt;&lt;/span>beam-sdks-java-core&lt;span class="nt">&amp;lt;/artifactId&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Or Gradle:&lt;/p>
&lt;pre tabindex="0">&lt;code>implementation(&amp;#34;org.apache.beam:beam-sdks-java-core&amp;#34;)
&lt;/code>&lt;/pre>&lt;p>For a full list of dependency versions that are managed by a specific BOM, the
Maven tool &lt;code>help:effective-pom&lt;/code> can be used. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">mvn help:effective-pom -f ~/.m2/repository/org/apache/beam/beam-sdks-java-google-cloud-platform-bom/BEAM_VERSION/beam-sdks-java-google-cloud-platform-bom-BEAM_VERSION.pom
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The third-party
website &lt;a href="https://mvnrepository.com/artifact/org.apache.beam/beam-sdks-java-google-cloud-platform-bom/">mvnrepository.com&lt;/a>
can also be used to display such version information.&lt;/p>
&lt;p>We hope you find this
useful. &lt;a href="https://beam.apache.org/community/contact-us/">Feedback&lt;/a> and
contributions are always welcome! So feel free to create a GitHub issue, or open
a Pull Request if you encounter any problem when using those artifacts.&lt;/p></description></item><item><title>Blog: Getting started with Apache Beam: An open source proficiency credential sponsored by Google Cloud</title><link>/blog/beamquest/</link><pubDate>Tue, 06 Jun 2023 00:00:01 -0800</pubDate><guid>/blog/beamquest/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-badge-image-scaled.png"
alt="Quest image">&lt;/p>
&lt;p>We’re excited to announce the release of the &lt;a href="https://www.cloudskillsboost.google/quests/310">“Getting Started with Apache Beam” quest&lt;/a>, a series of four online labs that venture into different Apache Beam concepts. When you complete all four labs, you’ll earn a Google Cloud badge that you can share on platforms like LinkedIn. Earning this badge should take less than seven hours total, and signing up for the quest costs $20 (there are often free specials for people who attend Beam events, such as &lt;a href="https://www.meetup.com/topics/apache-beam/">Meetups&lt;/a>, &lt;a href="https://beamsummit.org/">Beam Summit&lt;/a>, and &lt;a href="https://beamcollege.dev/">Beam College&lt;/a>).&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>&amp;ldquo;I really like the workshop to be honest. I learnt a lot from doing those labs. I would suggest we offer that for any new members to Beam&amp;rdquo;&lt;/em> &amp;ndash; Shunping Huang, Software Engineer&lt;/p>
&lt;/blockquote>
&lt;p>Beam is one of the largest big data open source projects actively in development. Over the past six years, the Apache Beam community has seen tremendous growth in the number of contributors, committers, and users. If you’re a long time Beam user, you can now earn a badge to show your skills to potential employers. If you’re new to Beam, you can begin your learning journey with this quest. To attempt this quest, you don’t need any prior knowledge of data processing or distributed systems. All you need is elementary knowledge about programming.&lt;/p>
&lt;p>Individuals aren’t the only ones who can benefit from completing this quest - organizations can too! Because earning this badge represents deep knowledge of an industry leading big data library, having the badge validates your organization’s understanding of Beam. In addition, you can run the Beam library on a wide variety of runners, including Google Cloud Dataflow, Flink, Spark, and more, making knowledge about this library highly transferable. Finally, your organization can use this quest as onboarding material for new hires on big data teams, allowing teams and organizations to get their newest employees up-to-date on the latest and greatest that Apache Beam has to offer.&lt;/p>
&lt;p>Data Processing is a key part of AI/ML workflows. Given the recent advancements in artificial intelligence, now’s the time to jump into the world of data processing! Get started on your journey &lt;a href="https://www.cloudskillsboost.google/quests/310">here&lt;/a>.&lt;/p>
&lt;p>We are currently offering this quest &lt;strong>FREE OF CHARGE&lt;/strong> until &lt;strong>July 8, 2023&lt;/strong> for the &lt;strong>first 2,000&lt;/strong> people. To obtain your badge for &lt;strong>FREE&lt;/strong>, use the &lt;a href="https://www.cloudskillsboost.google/catalog?qlcampaign=1h-swiss-19">Access Code&lt;/a>, create an account, and search &lt;a href="https://www.cloudskillsboost.google/quests/310">&amp;ldquo;Getting Started with Apache Beam&amp;rdquo;&lt;/a>.&lt;/p>
&lt;p>PS: Once you earn your badge, please &lt;a href="https://support.google.com/qwiklabs/answer/9222527?hl=en&amp;amp;sjid=14905615709060962899-NA">share it on social media&lt;/a>!&lt;/p></description></item><item><title>Blog: Apache Beam 2.48.0</title><link>/blog/beam-2.48.0/</link><pubDate>Wed, 31 May 2023 11:30:00 -0400</pubDate><guid>/blog/beam-2.48.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.48.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2480-2023-05-31">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.48.0, check out the &lt;a href="https://github.com/apache/beam/milestone/12">detailed release notes&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Note: The release tag for Go SDK for this release is sdks/v2.48.2 instead of sdks/v2.48.0 because of incorrect commit attached to the release tag sdks/v2.48.0.&lt;/strong>&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>&amp;ldquo;Experimental&amp;rdquo; annotation cleanup: the annotation and concept have been removed from Beam to avoid
the misperception of code as &amp;ldquo;not ready&amp;rdquo;. Any proposed breaking changes will be subject to
case-by-case pro/con decision making (and generally avoided) rather than using the &amp;ldquo;Experimental&amp;rdquo;
to allow them.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added rename for GCS and copy for local filesystem (Go) (&lt;a href="https://github.com/apache/beam/issues/26064">#25779&lt;/a>).&lt;/li>
&lt;li>Added support for enhanced fan-out in KinesisIO.Read (Java) (&lt;a href="https://github.com/apache/beam/issues/19967">#19967&lt;/a>).
&lt;ul>
&lt;li>This change is not compatible with Flink savepoints created by Beam 2.46.0 applications which had KinesisIO sources.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Added textio.ReadWithFilename transform (Go) (&lt;a href="https://github.com/apache/beam/issues/25812">#25812&lt;/a>).&lt;/li>
&lt;li>Added fileio.MatchContinuously transform (Go) (&lt;a href="https://github.com/apache/beam/issues/26186">#26186&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Allow passing service name for google-cloud-profiler (Python) (&lt;a href="https://github.com/apache/beam/issues/26280">#26280&lt;/a>).&lt;/li>
&lt;li>Dead letter queue support added to RunInference in Python (&lt;a href="https://github.com/apache/beam/issues/24209">#24209&lt;/a>).&lt;/li>
&lt;li>Support added for defining pre/postprocessing operations on the RunInference transform (&lt;a href="https://github.com/apache/beam/issues/26308">#26308&lt;/a>)&lt;/li>
&lt;li>Adds a Docker Compose based transform service that can be used to discover and use portable Beam transforms (&lt;a href="https://github.com/apache/beam/pull/26023">#26023&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Passing a tag into MultiProcessShared is now required in the Python SDK (&lt;a href="https://github.com/apache/beam/issues/26168">#26168&lt;/a>).&lt;/li>
&lt;li>CloudDebuggerOptions is removed (deprecated in Beam v2.47.0) for Dataflow runner as the Google Cloud Debugger service is &lt;a href="https://cloud.google.com/debugger/docs/deprecations">shutting down&lt;/a>. (Java) (&lt;a href="https://github.com/apache/beam/issues/25959">#25959&lt;/a>).&lt;/li>
&lt;li>AWS 2 client providers (deprecated in Beam &lt;a href="#2380---2022-04-20">v2.38.0&lt;/a>) are finally removed (&lt;a href="https://github.com/apache/beam/issues/26681">#26681&lt;/a>).&lt;/li>
&lt;li>AWS 2 SnsIO.writeAsync (deprecated in Beam v2.37.0 due to risk of data loss) was finally removed (&lt;a href="https://github.com/apache/beam/issues/26710">#26710&lt;/a>).&lt;/li>
&lt;li>AWS 2 coders (deprecated in Beam v2.43.0 when adding Schema support for AWS Sdk Pojos) are finally removed (&lt;a href="https://github.com/apache/beam/issues/23315">#23315&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Java bootloader failing with Too Long Args due to long classpaths, with a pathing jar. (Java) (&lt;a href="https://github.com/apache/beam/issues/25582">#25582&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>PubsubIO writes will throw &lt;em>SizeLimitExceededException&lt;/em> for any message above 100 bytes, when used in batch (bounded) mode. (Java) (&lt;a href="https://github.com/apache/beam/issues/27000">#27000&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.48.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abzal Tuganbay&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Balázs Németh&lt;/p>
&lt;p>Bazyli Polednia&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Daniel Arn&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>George Novitskiy&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Jasper Van den Bossche&lt;/p>
&lt;p>Jeff Zhang&lt;/p>
&lt;p>Jeremy Edwards&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>Katie Liu&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kerry Donny-Clark&lt;/p>
&lt;p>Kuba Rauch&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Nick Li&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Pranjal Joshi&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rouslan&lt;/p>
&lt;p>RuiLong J&lt;/p>
&lt;p>RyujiTamaki&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sanil Jain&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vishal Bhise&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>kellen&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>mokamoka03210120&lt;/p>
&lt;p>psolomin&lt;/p></description></item><item><title>Blog: Apache Beam 2.47.0</title><link>/blog/beam-2.47.0/</link><pubDate>Wed, 10 May 2023 12:00:00 -0500</pubDate><guid>/blog/beam-2.47.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.47.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2470-2023-05-10">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.47.0, check out the &lt;a href="https://github.com/apache/beam/milestone/10">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Apache Beam adds Python 3.11 support (&lt;a href="https://github.com/apache/beam/issues/23848">#23848&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>BigQuery Storage Write API is now available in Python SDK via cross-language (&lt;a href="https://github.com/apache/beam/issues/21961">#21961&lt;/a>).&lt;/li>
&lt;li>Added HbaseIO support for writing RowMutations (ordered by rowkey) to Hbase (Java) (&lt;a href="https://github.com/apache/beam/issues/25830">#25830&lt;/a>).&lt;/li>
&lt;li>Added fileio transforms MatchFiles, MatchAll and ReadMatches (Go) (&lt;a href="https://github.com/apache/beam/issues/25779">#25779&lt;/a>).&lt;/li>
&lt;li>Add integration test for JmsIO + fix issue with multiple connections (Java) (&lt;a href="https://github.com/apache/beam/issues/25887">#25887&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>The Flink runner now supports Flink 1.16.x (&lt;a href="https://github.com/apache/beam/issues/25046">#25046&lt;/a>).&lt;/li>
&lt;li>Schema&amp;rsquo;d PTransforms can now be directly applied to Beam dataframes just like PCollections.
(Note that when doing multiple operations, it may be more efficient to explicitly chain the operations
like &lt;code>df | (Transform1 | Transform2 | ...)&lt;/code> to avoid excessive conversions.)&lt;/li>
&lt;li>The Go SDK adds new transforms periodic.Impulse and periodic.Sequence that extends support
for slowly updating side input patterns. (&lt;a href="https://github.com/apache/beam/issues/23106">#23106&lt;/a>)&lt;/li>
&lt;li>Several Google client libraries in Python SDK dependency chain were updated to latest available major versions. (&lt;a href="https://github.com/apache/beam/pull/24599">#24599&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>If a main session fails to load, the pipeline will now fail at worker startup. (&lt;a href="https://github.com/apache/beam/issues/25401">#25401&lt;/a>).&lt;/li>
&lt;li>Python pipeline options will now ignore unparsed command line flags prefixed with a single dash. (&lt;a href="https://github.com/apache/beam/issues/25943">#25943&lt;/a>).&lt;/li>
&lt;li>The SmallestPerKey combiner now requires keyword-only arguments for specifying optional parameters, such as &lt;code>key&lt;/code> and &lt;code>reverse&lt;/code>. (&lt;a href="https://github.com/apache/beam/issues/25888">#25888&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Cloud Debugger support and its pipeline options are deprecated and will be removed in the next Beam version,
in response to the Google Cloud Debugger service &lt;a href="https://cloud.google.com/debugger/docs/deprecations">turning down&lt;/a>. (Java) (&lt;a href="https://github.com/apache/beam/issues/25959">#25959&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>BigQuery sink in STORAGE_WRITE_API mode in batch pipelines might result in data consistency issues during the handling of other unrelated transient errors for Beam SDKs 2.35.0 - 2.46.0 (inclusive). For more details see: &lt;a href="https://github.com/apache/beam/issues/26521">https://github.com/apache/beam/issues/26521&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>BigQueryIO Storage API write with autoUpdateSchema may cause data corruption for Beam SDKs 2.45.0 - 2.47.0 (inclusive) (&lt;a href="https://github.com/apache/beam/issues/26789">#26789&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.47.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Amir Fayazi&lt;/p>
&lt;p>Amrane Ait Zeouay&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Andrey Kot&lt;/p>
&lt;p>Bjorn Pedersen&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Buqian Zheng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>ChangyuLi28&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>George Ma&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jasper Van den Bossche&lt;/p>
&lt;p>Jeremy Edwards&lt;/p>
&lt;p>Jiangjie (Becket) Qin&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Juta Staes&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kyle Weaver&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Nick Li&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Reza Rokni&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Saadat Su&lt;/p>
&lt;p>Saifuddin53&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Shubham Krishna&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Theodore Ni&lt;/p>
&lt;p>Thomas Gaddy&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yanan Hao&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Yuvi Panda&lt;/p>
&lt;p>andres-vv&lt;/p>
&lt;p>bochap&lt;/p>
&lt;p>dannikay&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>harrisonlimh&lt;/p>
&lt;p>hnnsgstfssn&lt;/p>
&lt;p>jrmccluskey&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xianhualiu&lt;/p>
&lt;p>zhangskz&lt;/p></description></item><item><title>Blog: Apache Beam 2.46.0</title><link>/blog/beam-2.46.0/</link><pubDate>Fri, 10 Mar 2023 13:00:00 -0500</pubDate><guid>/blog/beam-2.46.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.46.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2460-2023-03-10">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.46.0, check out the &lt;a href="https://github.com/apache/beam/milestone/9?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Java SDK containers migrated to &lt;a href="https://hub.docker.com/_/eclipse-temurin">Eclipse Temurin&lt;/a>
as a base. This change migrates away from the deprecated &lt;a href="https://hub.docker.com/_/openjdk">OpenJDK&lt;/a>
container. Eclipse Temurin is currently based upon Ubuntu 22.04 while the OpenJDK
container was based upon Debian 11.&lt;/li>
&lt;li>RunInference PTransform will accept model paths as SideInputs in Python SDK. (&lt;a href="https://github.com/apache/beam/issues/24042">#24042&lt;/a>)&lt;/li>
&lt;li>RunInference supports ONNX runtime in Python SDK (&lt;a href="https://github.com/apache/beam/issues/22972">#22972&lt;/a>)&lt;/li>
&lt;li>Tensorflow Model Handler for RunInference in Python SDK (&lt;a href="https://github.com/apache/beam/issues/25366">#25366&lt;/a>)&lt;/li>
&lt;li>Java SDK modules migrated to use &lt;code>:sdks:java:extensions:avro&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/24748">#24748&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added in JmsIO a retry policy for failed publications (Java) (&lt;a href="https://github.com/apache/beam/issues/24971">#24971&lt;/a>).&lt;/li>
&lt;li>Support for &lt;code>LZMA&lt;/code> compression/decompression of text files added to the Python SDK (&lt;a href="https://github.com/apache/beam/issues/25316">#25316&lt;/a>)&lt;/li>
&lt;li>Added ReadFrom/WriteTo Csv/Json as top-level transforms to the Python SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Add UDF metrics support for Samza portable mode.&lt;/li>
&lt;li>Option for SparkRunner to avoid the need of SDF output to fit in memory (&lt;a href="https://github.com/apache/beam/issues/23852">#23852&lt;/a>).
This helps e.g. with ParquetIO reads. Turn the feature on by adding experiment &lt;code>use_bounded_concurrent_output_for_sdf&lt;/code>.&lt;/li>
&lt;li>Add &lt;code>WatchFilePattern&lt;/code> transform, which can be used as a side input to the RunInference PTransfrom to watch for model updates using a file pattern. (&lt;a href="https://github.com/apache/beam/issues/24042">#24042&lt;/a>)&lt;/li>
&lt;li>Add support for loading TorchScript models with &lt;code>PytorchModelHandler&lt;/code>. The TorchScript model path can be
passed to PytorchModelHandler using &lt;code>torch_script_model_path=&amp;lt;path_to_model&amp;gt;&lt;/code>. (&lt;a href="https://github.com/apache/beam/pull/25321">#25321&lt;/a>)&lt;/li>
&lt;li>The Go SDK now requires Go 1.19 to build. (&lt;a href="https://github.com/apache/beam/pull/25545">#25545&lt;/a>)&lt;/li>
&lt;li>The Go SDK now has an initial native Go implementation of a portable Beam Runner called Prism. (&lt;a href="https://github.com/apache/beam/pull/24789">#24789&lt;/a>)
&lt;ul>
&lt;li>For more details and current state see &lt;a href="https://github.com/apache/beam/tree/master/sdks/go/pkg/beam/runners/prism">https://github.com/apache/beam/tree/master/sdks/go/pkg/beam/runners/prism&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The deprecated SparkRunner for Spark 2 (see &lt;a href="#2410---2022-08-23">2.41.0&lt;/a>) was removed (&lt;a href="https://github.com/apache/beam/pull/25263">#25263&lt;/a>).&lt;/li>
&lt;li>Python&amp;rsquo;s BatchElements performs more aggressive batching in some cases,
capping at 10 second rather than 1 second batches by default and excluding
fixed cost in this computation to better handle cases where the fixed cost
is larger than a single second. To get the old behavior, one can pass
&lt;code>target_batch_duration_secs_including_fixed_cost=1&lt;/code> to BatchElements.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Avro related classes are deprecated in module &lt;code>beam-sdks-java-core&lt;/code> and will be eventually removed. Please, migrate to a new module &lt;code>beam-sdks-java-extensions-avro&lt;/code> instead by importing the classes from &lt;code>org.apache.beam.sdk.extensions.avro&lt;/code> package.
For the sake of migration simplicity, the relative package path and the whole class hierarchy of Avro related classes in new module is preserved the same as it was before.
For example, import &lt;code>org.apache.beam.sdk.extensions.avro.coders.AvroCoder&lt;/code> class instead of&lt;code>org.apache.beam.sdk.coders.AvroCoder&lt;/code>. (&lt;a href="https://github.com/apache/beam/issues/24749">#24749&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.46.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alan Zhang&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Amrane Ait Zeouay&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Brian Hulette&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>David Katz&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Doug Judd&lt;/p>
&lt;p>Egbert van der Wal&lt;/p>
&lt;p>Elizaveta Lomteva&lt;/p>
&lt;p>Evan Galpin&lt;/p>
&lt;p>Herman Mak&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>Jozef Vilcek&lt;/p>
&lt;p>Junhao Liu&lt;/p>
&lt;p>Juta Staes&lt;/p>
&lt;p>Katie Liu&lt;/p>
&lt;p>Kiley Sok&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>Luke Cwik&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Ning Kang&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo E&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Ruslan Altynnikov&lt;/p>
&lt;p>Ryan Zhang&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sam sam&lt;/p>
&lt;p>Sergei Lilichenko&lt;/p>
&lt;p>Shivam&lt;/p>
&lt;p>Shubham Krishna&lt;/p>
&lt;p>Theodore Ni&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Vachan&lt;/p>
&lt;p>Veronica Wasson&lt;/p>
&lt;p>Vincent Devillers&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>William Ross Morrow&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>ZhengLin Li&lt;/p>
&lt;p>Ziqi Ma&lt;/p>
&lt;p>ahmedabu98&lt;/p>
&lt;p>alexeyinkin&lt;/p>
&lt;p>aliftadvantage&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>dannikay&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>kamrankoupayi&lt;/p>
&lt;p>kileys&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>nancyxu123&lt;/p>
&lt;p>nickuncaged1201&lt;/p>
&lt;p>pablo rodriguez defino&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xqhu&lt;/p></description></item><item><title>Blog: Apache Beam 2.45.0</title><link>/blog/beam-2.45.0/</link><pubDate>Wed, 15 Feb 2023 09:00:00 -0700</pubDate><guid>/blog/beam-2.45.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.45.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2430-2023-01-13">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.45.0, check out the &lt;a href="https://github.com/apache/beam/milestone/8?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>MongoDB IO connector added (Go) (&lt;a href="https://github.com/apache/beam/issues/24575">#24575&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>RunInference Wrapper with Sklearn Model Handler support added in Go SDK (&lt;a href="https://github.com/apache/beam/issues/23382">#24497&lt;/a>).&lt;/li>
&lt;li>Adding override of allowed TLS algorithms (Java), now maintaining the disabled/legacy algorithms
present in 2.43.0 (up to 1.8.0_342, 11.0.16, 17.0.2 for respective Java versions). This is accompanied
by an explicit re-enabling of TLSv1 and TLSv1.1 for Java 8 and Java 11.&lt;/li>
&lt;li>Add UDF metrics support for Samza portable mode.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Portable Java pipelines, Go pipelines, Python streaming pipelines, and portable Python batch
pipelines on Dataflow are required to use Runner V2. The &lt;code>disable_runner_v2&lt;/code>,
&lt;code>disable_runner_v2_until_2023&lt;/code>, &lt;code>disable_prime_runner_v2&lt;/code> experiments will raise an error during
pipeline construction. You can no longer specify the Dataflow worker jar override. Note that
non-portable Java jobs and non-portable Python batch jobs are not impacted. (&lt;a href="https://github.com/apache/beam/issues/24515">#24515&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Avoids Cassandra syntax error when user-defined query has no where clause in it (Java) (&lt;a href="https://github.com/apache/beam/issues/24829">#24829&lt;/a>).&lt;/li>
&lt;li>Fixed JDBC connection failures (Java) during handshake due to deprecated TLSv1(.1) protocol for the JDK. (&lt;a href="https://github.com/apache/beam/issues/24623">#24623&lt;/a>)&lt;/li>
&lt;li>Fixed Python BigQuery Batch Load write may truncate valid data when deposition sets to WRITE_TRUNCATE and incoming data is large (Python) (&lt;a href="https://github.com/apache/beam/issues/24535">#24623&lt;/a>).&lt;/li>
&lt;li>Fixed Kafka watermark issue with sparse data on many partitions (&lt;a href="https://github.com/apache/beam/pull/24205">#24205&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.45.0 release. Thank you to all contributors!&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrea Nardelli&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Benjamin Gonzalez&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Brian Hulette&lt;/p>
&lt;p>Bulat&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Charles Rothrock&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Daniela Martín&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>Dejan Spasic&lt;/p>
&lt;p>Diego Gomez&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Doug Judd&lt;/p>
&lt;p>Elias Segundo Antonio&lt;/p>
&lt;p>Evan Galpin&lt;/p>
&lt;p>Evgeny Antyshev&lt;/p>
&lt;p>Fernando Morales&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>Junhao Liu&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kiley Sok&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>Lucas Marques&lt;/p>
&lt;p>Luke Cwik&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Marco Robles&lt;/p>
&lt;p>Mark Zitnik&lt;/p>
&lt;p>Melanie&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Ning Kang&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Philippe Moussalli&lt;/p>
&lt;p>Piyush Sagar&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Rick Viscomi&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sergei Lilichenko&lt;/p>
&lt;p>Seung Jin An&lt;/p>
&lt;p>Shane Hansen&lt;/p>
&lt;p>Sho Nakatani&lt;/p>
&lt;p>Shunya Ueta&lt;/p>
&lt;p>Siddharth Agrawal&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Veronica Wasson&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Xinbin Huang&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Xinyue Zhang&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>ZhengLin Li&lt;/p>
&lt;p>alexeyinkin&lt;/p>
&lt;p>andoni-guzman&lt;/p>
&lt;p>andthezhang&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>camphillips22&lt;/p>
&lt;p>gabihodoroaga&lt;/p>
&lt;p>harrisonlimh&lt;/p>
&lt;p>pablo rodriguez defino&lt;/p>
&lt;p>ruslan-ikhsan&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>yyy1000&lt;/p>
&lt;p>zhengbuqian&lt;/p></description></item><item><title>Blog: Apache Beam 2.44.0</title><link>/blog/beam-2.44.0/</link><pubDate>Tue, 17 Jan 2023 09:00:00 -0700</pubDate><guid>/blog/beam-2.44.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.44.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2430-2023-01-13">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.44.0, check out the &lt;a href="https://github.com/apache/beam/milestone/7?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for Bigtable sink (Write and WriteBatch) added (Go) (&lt;a href="https://github.com/apache/beam/issues/23324">#23324&lt;/a>).&lt;/li>
&lt;li>S3 implementation of the Beam filesystem (Go) (&lt;a href="https://github.com/apache/beam/issues/23991">#23991&lt;/a>).&lt;/li>
&lt;li>Support for SingleStoreDB source and sink added (Java) (&lt;a href="https://github.com/apache/beam/issues/22617">#22617&lt;/a>).&lt;/li>
&lt;li>Added support for DefaultAzureCredential authentication in Azure Filesystem (Python) (&lt;a href="https://github.com/apache/beam/issues/24210">#24210&lt;/a>).&lt;/li>
&lt;li>Added new CdapIO for CDAP Batch and Streaming Source/Sinks (Java) (&lt;a href="https://github.com/apache/beam/issues/24961">#24961&lt;/a>).&lt;/li>
&lt;li>Added new SparkReceiverIO for Spark Receivers 2.4.* (Java) (&lt;a href="https://github.com/apache/beam/issues/24960">#24960&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Beam now provides a portable &amp;ldquo;runner&amp;rdquo; that can render pipeline graphs with
graphviz. See &lt;code>python -m apache_beam.runners.render --help&lt;/code> for more details.&lt;/li>
&lt;li>Local packages can now be used as dependencies in the requirements.txt file, rather
than requiring them to be passed separately via the &lt;code>--extra_package&lt;/code> option
(Python) (&lt;a href="https://github.com/apache/beam/pull/23684">#23684&lt;/a>).&lt;/li>
&lt;li>Pipeline Resource Hints now supported via &lt;code>--resource_hints&lt;/code> flag (Go) (&lt;a href="https://github.com/apache/beam/pull/23990">#23990&lt;/a>).&lt;/li>
&lt;li>Make Python SDK containers reusable on portable runners by installing dependencies to temporary venvs (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12792">BEAM-12792&lt;/a>).&lt;/li>
&lt;li>RunInference model handlers now support the specification of a custom inference function in Python (&lt;a href="https://github.com/apache/beam/issues/22572">#22572&lt;/a>)&lt;/li>
&lt;li>Support for &lt;code>map_windows&lt;/code> urn added to Go SDK (&lt;a href="https://github.apache/beam/pull/24307">#24307&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>&lt;code>ParquetIO.withSplit&lt;/code> was removed since splittable reading has been the default behavior since 2.35.0. The effect of
this change is to drop support for non-splittable reading (Java)(&lt;a href="https://github.com/apache/beam/issues/23832">#23832&lt;/a>).&lt;/li>
&lt;li>&lt;code>beam-sdks-java-extensions-google-cloud-platform-core&lt;/code> is no longer a
dependency of the Java SDK Harness. Some users of a portable runner (such as Dataflow Runner v2)
may have an undeclared dependency on this package (for example using GCS with
TextIO) and will now need to declare the dependency.&lt;/li>
&lt;li>&lt;code>beam-sdks-java-core&lt;/code> is no longer a dependency of the Java SDK Harness. Users of a portable
runner (such as Dataflow Runner v2) will need to provide this package and its dependencies.&lt;/li>
&lt;li>Slices now use the Beam Iterable Coder. This enables cross language use, but breaks pipeline updates
if a Slice type is used as a PCollection element or State API element. (Go)&lt;a href="https://github.com/apache/beam/issues/24339">#24339&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed JmsIO acknowledgment issue (Java) (&lt;a href="https://github.com/apache/beam/issues/20814">#20814&lt;/a>)&lt;/li>
&lt;li>Fixed Beam SQL CalciteUtils (Java) and Cross-language JdbcIO (Python) did not support JDBC CHAR/VARCHAR, BINARY/VARBINARY logical types (&lt;a href="https://github.com/apache/beam/issues/23747">#23747&lt;/a>, &lt;a href="https://github.com/apache/beam/issues/23526">#23526&lt;/a>).&lt;/li>
&lt;li>Ensure iterated and emitted types are used with the generic register package are registered with the type and schema registries.(Go) (&lt;a href="https://github.com/apache/beam/pull/23889">#23889&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.44.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alex Merose&lt;/p>
&lt;p>Alexey Inkin&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrej Galad&lt;/p>
&lt;p>Andrew Pilloud&lt;/p>
&lt;p>Ayush Sharma&lt;/p>
&lt;p>Benjamin Gonzalez&lt;/p>
&lt;p>Bjorn Pedersen&lt;/p>
&lt;p>Brian Hulette&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Bulat Safiullin&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Chris Gavin&lt;/p>
&lt;p>Damon Douglas&lt;/p>
&lt;p>Danielle Syse&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>David Cavazos&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Doug Judd&lt;/p>
&lt;p>Elias Segundo Antonio&lt;/p>
&lt;p>Evan Galpin&lt;/p>
&lt;p>Evgeny Antyshev&lt;/p>
&lt;p>Heejong Lee&lt;/p>
&lt;p>Henrik Heggelund-Berg&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Janek Bevendorff&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>John J. Casey&lt;/p>
&lt;p>Jozef Vilcek&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kiley Sok&lt;/p>
&lt;p>Laksh&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>Luke Cwik&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Minbo Bae&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Nancy Xu&lt;/p>
&lt;p>Ning Kang&lt;/p>
&lt;p>Nivaldo Tokuda&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Philippe Moussalli&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Rick Smit&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Ryan Thompson&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sanil Jain&lt;/p>
&lt;p>Scott Strong&lt;/p>
&lt;p>Shubham Krishna&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Thiago Nunes&lt;/p>
&lt;p>Tianyang Hu&lt;/p>
&lt;p>Trevor Gevers&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vladislav Chunikhin&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Yichi Zhang&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>agvdndor&lt;/p>
&lt;p>andremissaglia&lt;/p>
&lt;p>arne-alex&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>camphillips22&lt;/p>
&lt;p>capthiron&lt;/p>
&lt;p>creste&lt;/p>
&lt;p>fab-jul&lt;/p>
&lt;p>illoise&lt;/p>
&lt;p>kn1kn1&lt;/p>
&lt;p>nancyxu123&lt;/p>
&lt;p>peridotml&lt;/p>
&lt;p>shinannegans&lt;/p>
&lt;p>smeet07&lt;/p></description></item><item><title>Blog: Apache Beam Playground: An interactive environment to try transforms and examples</title><link>/blog/apacheplayground/</link><pubDate>Wed, 30 Nov 2022 00:00:01 -0800</pubDate><guid>/blog/apacheplayground/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h3 id="what-is-apache-beam-playground">&lt;strong>What is Apache Beam Playground?&lt;/strong>&lt;/h3>
&lt;p>&lt;a href="https://play.beam.apache.org/">Apache Beam Playground&lt;/a> is an interactive environment to try Apache Beam transforms and examples without requiring to install or set up a Beam environment.&lt;/p>
&lt;h3 id="apache-beam-playground-features">&lt;strong>Apache Beam Playground Features&lt;/strong>&lt;/h3>
&lt;p>&lt;img class="center-block"
src="/images/blog/BeamPlayground.gif"
alt="Apache Beam Playground">&lt;/p>
&lt;ul>
&lt;li>Discover transform examples that you can try right away by browsing or searching Catalog that is sourced from Apache Beam GitHub&lt;/li>
&lt;li>Supports Java, Python, Go SDKs, and Scio to execute the example in Beam Direct Runner&lt;/li>
&lt;li>Displays pipeline execution graph (DAG)&lt;/li>
&lt;li>Code editor to modify examples or try your own custom pipeline with a Direct Runner&lt;/li>
&lt;li>Code editor with code highlighting, flexible layout, color schemes, and other features to provide responsive UX in desktop browsers&lt;/li>
&lt;li>Embedding a Playground example on a web page prompts the web page readers to try the example pipeline in the Playground - e.g., &lt;a href="/get-started/try-beam-playground/">Playground Quickstart&lt;/a> page&lt;/li>
&lt;/ul>
&lt;h3 id="whats-next">&lt;strong>What’s Next&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>Try examples in &lt;a href="https://play.beam.apache.org/">Apache Beam Playground&lt;/a>&lt;/li>
&lt;li>Submit your feedback using “Enjoying Playground?” in Apache Beam Playground or via &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSd5_5XeOwwW2yjEVHUXmiBad8Lxk-4OtNcgG45pbyAZzd4EbA/viewform?usp=pp_url">this form&lt;/a>&lt;/li>
&lt;li>Join the Beam &lt;a href="/community/contact-us">users@&lt;/a> mailing list&lt;/li>
&lt;li>Contribute to the Apache Beam Playground codebase by following a few steps in this &lt;a href="/contribute">Contribution Guide&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Please &lt;a href="/community/contact-us">reach out&lt;/a> if you have any feedback or encounter any issues!&lt;/p></description></item><item><title>Blog: Apache Beam 2.43.0</title><link>/blog/beam-2.43.0/</link><pubDate>Thu, 17 Nov 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.43.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.43.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2430-2022-11-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.43.0, check out the &lt;a href="https://github.com/apache/beam/milestone/5?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Python 3.10 support in Apache Beam (&lt;a href="https://github.com/apache/beam/issues/21458">#21458&lt;/a>).&lt;/li>
&lt;li>An initial implementation of a runner that allows us to run Beam pipelines on Dask. Try it out and give us feedback! (Python) (&lt;a href="https://github.com/apache/beam/issues/18962">#18962&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Decreased TextSource CPU utilization by 2.3x (Java) (&lt;a href="https://github.com/apache/beam/issues/23193">#23193&lt;/a>).&lt;/li>
&lt;li>Fixed bug when using SpannerIO with RuntimeValueProvider options (Java) (&lt;a href="https://github.com/apache/beam/issues/22146">#22146&lt;/a>).&lt;/li>
&lt;li>Fixed issue for unicode rendering on WriteToBigQuery (&lt;a href="https://github.com/apache/beam/issues/22312">#22312&lt;/a>)&lt;/li>
&lt;li>Remove obsolete variants of BigQuery Read and Write, always using Beam-native variant
(&lt;a href="https://github.com/apache/beam/issues/23564">#23564&lt;/a> and &lt;a href="https://github.com/apache/beam/issues/23559">#23559&lt;/a>).&lt;/li>
&lt;li>Bumped google-cloud-spanner dependency version to 3.x for Python SDK (&lt;a href="https://github.com/apache/beam/issues/21198">#21198&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Dataframe wrapper added in Go SDK via Cross-Language (with automatic expansion service). (Go) (&lt;a href="https://github.com/apache/beam/issues/23384">#23384&lt;/a>).&lt;/li>
&lt;li>Name all Java threads to aid in debugging (&lt;a href="https://github.com/apache/beam/issues/23049">#23049&lt;/a>).&lt;/li>
&lt;li>An initial implementation of a runner that allows us to run Beam pipelines on Dask. (Python) (&lt;a href="https://github.com/apache/beam/issues/18962">#18962&lt;/a>).&lt;/li>
&lt;li>Allow configuring GCP OAuth scopes via pipeline options. This unblocks usages of Beam IOs that require additional scopes.
For example, this feature makes it possible to access Google Drive backed tables in BigQuery (&lt;a href="https://github.com/apache/beam/issues/23290">#23290&lt;/a>).&lt;/li>
&lt;li>An example for using Python RunInference from Java (&lt;a href="https://github.com/apache/beam/pull/23619">#23290&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>CoGroupByKey transform in Python SDK has changed the output typehint. The typehint component representing grouped values changed from List to Iterable,
which more accurately reflects the nature of the arbitrarily large output collection. &lt;a href="https://github.com/apache/beam/issues/21556">#21556&lt;/a> Beam users may see an error on transforms downstream from CoGroupByKey. Users must change methods expecting a List to expect an Iterable going forward. See &lt;a href="https://docs.google.com/document/d/1RIzm8-g-0CyVsPb6yasjwokJQFoKHG4NjRUcKHKINu0">document&lt;/a> for information and fixes.&lt;/li>
&lt;li>The PortableRunner for Spark assumes Spark 3 as default Spark major version unless configured otherwise using &lt;code>--spark_version&lt;/code>.
Spark 2 support is deprecated and will be removed soon (&lt;a href="https://github.com/apache/beam/issues/23728">#23728&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Python cross-language JDBC IO Connector cannot read or write rows containing Numeric/Decimal type values (&lt;a href="https://github.com/apache/beam/issues/19817">#19817&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.43.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud
AlexZMLyu
Alexey Romanenko
Anand Inguva
Andrew Pilloud
Andy Ye
Arnout Engelen
Benjamin Gonzalez
Bharath Kumarasubramanian
BjornPrime
Brian Hulette
Bruno Volpato
Chamikara Jayalath
Colin Versteeg
Damon
Daniel Smilkov
Daniela Martín
Danny McCormick
Darkhan Nausharipov
David Huntsperger
Denis Pyshev
Dmitry Repin
Evan Galpin
Evgeny Antyshev
Fernando Morales
Geddy05
Harshit Mehrotra
Iñigo San Jose Visiers
Ismaël Mejía
Israel Herraiz
Jan Lukavský
Juta Staes
Kanishk Karanawat
Kenneth Knowles
KevinGG
Kiley Sok
Liam Miller-Cushon
Luke Cwik
Mc
Melissa Pashniak
Moritz Mack
Ning Kang
Pablo Estrada
Philippe Moussalli
Pranav Bhandari
Rebecca Szper
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Ryohei Nagao
Sam Rohde
Sam Whittle
Sanil Jain
Seunghwan Hong
Shane Hansen
Shubham Krishna
Shunsuke Otani
Steve Niemitz
Steven van Rossum
Svetak Sundhar
Thiago Nunes
Toran Sahu
Veronica Wasson
Vitaly Terentyev
Vladislav Chunikhin
Xinyu Liu
Yi Hu
Yixiao Shen
alexeyinkin
arne-alex
azhurkevich
bulat safiullin
bullet03
coldWater
dpcollins-google
egalpin
johnjcasey
liferoad
rvballada
shaojwu
tvalentyn&lt;/p></description></item><item><title>Blog: New Resources Available for Beam ML</title><link>/blog/ml-resources/</link><pubDate>Wed, 09 Nov 2022 00:00:01 -0800</pubDate><guid>/blog/ml-resources/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>If you&amp;rsquo;ve been paying attention, over the past year you&amp;rsquo;ve noticed that
Beam has released a number of features designed to make Machine Learning
easy. Ranging from things like the introduction of the &lt;code>RunInference&lt;/code>
transform to the continued refining of &lt;code>Beam Dataframes&lt;/code>, this has been
an area where we&amp;rsquo;ve seen Beam make huge strides. While development has
advanced quickly, however, until recently there has been a lack of
resources to help people discover and use these new features.&lt;/p>
&lt;p>Over the past several months, we&amp;rsquo;ve been hard at work building out
documentation and notebooks to make it easier to use these new features
and to show how Beam can be used to solve common Machine Learning problems.
We&amp;rsquo;re now happy to present this new and improved Beam ML experience!&lt;/p>
&lt;p>To get started, we encourage you to visit Beam&amp;rsquo;s new &lt;a href="/documentation/ml/overview/">AI/ML landing page&lt;/a>.
We&amp;rsquo;ve got plenty of content on things like &lt;a href="/documentation/ml/multi-model-pipelines/">multi-model pipelines&lt;/a>,
&lt;a href="/documentation/ml/runinference-metrics/">performing inference with metrics&lt;/a>,
&lt;a href="/documentation/ml/online-clustering/">online training&lt;/a>, and much more.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/ml-landing.png"
alt="ML landing page">&lt;/p>
&lt;p>We&amp;rsquo;ve also introduced a number of example &lt;a href="https://github.com/apache/beam/tree/master/examples/notebooks/beam-ml">Jupyter Notebooks&lt;/a>
showing how to use built in beam transforms like &lt;code>RunInference&lt;/code> and &lt;code>Beam Dataframes&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/ensemble-model-notebook.png"
alt="Example ensemble notebook with RunInference">&lt;/p>
&lt;p>Adding more examples and notebooks will be a point of emphasis going forward.
For our next round of improvements, we are planning on adding examples of
using RunInference with &amp;gt;30GB models, with multi-language pipelines, with
common Beam concepts, and with TensorRT. We will also add examples showing
other pieces of the Machine Learning lifecycle like model evaluation with TFMA,
per-entity training, and more online training.&lt;/p>
&lt;p>We hope you find this useful! As always, if you see any areas for improvement, please &lt;a href="https://github.com/apache/beam/issues/new/choose">open an issue&lt;/a>
or a &lt;a href="https://github.com/apache/beam/pulls">pull request&lt;/a>!&lt;/p></description></item><item><title>Blog: Beam starter projects</title><link>/blog/beam-starter-projects/</link><pubDate>Thu, 03 Nov 2022 09:00:00 -0700</pubDate><guid>/blog/beam-starter-projects/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We&amp;rsquo;re happy to announce that we&amp;rsquo;re providing new Beam starter projects! 🎉&lt;/p>
&lt;p>Setting up and configuring a new project can be time consuming, and varies in different languages. We hope this will make it easier for you to get started in creating new Apache Beam projects and pipelines.&lt;/p>
&lt;p>All the starter projects come in their own GitHub repository, so you can simply clone a repo and you&amp;rsquo;re ready to go. Each project comes with a README with how to use it, a simple &amp;ldquo;Hello World&amp;rdquo; pipeline, and a test for the pipeline. The GitHub repositories come pre-configured with GitHub Actions to automatically run tests when pull requests are opened or modified, and Dependabot is enabled to make sure all the dependencies are up to date. This all comes out of the box, so you can start playing with your Beam pipeline without a hassle.&lt;/p>
&lt;p>For example, here&amp;rsquo;s how to get started with Java:&lt;/p>
&lt;pre tabindex="0">&lt;code>git clone https://github.com/apache/beam-starter-java
cd beam-starter-java
# Install Java and Gradle with sdkman.
curl -s &amp;#34;https://get.sdkman.io&amp;#34; | bash
sdk install java 11.0.12-tem
sdk install gradle
# To run the pipeline.
gradle run
# To run the tests.
gradle test
&lt;/code>&lt;/pre>&lt;p>And here&amp;rsquo;s how to get started with Python:&lt;/p>
&lt;pre tabindex="0">&lt;code>git clone https://github.com/apache/beam-starter-python
cd beam-starter-python
# Set up a virtual environment with the dependencies.
python -m venv env
source env/bin/activate
pip install -r requirements.txt
# To run the pipeline.
python main.py
# To run the tests.
python -m unittest
&lt;/code>&lt;/pre>&lt;p>Here are the starter projects; you can choose your favorite language:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>[Java]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-java">github.com/apache/beam-starter-java&lt;/a> – Includes both Gradle and Maven configurations.&lt;/li>
&lt;li>&lt;strong>[Python]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-python">github.com/apache/beam-starter-python&lt;/a> – Includes a setup.py file to allow multiple files in your pipeline.&lt;/li>
&lt;li>&lt;strong>[Go]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-go">github.com/apache/beam-starter-go&lt;/a> – Includes how to register different types of functions for ParDo.&lt;/li>
&lt;li>&lt;strong>[Kotlin]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-kotlin">github.com/apache/beam-starter-kotlin&lt;/a> – Adapted to idiomatic Kotlin&lt;/li>
&lt;li>&lt;strong>[Scala]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-scala">github.com/apache/beam-starter-scala&lt;/a> – Coming soon!&lt;/li>
&lt;/ul>
&lt;p>We have updated the &lt;a href="/get-started/quickstart/java/">Java quickstart&lt;/a> to use the new starter project, and we&amp;rsquo;re working on updating the Python and Go quickstarts as well.&lt;/p>
&lt;p>We hope you find this useful. Feedback and contributions are always welcome! So feel free to create a GitHub issue, or open a Pull Request to any of the starter project repositories.&lt;/p></description></item><item><title>Blog: Apache Beam 2.42.0</title><link>/blog/beam-2.42.0/</link><pubDate>Mon, 17 Oct 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.42.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.42.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2420-2022-10-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.42.0, check out the &lt;a href="https://github.com/apache/beam/milestone/4?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added support for stateful DoFns to the Go SDK.&lt;/li>
&lt;li>Added support for &lt;a href="/documentation/programming-guide/#batched-dofns">Batched
DoFns&lt;/a>
to the Python SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added support for Zstd compression to the Python SDK.&lt;/li>
&lt;li>Added support for Google Cloud Profiler to the Go SDK.&lt;/li>
&lt;li>Added support for stateful DoFns to the Go SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Go SDK&amp;rsquo;s Row Coder now uses a different single-precision float encoding for float32 types to match Java&amp;rsquo;s behavior (&lt;a href="https://github.com/apache/beam/issues/22629">#22629&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Python cross-language JDBC IO Connector cannot read or write rows containing Timestamp type values &lt;a href="https://github.com/apache/beam/issues/19817">19817&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Go SDK doesn&amp;rsquo;t yet support Slowly Changing Side Input pattern (&lt;a href="https://github.com/apache/beam/issues/23106">#23106&lt;/a>)&lt;/li>
&lt;li>See a full list of open &lt;a href="https://github.com/apache/beam/milestone/4">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.42.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abirdcfly
Ahmed Abualsaud
Alexander Zhuravlev
Alexey Inkin
Alexey Romanenko
Anand Inguva
Andrej Galad
Andrew Pilloud
Andy Ye
Balázs Németh
Brian Hulette
Bruno Volpato
bulat safiullin
bullet03
Chamikara Jayalath
ChangyuLi28
Clément Guillaume
Damon
Danny McCormick
Darkhan Nausharipov
David Huntsperger
dpcollins-google
Evgeny Antyshev
grufino
Heejong Lee
Ismaël Mejía
Jack McCluskey
johnjcasey
Jonathan Shen
Kenneth Knowles
Ke Wu
Kiley Sok
Liam Miller-Cushon
liferoad
Lucas Nogueira
Luke Cwik
MakarkinSAkvelon
Manit Gupta
masahitojp
Michael Hu
Michel Davit
Moritz Mack
Naireen Hussain
nancyxu123
Nikhil Nadig
oborysevych
Pablo Estrada
Pranav Bhandari
Rajat Bhatta
Rebecca Szper
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Sergey Pronin
Shivam
Shunsuke Otani
Shunya Ueta
Steven Niemitz
Stuart
Svetak Sundhar
Valentyn Tymofieiev
Vitaly Terentyev
Vlad
Vladislav Chunikhin
Yichi Zhang
Yi Hu
Yixiao Shen&lt;/p></description></item><item><title>Blog: Apache Hop web version with Cloud Dataflow</title><link>/blog/hop-web-cloud/</link><pubDate>Sat, 15 Oct 2022 00:00:01 -0800</pubDate><guid>/blog/hop-web-cloud/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Hop is a codeless visual development environment for Apache Beam pipelines that
can run jobs in any Beam runner, such as Dataflow, Flink or Spark. &lt;a href="/blog/apache-hop-with-dataflow/">In a
previous post&lt;/a>, we
introduced the desktop version of Apache Hop. Hop also has a web environment,
Hop Web, that you can run from a container, so you don&amp;rsquo;t have to install
anything on your computer to use it.&lt;/p>
&lt;p>In this detailed tutorial, you access Hop through the internet using a web
browser and point to a container running in a virtual machine on Google
Cloud. That container will launch jobs in Dataflow and report back the results
of those jobs. Because we don&amp;rsquo;t want just anyone to access your Hop instance,
we’re going to secure it so that only you can access that virtual machine. The
following diagram illustrates the setup:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image2.png" alt="Architecture deployed with this tutorial">&lt;/p>
&lt;p>We will show how to do the deployment described previously, creating a web and
visual development environment that builds Beam pipelines using just a web
browser. When complete, you will have a secure web environment that you can use
to create pipelines with your web browser and launch them using Google Cloud
Dataflow.&lt;/p>
&lt;h2 id="what-do-you-need-to-run-this-example">What do you need to run this example?&lt;/h2>
&lt;p>We are using Google Cloud, so the first thing you need is a Google Cloud
project. If needed, you can sign up for the free trial of Google Cloud at
&lt;a href="https://cloud.google.com/free">https://cloud.google.com/free&lt;/a>.&lt;/p>
&lt;p>When you have a project, you can use &lt;a href="https://cloud.google.com/shell">Cloud
Shell&lt;/a> in your web browser with no additional
setup. In Cloud Shell, the Google Cloud SDK is automatically configured for your
project and credentials. That&amp;rsquo;s the option we use here. Alternatively, you can
configure the Google Cloud SDK in your local computer. For instructions, see
&lt;a href="https://cloud.google.com/sdk/docs/install">https://cloud.google.com/sdk/docs/install&lt;/a>.&lt;/p>
&lt;p>To open Cloud Shell, go to the [Google Cloud console]
(&lt;a href="http://console.cloud.google.com">http://console.cloud.google.com&lt;/a>), make sure your project is selected, and click
the Cloud Shell button &lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image1.png" alt="Cloud Shellbutton">. Cloud Shell opens,
and you can use it to run the commands shown in this post.&lt;/p>
&lt;p>The commands that we are going to use in the next steps are &lt;a href="https://gist.github.com/iht/6219b227424ada477462c7b9d9d93c57">available in a Gist
in Github&lt;/a>, just
in case you prefer to run that script instead of copying the commands from this
tutorial.&lt;/p>
&lt;h2 id="permissions-and-accounts">Permissions and accounts&lt;/h2>
&lt;p>When we run a Dataflow pipeline, we can use our personal Google Cloud
credentials to run the job. But Hop web will be running in a virtual machine,
and in Google Cloud, virtual machines run using service accounts as
credentials. So we need to make sure that we have a service account that has
permission to run Dataflow jobs.&lt;/p>
&lt;p>By default, virtual machines use the service account called &lt;em>Compute Engine
default service account&lt;/em>. For the sake of simplicity, we will use this
account. Still, we need to add some permissions to run Dataflow jobs with that
service account.&lt;/p>
&lt;p>First, let&amp;rsquo;s make sure that you have enabled all the required Google Cloud
APIs. &lt;a href="https://console.cloud.google.com/flows/enableapi?apiid=dataflow,compute_component,logging,storage_component,storage_api,bigquery,pubsub">Click this link to enable Dataflow, BigQuery and
Pub/Sub&lt;/a>,
which we’ll use in this workflow. The link takes you to your project in the
Google Cloud console, where you can enable the APIs.&lt;/p>
&lt;p>Let&amp;rsquo;s now give permissions to the VM account. First, find the ID of the service
account. Open Cloud Shell, and run the following command.&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud iam service-accounts list | grep compute
&lt;/code>&lt;/pre>&lt;p>The output is similar to the following, with &lt;code>&amp;lt;PROJECT_NUMBER&amp;gt;&lt;/code> replaced by your
project number:&lt;/p>
&lt;pre tabindex="0">&lt;code>EMAIL: &amp;lt;PROJECT_NUMBER&amp;gt;-compute@developer.gserviceaccount.com
&lt;/code>&lt;/pre>&lt;p>Copy that service account ID, because we use it in the next step. Run the
following command to grant the &lt;a href="https://cloud.google.com/dataflow/docs/concepts/access-control">Dataflow Admin
role&lt;/a> to the
service account. This role is required to run jobs:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member=&amp;#34;serviceAccount:&amp;lt;SERVICE_ACCOUNT_ID&amp;gt;&amp;#34; --role=&amp;#34;roles/dataflow.admin&amp;#34;
&lt;/code>&lt;/pre>&lt;p>where &lt;code>&amp;lt;SERVICE_ACCOUNT_ID&amp;gt;&lt;/code> is the ID that you retrieved previously. If you are
running these commands in Cloud Shell, the environment variable
&lt;code>GOOGLE_CLOUD_PROJECT&lt;/code> is already set to your project ID. If you are running
this from any other place, set the &lt;code>$GOOGLE_CLOUD_PROJECT&lt;/code> variable with the ID
of your project.&lt;/p>
&lt;p>Now your &amp;ldquo;user&amp;rdquo; for Dataflow is that service account. If your jobs are accessing
data in BigQuery, Cloud Storage, Pub/Sub, and so on, you also need to grant
roles for those services to the service account.&lt;/p>
&lt;h2 id="disk-and-virtual-machine">Disk and virtual machine&lt;/h2>
&lt;p>Let&amp;rsquo;s create a virtual machine (VM) in Compute Engine to run the Docker
container of Apache Hop.&lt;/p>
&lt;p>In Compute Engine, it is possible to run a container directly in a VM. There are
other options to run containers in Google Cloud, but a VM is probably the
simplest and most straightforward. The full details are in the &lt;a href="https://cloud.google.com/compute/docs/containers/deploying-containers">Deploying
containers on VMs and
MIGs&lt;/a>
page of the Google Cloud documentation.&lt;/p>
&lt;p>In this tutorial, we will always be working in the zone &lt;code>europe-west1-b&lt;/code>, so you
will see that zone in a lot of the commands. However, you can choose any Google
Cloud zone; just remember to use the value for your zone instead of
&lt;code>europe-west1-b&lt;/code>. Always use the same zone for all the resources, such as disks
and VMs. To minimize the latency when using Hop web, choose a zone that is
geographically close to your location. Let&amp;rsquo;s define the zone now and use this
variable for the rest of the commands:&lt;/p>
&lt;pre tabindex="0">&lt;code>ZONE=europe-west1-b
&lt;/code>&lt;/pre>&lt;p>Containers have ephemeral storage: when you restart the container, the disk of
the container returns to its original state. Therefore, if we restart the Hop
web container, we lose all our precious pipelines. To avoid that, we are going
to create a persistent disk, where we will store all our work with Hop web. Run
the following command to create the disk:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud compute disks create my-hop-disk \
--type=pd-balanced \
--size=10GB \
--zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>Thanks to this disk, we’re able to stop the virtual machine and still keep all
our personal files in Hop web intact.&lt;/p>
&lt;p>Let&amp;rsquo;s now create the VM. For the VM, we need to select the network (&lt;code>default&lt;/code> in
the, well, default case) so the VM will not have a public IP address. This is
important for security reasons, but it won’t stop us from using the VM from our
web browser thanks to the Identity Aware Proxy. More on this later; for now
let&amp;rsquo;s create the VM:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud compute instances create-with-container my-hop-vm \
--zone=$ZONE \
--network-interface=subnet=default,no-address \
--scopes=https://www.googleapis.com/auth/cloud-platform \
--tags=http-server,https-server,ssh \
--container-image=apache/hop-web:2.0.1 \
--container-restart-policy=on-failure \
--container-mount-disk=mode=rw,mount-path=/root,name=my-hop-disk,partition=0 \
--disk=boot=no,device-name=my-hop-disk,mode=rw,name=my-hop-disk
&lt;/code>&lt;/pre>&lt;p>You might be wondering what those additional options are. They are required for
the VM to work properly with Hop web. For instance, the &lt;code>scopes&lt;/code> option is what
allows the VM to use Dataflow, and the &lt;code>tags&lt;/code> option lets your browser reach the
Hop web address through the network firewall.&lt;/p>
&lt;p>Apache Hop listens on port 8080 for HTTP connections, so if you have additional
custom firewall rules in your project, make sure you are not stopping TCP
traffic on port 8080.&lt;/p>
&lt;p>But wait a minute; we have created a machine with only private IPs. How can we
reach Hop web from the web browser on our computer? Don&amp;rsquo;t we need a public IP
address for that?&lt;/p>
&lt;p>Google Cloud has a feature called the Identity Aware Proxy (IAP) that can be
used to wrap services with an authorization layer, allowing connections to
resources with only internal IPs.&lt;/p>
&lt;p>We can use the IAP to wrap our Apache Hop web server. With the following
command, we create a tunnel listening on local port 8080 that connects to port
8080 on the VM:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud compute start-iap-tunnel my-hop-vm 8080 --local-host-port=localhost:8080 --zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>To keep the tunnel open, leave that command running. If the command fails right
after creating the VM, wait a few seconds and try again; the container might
still be booting up.&lt;/p>
&lt;p>We now have a tunnel that we can connect to using our web browser. If you’re
running these commands on your local computer and not in Cloud Shell, point your
browser to &lt;code>localhost:8080&lt;/code>. The Hop UI should load.&lt;/p>
&lt;p>If you are running these command in Cloud Shell, where do we point the browser
to? Cloud Shell comes with an utility for situations like this one. In Cloud
Shell, locate the &lt;strong>Web Preview&lt;/strong> button:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image3.png" alt="Web preview options">&lt;/p>
&lt;p>If the preview isn’t using port 8080, click &lt;strong>Change port&lt;/strong>, and switch to
port 8080. When you click &lt;strong>Preview on port&lt;/strong>, Cloud Shell opens a new tab in
your browser that points to the tunnel address.&lt;/p>
&lt;p>The &lt;strong>Identity Aware Proxy&lt;/strong> will ask you to identify yourself using your Google
account.&lt;/p>
&lt;p>After that, the Apache Hop web interface loads:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image4.png" alt="Hop web UI">&lt;/p>
&lt;p>That URL is authenticated using your Google account, the same one that you are
using for Google Cloud (the one you are authenticated with in the Google Cloud
SDK). So even if another person gets that URL address, they won’t be able to
access your Apache Hop instance.&lt;/p>
&lt;p>You are now ready to use Apache Hop in a web browser!&lt;/p>
&lt;p>You can try to replicate the example that was given &lt;a href="/blog/apache-hop-with-dataflow/">in a previous
post&lt;/a> using Hop web, or
just try to launch any other project from the samples included with Hop:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image5.png" alt="Sample projects in Hop">&lt;/p>
&lt;h2 id="where-should-i-store-my-stuff">Where should I store my stuff?&lt;/h2>
&lt;p>The directories in the file system of a container are ephemeral. How can you be
sure that you store your pipelines and JARs in a persistent location?&lt;/p>
&lt;p>The home directory container is &lt;code>/root&lt;/code>, and it is the only &lt;strong>persistent&lt;/strong>
directory in the container (thanks to the disk we created previously). When you
restart the VM for whatever reason, any file included in that directory is
retained. But the rest of the directories reset to their original state. So make
sure you save your stuff, such as your pipelines, the fat JAR generated for
Dataflow, and so on, in the &lt;code>/root&lt;/code> directory or its subdirectories.&lt;/p>
&lt;p>In the Hop file dialogs, when you click the home icon, you are directed to the
&lt;code>/root&lt;/code> directory, so it is very straightforward to use it to store
everything. In the example in the picture, we clicked the &lt;strong>Home&lt;/strong> button and
are storing a JAR in that persistent directory:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image6.png" alt="Hop file dialog">&lt;/p>
&lt;h2 id="turning-off-the-virtual-machine">Turning off the virtual machine&lt;/h2>
&lt;p>If you want to save some money when you are not using the virtual machine, stop
the VM and launch it again when needed. The content of the &lt;em>/root&lt;/em> directory is
saved when you stop the virtual machine.&lt;/p>
&lt;p>To stop the VM, run the following command (or in the console, on the Compute
Engine VM page, click &lt;strong>Stop&lt;/strong>):&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud compute instances stop my-hop-vm --zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>And to start it again, run the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud compute instances start my-hop-vm --zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>Remember that you need to have the Identity Aware Proxy running in order to
access Hop web, so after starting the VM, don&amp;rsquo;t forget to run the command to
start the Identity Aware Proxy (and if it fails right after starting, wait a few
seconds and run it again):&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud compute start-iap-tunnel my-hop-vm 8080 --local-host-port=localhost:8080 --zone=$ZONE
&lt;/code>&lt;/pre>&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>This post has shown that all that you need to run Hop is a web browser. And,
well, a Google Cloud project too.&lt;/p>
&lt;p>We deployed the container to a virtual machine in Google Cloud, so you can
access Hop from anywhere, and we created a persistent disk, so you can have
permanent storage for your pipelines. Now you can use your web browser to create
your pipelines and to run Dataflow jobs without having to install anything
locally in your computer: not Java, not Docker, not the Google Cloud SDK;
nothing, just your favourite web browser.&lt;/p>
&lt;p>If you followed the instructions in this post, head over to the post &lt;a href="/blog/apache-hop-with-dataflow/">Running
Apache Hop visual pipelines with Google Cloud
Dataflow&lt;/a> to run a
Dataflow pipeline right from your web browser!&lt;/p></description></item><item><title>Blog: Apache Beam 2.41.0</title><link>/blog/beam-2.41.0/</link><pubDate>Tue, 23 Aug 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.41.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.41.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2410-2022-08-23">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.41.0, check out the &lt;a href="https://github.com/apache/beam/milestone/3?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Projection Pushdown optimizer is now on by default for streaming, matching the behavior of batch pipelines since 2.38.0. If you encounter a bug with the optimizer, please file an issue and disable the optimizer using pipeline option &lt;code>--experiments=disable_projection_pushdown&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Previously available in Java sdk, Python sdk now also supports logging level overrides per module. (&lt;a href="https://github.com/apache/beam/issues/18222">#18222&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Projection Pushdown optimizer may break Dataflow upgrade compatibility for optimized pipelines when it removes unused fields. If you need to upgrade and encounter a compatibility issue, disable the optimizer using pipeline option &lt;code>--experiments=disable_projection_pushdown&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Support for Spark 2.4.x is deprecated and will be dropped with the release of Beam 2.44.0 or soon after (Spark runner) (&lt;a href="https://github.com/apache/beam/issues/22094">#22094&lt;/a>).&lt;/li>
&lt;li>The modules &lt;a href="https://github.com/apache/beam/tree/master/sdks/java/io/amazon-web-services">amazon-web-services&lt;/a> and
&lt;a href="https://github.com/apache/beam/tree/master/sdks/java/io/kinesis">kinesis&lt;/a> for AWS Java SDK v1 are deprecated
in favor of &lt;a href="https://github.com/apache/beam/tree/master/sdks/java/io/amazon-web-services2">amazon-web-services2&lt;/a>
and will be eventually removed after a few Beam releases (Java) (&lt;a href="https://github.com/apache/beam/issues/21249">#21249&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed a condition where retrying queries would yield an incorrect cursor in the Java SDK Firestore Connector (&lt;a href="https://github.com/apache/beam/issues/22089">#22089&lt;/a>).&lt;/li>
&lt;li>Fixed plumbing allowed lateness in Go SDK. It was ignoring the user set value earlier and always used to set to 0. (&lt;a href="https://github.com/apache/beam/issues/22474">#22474&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://github.com/apache/beam/milestone/3">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.41.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud
Ahmet Altay
akashorabek
Alexey Inkin
Alexey Romanenko
Anand Inguva
andoni-guzman
Andrew Pilloud
Andrey
Andy Ye
Balázs Németh
Benjamin Gonzalez
BjornPrime
Brian Hulette
bulat safiullin
bullet03
Byron Ellis
Chamikara Jayalath
Damon Douglas
Daniel Oliveira
Daniel Thevessen
Danny McCormick
David Huntsperger
Dheeraj Gharde
Etienne Chauchot
Evan Galpin
Fernando Morales
Heejong Lee
Jack McCluskey
johnjcasey
Kenneth Knowles
Ke Wu
Kiley Sok
Liam Miller-Cushon
Lucas Nogueira
Luke Cwik
MakarkinSAkvelon
Manu Zhang
Minbo Bae
Moritz Mack
Naireen Hussain
Ning Kang
Oleh Borysevych
Pablo Estrada
pablo rodriguez defino
Pranav Bhandari
Rebecca Szper
Red Daly
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Steven Niemitz
Valentyn Tymofieiev
Vincent Marquez
Vitaly Terentyev
Vlad
Vladislav Chunikhin
Yichi Zhang
Yi Hu
yirutang
Yixiao Shen
Yu Feng&lt;/p></description></item><item><title>Blog: Big Improvements in Beam Go's 2.40 Release</title><link>/blog/go-2.40/</link><pubDate>Wed, 06 Jul 2022 00:00:01 -0800</pubDate><guid>/blog/go-2.40/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The 2.40 release is one of Beam Go&amp;rsquo;s biggest yet, and we wanted to highlight
some of the biggest changes coming with this important release!&lt;/p>
&lt;h1 id="native-streaming-support">Native Streaming Support&lt;/h1>
&lt;p>2.40 marks the release of one of our most anticipated feature sets yet:
native streaming Go pipelines. This includes adding support for:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="/documentation/programming-guide/#user-initiated-checkpoint">Self Checkpointing&lt;/a>&lt;/li>
&lt;li>&lt;a href="/documentation/programming-guide/#watermark-estimation">Watermark Estimation&lt;/a>&lt;/li>
&lt;li>&lt;a href="/documentation/programming-guide/#truncating-during-drain">Pipeline Drain/Truncation&lt;/a>&lt;/li>
&lt;li>&lt;a href="/documentation/programming-guide/#bundle-finalization">Bundle Finalization&lt;/a> (added in 2.39)&lt;/li>
&lt;/ul>
&lt;p>With all of these features, it is now possible to write your own streaming
pipeline source DoFns in Go without relying on cross-language transforms
from Java or Python. We encourage you to try out all of these new features
in your streaming pipelines! The &lt;a href="/documentation/programming-guide/#splittable-dofns">programming guide&lt;/a>
has additional information on getting started with native Go streaming DoFns.&lt;/p>
&lt;h1 id="generic-registration-make-your-pipelines-3x-faster">Generic Registration (Make Your Pipelines 3x Faster)&lt;/h1>
&lt;p>The release of &lt;a href="https://go.dev/blog/intro-generics">Go Generics&lt;/a> in Go 1.18
unlocked significant performance improvements for Beam Go. With generics,
we were able to add simple registration functions that can massively reduce
your pipeline&amp;rsquo;s runtime and resource consumption. For example, registering
the ParDo&amp;rsquo;s in our load tests which are designed to simulate a basic pipeline
reduced execution time from around 25 minutes to around 7 minutes on average&lt;/p>
&lt;ul>
&lt;li>an over 70% reduction!&lt;/li>
&lt;/ul>
&lt;p>&lt;img class="center-block"
src="/images/blog/go-registration.png"
alt="Beam Registration Load Tests ParDo Improvements">&lt;/p>
&lt;p>To get started with registering your own DoFns and unlocking these performance
gains, check out the &lt;a href="https://pkg.go.dev/github.com/apache/beam/sdks/go/pkg/beam/register">registration doc page&lt;/a>.&lt;/p>
&lt;h1 id="whats-next">What&amp;rsquo;s Next?&lt;/h1>
&lt;p>Moving forward, we remain focused on improving the streaming experience and
leveraging generics to improve the SDK. Specific improvements we are considering
include adding &lt;a href="/documentation/programming-guide/#state-and-timers">State &amp;amp; Timers&lt;/a>
support, introducing a Go expansion service so that Go DoFns can be used in other
languages, and wrapping more Java and Python IOs so that they can be easily used
in Go. As always, please let us know what changes you would like to see by
&lt;a href="https://github.com/apache/beam/issues/new/choose">filing an issue&lt;/a>,
&lt;a href="dev@beam.apache.org">emailing the dev list&lt;/a>, or starting a &lt;a href="https://app.slack.com/client/T4S1WH2J3/C9H0YNP3P">slack thread&lt;/a>!&lt;/p></description></item><item><title>Blog: Apache Beam 2.40.0</title><link>/blog/beam-2.40.0/</link><pubDate>Sat, 25 Jun 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.40.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.40.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2400-2022-06-25">download page&lt;/a> for this
release.&lt;/p>
&lt;p>For more information on changes in 2.40.0 check out the &lt;a href="https://github.com/apache/beam/releases/tag/v2.40.0">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added &lt;a href="https://s.apache.org/inference-sklearn-pytorch">RunInference&lt;/a> API, a framework agnostic transform for inference. With this release, PyTorch and Scikit-learn are supported by the transform.
See also example at apache_beam/examples/inference/pytorch_image_classification.py&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Upgraded to Hive 3.1.3 for HCatalogIO. Users can still provide their own version of Hive. (Java) (&lt;a href="https://github.com/apache/beam/issues/19554">Issue-19554&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Go SDK users can now use generic registration functions to optimize their DoFn execution. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14347">BEAM-14347&lt;/a>)&lt;/li>
&lt;li>Go SDK users may now write self-checkpointing Splittable DoFns to read from streaming sources. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11104">BEAM-11104&lt;/a>)&lt;/li>
&lt;li>Go SDK textio Reads have been moved to Splittable DoFns exclusively. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14489">BEAM-14489&lt;/a>)&lt;/li>
&lt;li>Pipeline drain support added for Go SDK has now been tested. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11106">BEAM-11106&lt;/a>)&lt;/li>
&lt;li>Go SDK users can now see heap usage, sideinput cache stats, and active process bundle stats in Worker Status. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13829">BEAM-13829&lt;/a>)&lt;/li>
&lt;li>The serialization (pickling) library for Python is dill==0.3.1.1 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11167">BEAM-11167&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Go Sdk now requires a minimum version of 1.18 in order to support generics (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14347">BEAM-14347&lt;/a>).&lt;/li>
&lt;li>synthetic.SourceConfig field types have changed to int64 from int for better compatibility with Flink&amp;rsquo;s use of Logical types in Schemas (Go) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14173">BEAM-14173&lt;/a>)&lt;/li>
&lt;li>Default coder updated to compress sources used with &lt;code>BoundedSourceAsSDFWrapperFn&lt;/code> and &lt;code>UnboundedSourceAsSDFWrapper&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Java expansion service to allow specific files to stage (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14160">BEAM-14160&lt;/a>).&lt;/li>
&lt;li>Fixed Elasticsearch connection when using both ssl and username/password (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14000">BEAM-14000&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python&amp;rsquo;s &lt;code>beam.FlatMap&lt;/code> will raise &lt;code>AttributeError: 'builtin_function_or_method' object has no attribute '__func__'&lt;/code> when
constructed with some
&lt;a href="https://docs.python.org/3/library/functions.html">built-ins&lt;/a>, like &lt;code>sum&lt;/code>
and &lt;code>len&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/22091">#22091&lt;/a>).&lt;/li>
&lt;li>Java&amp;rsquo;s &lt;code>BigQueryIO.Write&lt;/code> can have an exception where it attempts to output a timestamp beyond the max timestamp range
&lt;code>Cannot output with timestamp 294247-01-10T04:00:54.776Z. Output timestamps must be no earlier than the timestamp of the current input or timer (294247-01-10T04:00:54.776Z) minus the allowed skew (0 milliseconds) and no later than 294247-01-10T04:00:54.775Z. See the DoFn#getAllowedTimestampSkew() Javadoc for details on changing the allowed skew.&lt;/code>
This happens when a sink is idle, causing the idle timeout to trigger, or when a specific table is idle long enough when using dynamic destinations.
When this happens, the job is no longer able to be drained. This has been fixed for the 2.41 release.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.40.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud
Ahmet Altay
Aizhamal Nurmamat kyzy
Alejandro Rodriguez-Morantes
Alexander Zhuravlev
Alexey Romanenko
Anand Inguva
andoni-guzman
Andy Ye
Balázs Németh
Benjamin Gonzalez
Brian Hulette
bulat safiullin
bullet03
Chamikara Jayalath
Damon Douglas
Daniel Oliveira
Danny McCormick
Darkhan Nausharipov
David Huntsperger
Diego Gomez
dpcollins-google
Ekaterina Tatanova
Elias Segundo
Etienne Chauchot
Evan Galpin
fbeevikm
Fernando Morales
Heejong Lee
Igor Krasavin
Ilion Beyst
Israel Herraiz
Jack McCluskey
Jan Kuehle
Jan Lukavský
johnjcasey
Jonathan Lui
jrmccluskey
Julien Tournay
Kenneth Knowles
Kerry Donny-Clark
Kevin Puthusseri
Kiley Sok
Kyle Weaver
kynx
Lucas Nogueira
Luke Cwik
LuNing Wang
Marco Robles
masahitojp
Minbo Bae
Moritz Mack
Naireen Hussain
Nancy Xu
Niel Markwick
Ning Kang
nishant jain
nishantjain91
Oskar Firlej
Pablo Estrada
pablo rodriguez defino
Rebecca Szper
Red Daly
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Thiago Nunes
Tom Stepp
vachan-shetty
Valentyn Tymofieiev
vikash2310
Vitaly Terentyev
Vladislav Chunikhin
Yichi Zhang
Yi Hu
Yiru Tang
yixiaoshen
zwestrick&lt;/p></description></item><item><title>Blog: Apache Beam 2.39.0</title><link>/blog/beam-2.39.0/</link><pubDate>Wed, 25 May 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.39.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.39.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2390-2022-05-25">download page&lt;/a> for this
release.&lt;/p>
&lt;p>For more information on changes in 2.39.0 check out the &lt;a href="https://issues.apache.org/jira/secure/ConfigureReleaseNote.jspa?projectId=12319527&amp;amp;version=12351170">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>JmsIO gains the ability to map any kind of input to any subclass of &lt;code>javax.jms.Message&lt;/code> (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>).&lt;/li>
&lt;li>JmsIO introduces the ability to write to dynamic topics (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>).
&lt;ul>
&lt;li>A &lt;code>topicNameMapper&lt;/code> must be set to extract the topic name from the input value.&lt;/li>
&lt;li>A &lt;code>valueMapper&lt;/code> must be set to convert the input value to JMS message.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Reduce number of threads spawned by BigqueryIO StreamingInserts (
&lt;a href="https://issues.apache.org/jira/browse/BEAM-14283">BEAM-14283&lt;/a>).&lt;/li>
&lt;li>Implemented Apache PulsarIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8218">BEAM-8218&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Support for flink scala 2.12, because most of the libraries support version 2.12 onwards. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14386">beam-14386&lt;/a>)&lt;/li>
&lt;li>&amp;lsquo;Manage Clusters&amp;rsquo; JupyterLab extension added for users to configure usage of Dataproc clusters managed by Interactive Beam (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14130">BEAM-14130&lt;/a>).&lt;/li>
&lt;li>Pipeline drain support added for Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11106">BEAM-11106&lt;/a>). &lt;strong>Note: this feature is not yet fully validated and should be treated as experimental in this release.&lt;/strong>&lt;/li>
&lt;li>&lt;code>DataFrame.unstack()&lt;/code>, &lt;code>DataFrame.pivot() &lt;/code> and &lt;code>Series.unstack()&lt;/code>
implemented for DataFrame API (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13948">BEAM-13948&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13966">BEAM-13966&lt;/a>).&lt;/li>
&lt;li>Support for impersonation credentials added to dataflow runner in the Java and Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14014">BEAM-14014&lt;/a>).&lt;/li>
&lt;li>Implemented Jupyterlab extension for managing Dataproc clusters (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14130">BEAM-14130&lt;/a>).&lt;/li>
&lt;li>ExternalPythonTransform API added for easily invoking Python transforms from
Java (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14143">BEAM-14143&lt;/a>).&lt;/li>
&lt;li>Added Add support for Elasticsearch 8.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14003">BEAM-14003&lt;/a>).&lt;/li>
&lt;li>Shard aware Kinesis record aggregation (AWS Sdk v2), (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14104">BEAM-14104&lt;/a>).&lt;/li>
&lt;li>Upgrade to ZetaSQL 2022.04.1 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14348">BEAM-14348&lt;/a>).&lt;/li>
&lt;li>Fixed ReadFromBigQuery cannot be used with the interactive runner (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14112">BEAM-14112&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Unused functions &lt;code>ShallowCloneParDoPayload()&lt;/code>, &lt;code>ShallowCloneSideInput()&lt;/code>, and &lt;code>ShallowCloneFunctionSpec()&lt;/code> have been removed from the Go SDK&amp;rsquo;s pipelinex package (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13739">BEAM-13739&lt;/a>).&lt;/li>
&lt;li>JmsIO requires an explicit &lt;code>valueMapper&lt;/code> to be set (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>). You can use the &lt;code>TextMessageMapper&lt;/code> to convert &lt;code>String&lt;/code> inputs to JMS &lt;code>TestMessage&lt;/code>s:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl"> &lt;span class="n">JmsIO&lt;/span>&lt;span class="o">.&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">write&lt;/span>&lt;span class="o">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="na">withConnectionFactory&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">jmsConnectionFactory&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">.&lt;/span>&lt;span class="na">withValueMapper&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">TextMessageMapper&lt;/span>&lt;span class="o">());&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Coders in Python are expected to inherit from Coder. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14351">BEAM-14351&lt;/a>).&lt;/li>
&lt;li>New abstract method &lt;code>metadata()&lt;/code> added to io.filesystem.FileSystem in the
Python SDK. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14314">BEAM-14314&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Flink 1.11 is no longer supported (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14139">BEAM-14139&lt;/a>).&lt;/li>
&lt;li>Python 3.6 is no longer supported (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13657">BEAM-13657&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Java Spanner IO NPE when ProjectID not specified in template executions (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14405">BEAM-14405&lt;/a>).&lt;/li>
&lt;li>Fixed potential NPE in BigQueryServicesImpl.getErrorInfo (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14133">BEAM-14133&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/browse/BEAM-14412?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.39.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.39.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud,
Ahmet Altay,
Aizhamal Nurmamat kyzy,
Alexander Zhuravlev,
Alexey Romanenko,
Anand Inguva,
Andrei Gurau,
Andrew Pilloud,
Andy Ye,
Arun Pandian,
Arwin Tio,
Aydar Farrakhov,
Aydar Zainutdinov,
AydarZaynutdinov,
Balázs Németh,
Benjamin Gonzalez,
Brian Hulette,
Buqian Zheng,
Chamikara Jayalath,
Chun Yang,
Daniel Oliveira,
Daniela Martín,
Danny McCormick,
David Huntsperger,
Deepak Nagaraj,
Denise Case,
Esun Kim,
Etienne Chauchot,
Evan Galpin,
Hector Miuler Malpica Gallegos,
Heejong Lee,
Hengfeng Li,
Ilango Rajagopal,
Ilion Beyst,
Israel Herraiz,
Jack McCluskey,
Kamil Bregula,
Kamil Breguła,
Ke Wu,
Kenneth Knowles,
KevinGG,
Kiley,
Kiley Sok,
Kyle Weaver,
Liam Miller-Cushon,
Luke Cwik,
Marco Robles,
Matt Casters,
Michael Li,
MiguelAnzoWizeline,
Milan Patel,
Minbo Bae,
Moritz Mack,
Nick Caballero,
Niel Markwick,
Ning Kang,
Oskar Firlej,
Pablo Estrada,
Pavel Avilov,
Reuven Lax,
Reza Rokni,
Ritesh Ghorse,
Robert Bradshaw,
Robert Burke,
Ryan Thompson,
Sam Whittle,
Steven Niemitz,
Thiago Nunes,
Tomo Suzuki,
Valentyn Tymofieiev,
Victor,
Yi Hu,
Yichi Zhang,
Yiru Tang,
ahmedabu98,
andoni-guzman,
brachipa,
bulat safiullin,
bullet03,
dannymartinm,
daria.malkova,
dpcollins-google,
egalpin,
emily,
fbeevikm,
johnjcasey,
kileys,
&lt;a href="mailto:msbukal@google.com">msbukal@google.com&lt;/a>,
nguyennk92,
pablo rodriguez defino,
rszper,
rvballada,
sachinag,
tvalentyn,
vachan-shetty,
yirutang&lt;/p></description></item><item><title>Blog: Running Beam SQL in notebooks</title><link>/blog/beam-sql-with-notebooks/</link><pubDate>Thu, 28 Apr 2022 00:00:01 -0800</pubDate><guid>/blog/beam-sql-with-notebooks/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>&lt;a href="/documentation/dsls/sql/overview/">Beam SQL&lt;/a> allows a
Beam user to query PCollections with SQL statements.
&lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/runners/interactive#interactive-beam">Interactive Beam&lt;/a>
provides an integration between Apache Beam and
&lt;a href="https://docs.jupyter.org/en/latest/">Jupyter Notebooks&lt;/a> (formerly known as
IPython Notebooks) to make pipeline prototyping and data exploration much faster
and easier.
You can set up your own notebook user interface (for example,
&lt;a href="https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html">JupyterLab&lt;/a>
or classic &lt;a href="https://docs.jupyter.org/en/latest/install.html">Jupyter Notebooks&lt;/a>)
on your own device following their documentations. Alternatively, you can
choose a hosted solution that does everything for you. You are free to select
whichever notebook user interface you prefer. For simplicity, this
post does not go through the notebook environment setup and uses
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development">Apache Beam Notebooks&lt;/a>
that provides a cloud-hosted
&lt;a href="https://jupyterlab.readthedocs.io/en/stable/">JupyterLab&lt;/a> environment and lets
a Beam user iteratively develop pipelines, inspect pipeline graphs, and parse
individual PCollections in a read-eval-print-loop (REPL) workflow.&lt;/p>
&lt;p>In this post, you will see how to use &lt;code>beam_sql&lt;/code>, a notebook
&lt;a href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">magic&lt;/a>, to
execute Beam SQL in notebooks and inspect the results.&lt;/p>
&lt;p>By the end of the post, it also demonstrates how to use the &lt;code>beam_sql&lt;/code> magic
with a production environment, such as running it as a one-shot job on
Dataflow. It&amp;rsquo;s optional. To follow those steps, you should have a project in
Google Cloud Platform with
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#before_you_begin">necessary APIs enabled&lt;/a>
, and you should have enough permissions to create a Google Cloud Storage bucket
(or to use an existing one), query a public Google Cloud BigQuery dataset, and
run Dataflow jobs.&lt;/p>
&lt;p>If you choose to use the cloud hosted notebook solution, once you have your
Google Cloud project ready, you will need to create an Apache Beam Notebooks
instance and open the JupyterLab web interface. Please follow the instructions
given at:
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#launching_an_notebooks_instance">https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#launching_an_notebooks_instance&lt;/a>&lt;/p>
&lt;h2 id="getting-familiar-with-the-environment">Getting familiar with the environment&lt;/h2>
&lt;h3 id="landing-page">Landing page&lt;/h3>
&lt;p>After starting your own notebook user interface: for example, if using Apche
Beam Notebooks, after clicking the &lt;code>OPEN JUPYTERLAB&lt;/code> link, you will land on
the default launcher page of the notebook environment.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image1.png"
alt="Beam SQL in Notebooks: landing page">&lt;/p>
&lt;p>On the left side, there is a file explorer to view examples, tutorials and
assets on the notebook instance. To easily navigate the files, you may
double-click the &lt;code>00-Start_Here.md&lt;/code> (#1 in the screenshot) file to view detailed
information about the files.&lt;/p>
&lt;p>On the right side, it displays the default launcher page of JupyterLab. To
create and open a completely new notebook file and code with a selected version
of Apache Beam, click one of (#2) the items with Apache Beam &amp;gt;=2.34.0 (because
&lt;code>beam_sql&lt;/code> was introduced in 2.34.0) installed.&lt;/p>
&lt;h3 id="createopen-a-notebook">Create/open a notebook&lt;/h3>
&lt;p>For example, if you clicked the image button with Apache Beam 2.36.0, you would
see an &lt;code>Untitled.ipynb&lt;/code> file created and opened.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image2.png"
alt="Beam SQL in Notebooks: create/open a notebook ">&lt;/p>
&lt;p>In the file explorer, your new notebook file has been created as
&lt;code>Untitled.ipynb&lt;/code>.&lt;/p>
&lt;p>On the right side, in the opened notebook, there are 4 buttons on top that you
may interact most frequently with:&lt;/p>
&lt;ul>
&lt;li>#1: insert an empty code block after the selected / highlighted code block&lt;/li>
&lt;li>#2: execute the code in the block that is selected / highlighted&lt;/li>
&lt;li>#3: interrupt code execution if your code execution is stuck&lt;/li>
&lt;li>#4: “Restart the kernel”: clear all states from code executions and start
from fresh&lt;/li>
&lt;/ul>
&lt;p>There is a button on the top-right (#5) for you to choose a different Apache
Beam version if needed, so it’s not set in stone.&lt;/p>
&lt;p>You can always double-click a file from the file explorer to open it without
creating a new one.&lt;/p>
&lt;h2 id="beam-sql">Beam SQL&lt;/h2>
&lt;h3 id="beam_sql-magic">&lt;code>beam_sql&lt;/code> magic&lt;/h3>
&lt;p>&lt;code>beam_sql&lt;/code> is an IPython
&lt;a href="https://ipython.readthedocs.io/en/stable/config/custommagics.html">custom magic&lt;/a>.
If you&amp;rsquo;re not familiar with magics, here are some
&lt;a href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">built-in examples&lt;/a>.
It&amp;rsquo;s a convenient way to validate your queries locally against known/test data
sources when prototyping a Beam pipeline with SQL, before productionizing it on
remote cluster/services.&lt;/p>
&lt;p>The Apache Beam Notebooks environment has preloaded the &lt;code>beam_sql&lt;/code> magic and
basic &lt;code>apache-beam&lt;/code> modules so you can directly use them without additional
imports. You can also explicitly load the magic via
&lt;code>%load_ext apache_beam.runners.interactive.sql.beam_sql_magics&lt;/code> and
&lt;code>apache-beam&lt;/code> modules if you set up your own notebook elsewhere.&lt;/p>
&lt;p>You can type:&lt;/p>
&lt;pre tabindex="0">&lt;code>%beam_sql -h
&lt;/code>&lt;/pre>&lt;p>and then execute the code to learn how to use the magic:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image3.png"
alt="Beam SQL in Notebooks: beam_sql magic help message ">&lt;/p>
&lt;p>The selected/highlighted block is called a notebook cell. It mainly has 3
components:&lt;/p>
&lt;ul>
&lt;li>#1: The execution count. &lt;code>[1]&lt;/code> indicates this block is the first executed
code. It increases by 1 for each piece of code you execute even if you
re-execute the same piece of code. &lt;code>[ ]&lt;/code> indicates this block is not
executed.&lt;/li>
&lt;li>#2: The cell input: the code gets executed.&lt;/li>
&lt;li>#3: The cell output: the output of the code execution. Here it contains the
help documentation of the &lt;code>beam_sql&lt;/code> magic.&lt;/li>
&lt;/ul>
&lt;h3 id="create-a-pcollection">Create a PCollection&lt;/h3>
&lt;p>There are 3 scenarios for Beam SQL when creating a PCollection:&lt;/p>
&lt;ol>
&lt;li>Use Beam SQL to create a PCollection from constant values&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o pcoll
SELECT CAST(1 AS INT) AS id, CAST(&amp;#39;foo&amp;#39; AS VARCHAR) AS str, CAST(3.14 AS DOUBLE) AS flt
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image4.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from raw values.">&lt;/p>
&lt;p>The &lt;code>beam_sql&lt;/code> magic creates and outputs a PCollection named &lt;code>pcoll&lt;/code> with
element_type like &lt;code>BeamSchema_...(id: int32, str: str, flt: float64)&lt;/code>.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> that you have &lt;strong>not&lt;/strong> explicitly created a Beam pipeline. You get a
PCollection because the &lt;code>beam_sql&lt;/code> magic always implicitly creates a pipeline to
execute your SQL query. To hold the elements with each field&amp;rsquo;s type info, Beam
automatically creates a
&lt;a href="/documentation/programming-guide/#what-is-a-schema">schema&lt;/a>
as the &lt;code>element_type&lt;/code> for the created PCollection. You will learn more about
schema-aware PCollections later.&lt;/p>
&lt;ol start="2">
&lt;li>Use Beam SQL to query a PCollection&lt;/li>
&lt;/ol>
&lt;p>You can chain another SQL using the output from a previous SQL (or any
schema-aware PCollection produced by any normal Beam PTransforms) as the input
to produce a new PCollection.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: if you name the output PCollection, make sure that it’s unique in your
notebook to avoid overwriting a different PCollection.&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o id_pcoll
SELECT id FROM pcoll
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image5.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from another.">&lt;/p>
&lt;ol start="3">
&lt;li>Use Beam SQL to join multiple PCollections&lt;/li>
&lt;/ol>
&lt;p>You can query multiple PCollections from a single query.&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o str_with_same_id
SELECT id, str FROM pcoll JOIN id_pcoll USING (id)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image6.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from multiple PCollections.">&lt;/p>
&lt;p>Now you have learned how to use the &lt;code>beam_sql&lt;/code> magic to create PCollections and
inspect their results.&lt;/p>
&lt;p>&lt;strong>Tip&lt;/strong>: if you accidentally delete some of the notebook cell outputs, you can
always check the content of a PCollection by invoking &lt;code>ib.show(pcoll_name)&lt;/code> or
&lt;code>ib.collect(pcoll_name)&lt;/code> where &lt;code>ib&lt;/code> stands for “Interactive Beam”
(&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#reading_and_visualizing_the_data">learn more&lt;/a>).&lt;/p>
&lt;h3 id="schema-aware-pcollections">Schema-aware PCollections&lt;/h3>
&lt;p>The &lt;code>beam_sql&lt;/code> magic provides the flexibility to seamlessly mix SQL and non-SQL
Beam statements to build pipelines and even run them on Dataflow. However, each
PCollection queried by Beam SQL needs to have a
&lt;a href="/documentation/programming-guide/#what-is-a-schema">schema&lt;/a>.
For the &lt;code>beam_sql&lt;/code> magic, it’s recommended to use &lt;code>typing.NamedTuple&lt;/code> when a
schema is desired. You can go through the below example to learn more details
about schema-aware PCollections.&lt;/p>
&lt;h4 id="setup">Setup&lt;/h4>
&lt;p>In the setup of this example, you will:&lt;/p>
&lt;ul>
&lt;li>Install PyPI package &lt;code>names&lt;/code> using the built-in &lt;code>%pip&lt;/code> magic: you will use
the module to generate some random English names as the raw data input.&lt;/li>
&lt;li>Define a schema with &lt;code>NamedTuple&lt;/code> that has 2 attributes: &lt;code>id&lt;/code> - an unique
numeric identifier of a person; &lt;code>name&lt;/code> - a string name of a person.&lt;/li>
&lt;li>Define a pipeline with an &lt;code>InteractiveRunner&lt;/code> to utilize notebook related
features of Apache Beam.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="o">%&lt;/span>&lt;span class="n">pip&lt;/span> &lt;span class="n">install&lt;/span> &lt;span class="n">names&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">names&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">NamedTuple&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">id&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">name&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There is no visible output for the code execution.&lt;/p>
&lt;h4 id="create-schema-aware-pcollections-without-using-sql">Create schema-aware PCollections without using SQL&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">persons&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">names&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_full_name&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">)]))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">persons&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image7.png"
alt="Beam SQL in Notebooks: create a schema-aware PCollection without SQL.">&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">persons_2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">names&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_full_name&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">15&lt;/span>&lt;span class="p">)]))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">persons_2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image8.png"
alt="Beam SQL in Notebooks: create another schema-aware PCollection without SQL.">&lt;/p>
&lt;p>Now you have 2 PCollections both with the same schema defined by the &lt;code>Person&lt;/code>
class:&lt;/p>
&lt;ul>
&lt;li>&lt;code>persons&lt;/code> contains 10 records for 10 persons with ids ranging from 0 to 9,&lt;/li>
&lt;li>&lt;code>persons_2&lt;/code> contains another 10 records for 10 persons with ids ranging from
5 to 14.&lt;/li>
&lt;/ul>
&lt;h4 id="encode-and-decode-of-schema-aware-pcollections">Encode and Decode of schema-aware PCollections&lt;/h4>
&lt;p>For this example, you still need one more piece of data from the first &lt;code>pcoll&lt;/code>
that you have created with instructions in this post.&lt;/p>
&lt;p>You can use the original &lt;code>pcoll&lt;/code>. Optionally, if you want to exercise using
coders explicitly with schema-aware PCollections, you can add a Text I/O into
the mix: write the content of &lt;code>pcoll&lt;/code> into a text file retaining its schema
information, then read the file back into a new schema-aware PCollection called
&lt;code>pcoll_in_file&lt;/code>, and use the new PCollection to join &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code>
to find names with the common id in all three of them.&lt;/p>
&lt;p>To encode &lt;code>pcoll&lt;/code> into a file, execute:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">pcoll&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">textio&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;/tmp/pcoll&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pipeline&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">wait_until_finish&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">!&lt;/span>&lt;span class="n">cat&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">tmp&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="o">*&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image9.png"
alt="Beam SQL in Notebooks: write a schema-aware PCollection into a text file.">&lt;/p>
&lt;p>The above code execution writes the PCollection &lt;code>pcoll&lt;/code> (basically
&lt;code>{id: 1, str: foo, flt: 3.14}&lt;/code>) into a text file using the coder assigned by
Beam. As you can see, the file content is recorded in a binary non
human-readable format, and that’s normal.&lt;/p>
&lt;p>To decode the file content into a new PCollection, execute:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">pcoll_in_file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;/tmp/pcoll*&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll_in_file&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image10.png"
alt="Beam SQL in Notebooks: read a schema-aware PCollection from a text file.">&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> you have to use the same coder during encoding and decoding, and
furthermore you may assign the schema explicitly to the new PCollection through
&lt;code>with_output_types()&lt;/code>.&lt;/p>
&lt;p>Reading out the encoded binary content from the text file and decoding it with
the correct coder, the content of &lt;code>pcoll&lt;/code> is recovered into &lt;code>pcoll_in_file&lt;/code>. You
can use this technique to save and share your data through any Beam I/O (not
necessarily a text file) with collaborators who work on their own pipelines (not
just in your notebook session or pipelines).&lt;/p>
&lt;h4 id="schema-in-beam_sql-magic">Schema in &lt;code>beam_sql&lt;/code> magic&lt;/h4>
&lt;p>The &lt;code>beam_sql&lt;/code> magic automatically registers a &lt;code>RowCoder&lt;/code> for your &lt;code>NamedTuple&lt;/code>
schema so that you only need to focus on preparing your data for query without
worrying about coders. To see more verbose details of what the &lt;code>beam_sql&lt;/code> magic
does behind the scenes, you can use the &lt;code>-v&lt;/code> option.&lt;/p>
&lt;p>For example, you can look for all elements with &lt;code>id &amp;lt; 5&lt;/code> in &lt;code>persons&lt;/code> with the
below query and assign the output to &lt;code>persons_id_lt_5&lt;/code>.&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o persons_id_lt_5 -v
SELECT * FROM persons WHERE id &amp;lt; 5
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image11.png"
alt="Beam SQL in Notebooks: beam_sql registers a schema for a PCollection.">&lt;/p>
&lt;p>Since this is the first time running this query, you might see a warning message
about:&lt;/p>
&lt;blockquote>
&lt;p>Schema Person has not been registered to use a RowCoder. Automatically
registering it by running:
beam.coders.registry.register_coder(Person, beam.coders.RowCoder)&lt;/p>
&lt;/blockquote>
&lt;p>The &lt;code>beam_sql&lt;/code> magic helps registering a &lt;code>RowCoder&lt;/code> for each schema you define
and use whenever it finds one. You can also explicitly run the same code to do
so.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> the output element type is &lt;code>Person(id: int, name: str)&lt;/code> instead of
&lt;code>BeamSchema_…&lt;/code> because you have selected all the fields from a single
PCollection of the known type &lt;code>Person(id: int, name: str)&lt;/code>.&lt;/p>
&lt;p>Another example, you can query for all names from &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code> with
the same ids and assign the output to &lt;code>persons_with_common_id&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o persons_with_common_id -v
SELECT * FROM persons JOIN persons_2 USING (id)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image12.png"
alt="Beam SQL in Notebooks: beam_sql creates a schema for a query.">&lt;/p>
&lt;p>Note the output element type is now some
&lt;code>BeamSchema_...(id: int64, name: str, name0: str)&lt;/code>. Because you have selected
columns from both PCollections, there is no known schema to hold the result.
Beam automatically creates a schema and differentiates the conflicted field
&lt;code>name&lt;/code> by suffixing 0 to one of them.&lt;/p>
&lt;p>And since &lt;code>Person&lt;/code> is already previously registered with a &lt;code>RowCoder&lt;/code>, there is
no more warning about registering it even with the &lt;code>-v&lt;/code> option.&lt;/p>
&lt;p>Additionally, you can do a join with &lt;code>pcoll_in_file&lt;/code>, &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o entry_with_common_id
SELECT pcoll_in_file.id, persons.name AS name_1, persons_2.name AS name_2
FROM pcoll_in_file JOIN persons ON pcoll_in_file.id = persons.id
JOIN persons_2 ON pcoll_in_file.id = persons_2.id
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image13.png"
alt="Beam SQL in Notebooks: rename fields in a query.">&lt;/p>
&lt;p>The schema generated reflects the column renaming you have done in the SQL.&lt;/p>
&lt;h2 id="an-example">An Example&lt;/h2>
&lt;p>You will go through an example to find out the US state with the most COVID
positive cases on a specific day with data provided by the
&lt;a href="https://covidtracking.com/">covid tracking project&lt;/a>.&lt;/p>
&lt;h3 id="get-the-data">Get the data&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">json&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">requests&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># The covidtracking project has stopped collecting new data, current data ends on 2021-03-07&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">json_current&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;https://covidtracking.com/api/v1/states/current.json&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">get_json_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">with&lt;/span> &lt;span class="n">requests&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">json&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">loads&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">current_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_json_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">json_current&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">current_data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image14.png"
alt="Beam SQL in Notebooks: preview example data.">&lt;/p>
&lt;p>The data is dated as 2021-03-07. It contains many details about COVID cases for
different states in the US. &lt;code>current_data[0]&lt;/code> is just one of the data points.&lt;/p>
&lt;p>You can get rid of most of the columns of the data. For example, just focus on
“date”, “state”, “positive” and “negative”, and then define a schema
&lt;code>UsCovidData&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Optional&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">partition_date&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="c1"># Remember to str(e[&amp;#39;date&amp;#39;]).&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">positive&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">negative&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Note&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>date&lt;/code> is a keyword in (Calcite)SQL, use a different field name such as
&lt;code>partition_date&lt;/code>;&lt;/li>
&lt;li>&lt;code>date&lt;/code> from the data is an &lt;code>int&lt;/code> type, not &lt;code>str&lt;/code>. Make sure you convert the
data using &lt;code>str()&lt;/code> or use &lt;code>date: int&lt;/code>.&lt;/li>
&lt;li>&lt;code>negative&lt;/code> has missing values and the default is &lt;code>None&lt;/code>. So instead of
&lt;code>negative: int&lt;/code>, it should be &lt;code>negative: Optional[int]&lt;/code>. Or you can convert
&lt;code>None&lt;/code> into 0 when using the schema.&lt;/li>
&lt;/ul>
&lt;p>Then parse the json data into a PCollection with the schema:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">p_sql&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">runner&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">covid_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p_sql&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Create PCollection from json&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">current_data&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Parse&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">lambda&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">partition_date&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;date&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">positive&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;positive&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">negative&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;negative&amp;#39;&lt;/span>&lt;span class="p">]))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">covid_data&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image15.png"
alt="Beam SQL in Notebooks: parse example data with a schema.">&lt;/p>
&lt;h3 id="query">Query&lt;/h3>
&lt;p>You can now find the biggest positive on the “current day” (2021-03-07).&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o max_positive
SELECT partition_date, MAX(positive) AS positive
FROM covid_data
GROUP BY partition_date
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image16.png"
alt="Beam SQL in Notebooks: find the biggest positive from the data.">&lt;/p>
&lt;p>However, this is just the positive number. You cannot observe the state that has
this maximum number nor the negative case number for the state.&lt;/p>
&lt;p>To enrich your result, you have to join this data back to the original data set
you have parsed.&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o entry_with_max_positive
SELECT covid_data.partition_date, covid_data.state, covid_data.positive, {fn IFNULL(covid_data.negative, 0)} AS negative
FROM covid_data JOIN max_positive
USING (partition_date, positive)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image17.png"
alt="Beam SQL in Notebooks: enriched data with biggest positive.">&lt;/p>
&lt;p>Now you can see all columns of the data with the maximum positive case on
2021-03-07.
&lt;strong>Note&lt;/strong>: to handle missing values of the negative column in the original data,
you can use &lt;code>{fn IFNULL(covid_data.negative, 0)}&lt;/code> to set null values to 0.&lt;/p>
&lt;p>When you&amp;rsquo;re ready to scale up, you can translate the SQLs into a pipeline with
&lt;code>SqlTransform&lt;/code>s and run your pipeline on a distributed runner like Flink or
Spark. This post demonstrates it by launching a one-shot job on Dataflow from
the notebook with the help of &lt;code>beam_sql&lt;/code> magic.&lt;/p>
&lt;h3 id="run-on-dataflow">Run on Dataflow&lt;/h3>
&lt;p>Now that you have a pipeline that parses US COVID data from json to find
positive/negative/state information for the state with the most positive cases
on each day, you can try applying it to all historical daily data and running it
on Dataflow.&lt;/p>
&lt;p>The new data source you will use is a public dataset from USAFacts US
Coronavirus Database that contains all historical daily summary of COVID cases
in the US.&lt;/p>
&lt;p>The schema of data is very similar to what the covid tracking project website
provides. The fields you will query are: &lt;code>date&lt;/code>, &lt;code>state&lt;/code>, &lt;code>confirmed_cases&lt;/code>, and
&lt;code>deaths&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image18.png"
alt="Beam SQL in Notebooks: schema of cloud data.">&lt;/p>
&lt;p>A preview of the data looks like below (you may skip the inspection in BigQuery
and just take a look at the screenshot):&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image19.png"
alt="Beam SQL in Notebooks: preview of cloud data.">&lt;/p>
&lt;p>The format of the data is &lt;strong>slightly different&lt;/strong> from the json data you parsed
in the previous pipeline because the numbers are grouped by counties instead of
states, thus some additional aggregations need to be done in the SQLs.&lt;/p>
&lt;p>If you need a fresh execution, you may click the “Restart the kernel” button on
the top menu.&lt;/p>
&lt;p>Full code is as below, on-top of the original pipeline and queries:&lt;/p>
&lt;ul>
&lt;li>It changes the source from a single-day data to a more complete historical
data;&lt;/li>
&lt;li>It changes the I/O and schema to accommodate the new dataset;&lt;/li>
&lt;li>It changes the SQLs to include more aggregations to accommodate the new
format of the dataset.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Prepare the data with schema&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">NamedTuple&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Optional&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Public BQ dataset.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">table&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;bigquery-public-data:covid19_usafacts.summary&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Replace with your project.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">project&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;YOUR-PROJECT-NAME-HERE&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Replace with your GCS bucket.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gcs_location&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;gs://YOUR_GCS_BUCKET_HERE&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">partition_date&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">confirmed_cases&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">deaths&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p_on_dataflow&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">runner&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">covid_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p_on_dataflow&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Read dataset&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromBigQuery&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">project&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">project&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">table&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">gcs_location&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">gcs_location&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Parse&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">lambda&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">partition_date&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;date&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">confirmed_cases&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">deaths&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">])))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Run on Dataflow&lt;/strong>&lt;/p>
&lt;p>To run SQL on Dataflow is very simple, you just need to add the option
&lt;code>-r DataflowRunner&lt;/code>.&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o data_by_state -r DataflowRunner
SELECT partition_date, state, SUM(confirmed_cases) as confirmed_cases, SUM(deaths) as deaths
FROM covid_data
GROUP BY partition_date, state
&lt;/code>&lt;/pre>&lt;p>Different from previous &lt;code>beam_sql&lt;/code> magic executions, you won’t see the result
immediately. Instead, a form like below is printed in the notebook cell output:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image20.png"
alt="Beam SQL in Notebooks: empty run-on-dataflow form.">&lt;/p>
&lt;p>The &lt;code>beam_sql&lt;/code> magic tries its best to guess your project id and preferred cloud
region. You still have to input additional information necessary to submit a
Dataflow job, such as a GCS bucket to stage the Dataflow job and any additional
Python dependencies the job needs.&lt;/p>
&lt;p>For now, ignore the form in the cell output, because you still need 2 more SQLs
to: 1) find the maximum confirmed cases on each day; 2) join the maximum case
data with the full data_by_state. The &lt;code>beam_sql&lt;/code> magic allows you to chain SQLs,
so chain 2 more by executing:&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o max_cases -r DataflowRunner
SELECT partition_date, MAX(confirmed_cases) as confirmed_cases
FROM data_by_state
GROUP BY partition_date
&lt;/code>&lt;/pre>&lt;p>And&lt;/p>
&lt;pre tabindex="0">&lt;code>%%beam_sql -o data_with_max_cases -r DataflowRunner
SELECT data_by_state.partition_date, data_by_state.state, data_by_state.confirmed_cases, data_by_state.deaths
FROM data_by_state JOIN max_cases
USING (partition_date, confirmed_cases)
&lt;/code>&lt;/pre>&lt;p>By default, when running &lt;code>beam_sql&lt;/code> on Dataflow, the output PCollection will be
written to a text file on GCS. The “write” is automatically provided by
&lt;code>beam_sql&lt;/code> and mainly for your inspection of the output data for this one-shot
Dataflow job. It’s lightweight and does not encode elements for further
development. To save the output and share it with others, you can add more Beam
I/Os into the mix.&lt;/p>
&lt;p>For example, you can appropriately encode elements into text files using the
technique described in the above schema-aware PCollections example.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.options.pipeline_options&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">GoogleCloudOptions&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">coder&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">data_with_max_cases&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">max_data_file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">gcs_location&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="s1">&amp;#39;/encoded_max_data&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">data_with_max_cases&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">textio&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">max_data_file&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Furthermore, you can create a new BQ dataset in your own project to store the
processed data.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image21.png"
alt="Beam SQL in Notebooks: create a new BQ dataset.">&lt;/p>
&lt;p>You have to select the same data location as the public BigQuery data you are
reading. In this case, “us (multiple regions in United States)”.&lt;/p>
&lt;p>Once you finish creating an empty dataset, you can execute below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">output_table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">project&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s1">:covid_data.max_analysis&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">bq_schema&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;fields&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;partition_date&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;STRING&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;STRING&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;INTEGER&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;INTEGER&amp;#39;&lt;/span>&lt;span class="p">}]}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="n">data_with_max_cases&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;To json-like&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;partition_date&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">partition_date&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">state&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">confirmed_cases&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">deaths&lt;/span>&lt;span class="p">})&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToBigQuery&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">output_table&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">schema&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bq_schema&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">method&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;STREAMING_INSERTS&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">custom_gcs_temp_location&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">gcs_location&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now back in the form of the last SQL cell output, you may fill in necessary
information to run the pipeline on Dataflow. An example input looks like below:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image22.png"
alt="Beam SQL in Notebooks: fill in the run-on-Dataflow form.">&lt;/p>
&lt;p>Because this pipeline doesn’t use any additional Python dependency, “Additional
Packages” is left empty. In the previous example where you have installed a
package called &lt;code>names&lt;/code>, to run that pipeline on Dataflow, you have to put
&lt;code>names&lt;/code> in this field.&lt;/p>
&lt;p>Once you finish updating your inputs, you can click the &lt;code>Show Options&lt;/code> button to
view what pipeline options have been configured based on your inputs. A variable
&lt;code>options_[YOUR_OUTPUT_PCOLL_NAME]&lt;/code> is generated, and you can supply more
pipeline options to it if the form is not enough for your execution.&lt;/p>
&lt;p>Once you are ready to submit the Dataflow job, click the &lt;code>Run on Dataflow&lt;/code>
button. It tells you where the default output would be written, and after a
while, a line with:&lt;/p>
&lt;blockquote>
&lt;p>Click here for the details of your Dataflow job.&lt;/p>
&lt;/blockquote>
&lt;p>would be displayed. You can click on the hyperlink to go to your Dataflow job
page. (Optionally, you can ignore the form and continue development to extend
your pipeline. Once you are satisfied with the state of your pipeline, you can
come back to the form and submit the job to Dataflow.)&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image23.png"
alt="Beam SQL in Notebooks: a Dataflow job graph.">&lt;/p>
&lt;p>As you can see, each transform name of the generated Dataflow job is prefixed
with a string &lt;code>[number]: &lt;/code>. This is to distinguish re-executed codes in
notebooks because Beam requires each transform to have a distinct name. Under
the hood, the &lt;code>beam_sql&lt;/code> magic also stages your schema information to Dataflow,
so you might see transforms named as &lt;code>schema_loaded_beam_sql_…&lt;/code>. This is because
the &lt;code>NamedTuple&lt;/code> defined in the notebook is likely in the &lt;code>__main__&lt;/code> scope and
Dataflow is not aware of them at all. To minimize user intervention and avoid
pickling the whole main session (and it’s infeasible to pickle the main session
when it contains unpickle-able attributes), the &lt;code>beam_sql&lt;/code> magic optimizes the
staging process by serializing your schemas, staging them to Dataflow, and then
deserialize/load them for job execution.&lt;/p>
&lt;p>Once the job succeeds, the result of the output PCollection would be written to
places instructed by your I/O transforms. &lt;strong>Note&lt;/strong>: running &lt;code>beam_sql&lt;/code> on
Dataflow generates a one-shot job and it’s not interactive.&lt;/p>
&lt;p>A simple inspection of the data from the default output location:&lt;/p>
&lt;pre tabindex="0">&lt;code>!gsutil cat &amp;#39;gs://ningk-so-test/bq/staging/data_with_max_cases*&amp;#39;
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image24.png"
alt="Beam SQL in Notebooks: inspect the default output file.">&lt;/p>
&lt;p>The text file with encoded binary data written by your &lt;code>WriteToText&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>!gsutil cat &amp;#39;gs://ningk-so-test/bq/encoded_max_data*&amp;#39;
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image25.png"
alt="Beam SQL in Notebooks: inspect the user-defined output file.">&lt;/p>
&lt;p>The table &lt;code>YOUR-PROJECT:covid_data.max_analysis&lt;/code> created by your
&lt;code>WriteToBigQuery&lt;/code>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image26.png"
alt="Beam SQL in Notebooks: inspect the output BQ dataset.">&lt;/p>
&lt;h3 id="run-on-other-oss-runners-directly-with-the-beam_sql-magic">Run on other OSS runners directly with the &lt;code>beam_sql&lt;/code> magic&lt;/h3>
&lt;p>On the day this blog is posted, the &lt;code>beam_sql&lt;/code> magic only supports DirectRunner
(interactive) and DataflowRunner (one-shot). It&amp;rsquo;s a simple wrapper on top of
the &lt;code>SqlTransform&lt;/code> with interactive input widgets implemented by
&lt;a href="https://ipywidgets.readthedocs.io/en/stable/">ipywidgets&lt;/a>. You can implement
your own runner support or utilities by following the
&lt;a href="https://lists.apache.org/thread/psrx1xhbyjcqbhxx6trf5nvh66c6pk3y">instructions&lt;/a>.&lt;/p>
&lt;p>Additionally, support for other OSS runners are WIP, for example,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-14373">support using FlinkRunner with the &lt;code>beam_sql&lt;/code> magic&lt;/a>.&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>The &lt;code>beam_sql&lt;/code> magic and Apache Beam Notebooks combined is a convenient tool for
you to learn Beam SQL and mix Beam SQL into prototyping and productionizing (
e.g., to Dataflow) your Beam pipelines with minimum setups.&lt;/p>
&lt;p>For more details about the Beam SQL syntax, check out the Beam Calcite SQL
&lt;a href="/documentation/dsls/sql/calcite/overview/">compatibility&lt;/a>
and the Apache Calcite SQL
&lt;a href="https://calcite.apache.org/docs/reference.html">syntax&lt;/a>.&lt;/p></description></item><item><title>Blog: Running Apache Hop visual pipelines with Google Cloud Dataflow</title><link>/blog/apache-hop-with-dataflow/</link><pubDate>Fri, 22 Apr 2022 00:00:01 -0800</pubDate><guid>/blog/apache-hop-with-dataflow/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>Apache Hop (&lt;a href="https://hop.apache.org/">https://hop.apache.org/&lt;/a>) is a visual development environment for creating data pipelines using Apache Beam. You can run your Hop pipelines in Spark, Flink or Google Cloud Dataflow.&lt;/p>
&lt;p>In this post, we will see how to install Hop, and we will run a sample pipeline in the cloud with Dataflow. To follow the steps given in this post, you should have a project in Google Cloud Platform, and you should have enough permissions to create a Google Cloud Storage bucket (or to use an existing one), as well as to run Dataflow jobs.&lt;/p>
&lt;p>Once you have your Google Cloud project ready, you will need to &lt;a href="https://cloud.google.com/sdk/docs/install">install the Google Cloud SDK&lt;/a> to trigger the Dataflow pipeline.&lt;/p>
&lt;p>Also, don&amp;rsquo;t forget to configure the Google Cloud SDK to use your account and your project.&lt;/p>
&lt;h2 id="setup-and-local-execution">Setup and local execution&lt;/h2>
&lt;p>You can run Apache Hop as a local application, or use &lt;a href="https://hop.incubator.apache.org/manual/latest/hop-gui/hop-web.html">the Hop web version&lt;/a> from a Docker container. The instructions given in this post will work for the local application, as the authentication for Cloud Dataflow would be different if Hop is running in a container. All the rest of the instructions remain valid. The UI of Hop is exactly the same either running as a local app or in the web version.&lt;/p>
&lt;p>Now it&amp;rsquo;s time to download and install Apache Hop, following these &lt;a href="https://hop.apache.org/manual/latest/getting-started/hop-download-install.html">instructions&lt;/a>.&lt;/p>
&lt;p>For this post, I have used the binaries in the apache-hop-client package, version 1.2.0, released on March 7th, 2022.&lt;/p>
&lt;p>After installing Hop, we are ready to start.&lt;/p>
&lt;p>The Zip file contains a directory &lt;code>config&lt;/code> where you will find some sample projects and some pipeline run configuration for Dataflow and other runners.&lt;/p>
&lt;p>For this example, we are going to use the pipeline located in &lt;code>config/projects/samples/beam/pipelines/input-process-output.hpl.&lt;/code>&lt;/p>
&lt;p>Let&amp;rsquo;s start by opening Apache Hop. In the directory where you have unzipped the client, run&lt;/p>
&lt;p>&lt;code>./hop/hop-gui.sh&lt;/code>&lt;/p>
&lt;p>(or &lt;code>./hop/hop-gui.bat&lt;/code> if you are on Windows).&lt;/p>
&lt;p>Once we are in Hop, let&amp;rsquo;s open the pipeline.&lt;/p>
&lt;p>We first switch from the project &lt;code>default&lt;/code> to the project &lt;code>samples&lt;/code>. Locate the &lt;code>projects&lt;/code> box in the top left corner of the window, and select the project &lt;code>samples&lt;/code>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image18.png"
alt="Apache Hop projects">&lt;/p>
&lt;p>Now we click the open button:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image4.png"
alt="Apache Hop open project">&lt;/p>
&lt;p>Select the pipeline &lt;code>input-process-output.hpl&lt;/code> in the &lt;code>beam/pipelines&lt;/code> subdirectory:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image12.png"
alt="Apache Hop select pipeline">&lt;/p>
&lt;p>You should see a graph like the following in the main window of Hop:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image17.png"
alt="Apache Hop main window">&lt;/p>
&lt;p>This pipeline takes some customer data from a CSV file and filters out everything but the records with the column &lt;code>stateCode&lt;/code> equal to &lt;code>CA.&lt;/code>&lt;/p>
&lt;p>Then we select only some of the columns of the file, and the result is written to Google Cloud Storage.&lt;/p>
&lt;p>It is always a good idea to test the pipeline locally before submitting it to Dataflow. In Apache Hop, you can preview the output of each transform. Let&amp;rsquo;s have a look at the input &lt;code>Customers&lt;/code>.&lt;/p>
&lt;p>Click in the &lt;code>Customers&lt;/code> input transform and then in &lt;em>Preview Output&lt;/em> in the dialog box that opens after selecting the transform:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image10.png"
alt="Apache Hop Customers preview">&lt;/p>
&lt;p>Now select the option &lt;em>Quick launch&lt;/em> and you will see some of the input data:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image24.png"
alt="Apache Hop input data">&lt;/p>
&lt;p>Click &lt;em>Stop&lt;/em> when you finish reviewing the data.&lt;/p>
&lt;p>If we repeat the process right after the &lt;code>Only CA&lt;/code> transform, we will see that all the rows have the &lt;code>stateCode&lt;/code> column equal to &lt;code>CA&lt;/code>.&lt;/p>
&lt;p>The next transform selects only some of the columns of the input data and reorders the columns. Let&amp;rsquo;s have a look. Click the transform and then &lt;em>Preview Output&lt;/em>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image15.png"
alt="Apache Hop preview output">&lt;/p>
&lt;p>Then click _Quick Launch _again, and you should see output like the following:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image8.png"
alt="Apache Hop output">&lt;/p>
&lt;p>The column &lt;code>id&lt;/code> is now the first, and we see only a subset of the input columns. This is how the data will look once the pipeline finishes writing the full output.&lt;/p>
&lt;h2 id="using-the-beam-direct-runner">Using the Beam Direct Runner&lt;/h2>
&lt;p>Let&amp;rsquo;s run the pipeline. To run the pipeline, we need to specify a runner configuration. This is done through the Metadata tool of Apache Hop:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image6.png"
alt="Apache Hop runner configuration">&lt;/p>
&lt;p>In the &lt;code>samples&lt;/code> project, there are already several configurations created:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image9.png"
alt="Apache Hop configurations">&lt;/p>
&lt;p>The &lt;code>local&lt;/code> configuration is the one used to run the pipeline using Hop. For instance, this is the configuration that we used when we examined the previews of the output of different steps.&lt;/p>
&lt;p>The &lt;code>Direct&lt;/code> configuration uses the direct runner of Apache Beam. Let&amp;rsquo;s examine what it looks like. There are two tabs in the Pipeline Run Configurations: main and variables.&lt;/p>
&lt;p>For the direct runner, the main tab has the following options:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image28.png"
alt="Apache Hop direct runner">&lt;/p>
&lt;p>We can change the number of workers settings to match our number of CPUs, or even limit it just to 1 so the pipeline does not consume a lot of resources.&lt;/p>
&lt;p>In the variables tab, we find the configuration parameters for the pipeline itself (not for the runner): \&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image14.png"
alt="Apache Hop variables tab">&lt;/p>
&lt;p>For this pipeline, only the &lt;code>DATA_INPUT&lt;/code> and &lt;code>DATA_OUTPUT&lt;/code> variables are used. The &lt;code>STATE_INPUT&lt;/code> is used in a different example.&lt;/p>
&lt;p>If you go to the Beam transforms in the input and output nodes of the pipeline, you will see how these variables are used there:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image29.png"
alt="Apache Hop variables">&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image11.png"
alt="Apache Hop variables">&lt;/p>
&lt;p>Since those variables are correctly set up to point to the location of data in the samples project folder, let&amp;rsquo;s try to run the pipeline using the Beam Direct Runner.&lt;/p>
&lt;p>For that, we need to go back to the pipeline view (arrow button just above the Metadata tool), and click the run button (the small &amp;ldquo;play&amp;rdquo; button in the toolbar). Then choose the Direct pipeline run configuration, and click the &lt;em>Launch&lt;/em> button:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image20.png"
alt="Apache Hop launch">&lt;/p>
&lt;p>How do you know if the job has finished or not? You can check the logs at the bottom of the main window for that. You should see something like this:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image19.png"
alt="Apache Hop completed job">&lt;/p>
&lt;p>If we go to the location set by &lt;code>DATA_OUTPUT&lt;/code>, in our case &lt;code>config/projects/samples/beam/output&lt;/code>, we should see some output files there. In my case, I see these files:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image26.png"
alt="Apache Hop output files">&lt;/p>
&lt;p>The number of files depends on the number of workers that you have set in the run configuration.&lt;/p>
&lt;p>Great, so the pipeline works locally. It is time to run it in the cloud!&lt;/p>
&lt;h2 id="running-at-cloud-scale-with-dataflow">Running at cloud scale with Dataflow&lt;/h2>
&lt;p>Let&amp;rsquo;s have a look at the Dataflow Pipeline Run Configuration. Go to the metadata tool, then to Pipeline Run Configuration and select Dataflow:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image30.png"
alt="Apache Hop Pipeline Run Configuration">&lt;/p>
&lt;p>We have again the Main and the Variables tab. We will need to change some values in both. Let&amp;rsquo;s start with the Variables. Click the Variables tab, and you should see the following values:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image3.png"
alt="Apache Hop Variables tab">&lt;/p>
&lt;p>Those are Google Cloud Storage (GCS) locations that belong to the author of that sample project. We need to change them to point to our own GCS bucket.&lt;/p>
&lt;h2 id="project-setup-in-google-cloud">Project setup in Google Cloud&lt;/h2>
&lt;p>But for that, we will have to create a bucket. For the next step, you need to make sure that you have configured gcloud (the Google Cloud SDK), and that you have managed to authenticate.&lt;/p>
&lt;p>To double check, run the command &lt;code>gcloud config list&lt;/code> and check if the account and the project look correct. If they do, let&amp;rsquo;s triple check and run &lt;code>gcloud auth login&lt;/code>. That should open a tab in your web browser, to do the authentication process. Once you have done that, you can interact with your project using the SDK.&lt;/p>
&lt;p>For this example, I will use the region europe-west1 of GCP. Let&amp;rsquo;s create a regional bucket there. In my case, I am using the name &lt;code>ihr-apache-hop-blog&lt;/code> for the bucket name. Choose a different name for your bucket!&lt;/p>
&lt;pre tabindex="0">&lt;code>gsutil mb -c regional -l europe-west1 gs://ihr-apache-hop-blog
&lt;/code>&lt;/pre>&lt;p>Now let&amp;rsquo;s upload the sample data to the GCS bucket, to test how the pipeline would run in Dataflow. Go to the same directory where you have all the hop files (the same directory that &lt;code>hop-gui.sh&lt;/code> is in), and let&amp;rsquo;s copy the data to GCS:&lt;/p>
&lt;pre tabindex="0">&lt;code> gsutil cp config/projects/samples/beam/input/customers-noheader-1k.txt gs://ihr-apache-hop-blog/data/
&lt;/code>&lt;/pre>&lt;p>Notice the final slash &lt;code>/&lt;/code> in the path, indicating that you want to create a directory of name &lt;code>data&lt;/code>, with all the contents.&lt;/p>
&lt;p>To make sure that you have uploaded the data correctly, check the contents of that location:&lt;/p>
&lt;pre tabindex="0">&lt;code>gsutil ls gs://ihr-apache-hop-blog/data/
&lt;/code>&lt;/pre>&lt;p>You should see the file &lt;code>customer-noheader-1k.txt&lt;/code> in that location.&lt;/p>
&lt;p>Before we continue, make sure that Dataflow is enabled in your project, and that you have a service account ready to be used with Hop. Please check the instructions given at the documentation of Dataflow, in the &lt;em>&lt;a href="https://cloud.google.com/dataflow/docs/quickstarts/create-pipeline-java#before-you-begin">Before you begin section&lt;/a>&lt;/em> to see how to enable the API for Dataflow.&lt;/p>
&lt;p>Now we need to make sure that Hop can use the necessary credentials for accessing Dataflow. In the Hop documentation, you will find that it recommends creating a service account, exporting a key for that service account, and setting the GOOGLE_APPLICATION_CREDENTIALS environment variable. This is also the method given in the above link.&lt;/p>
&lt;p>Exporting the key of a service account is potentially dangerous, so we are going to use a different method, by leveraging the Google Cloud SDK. Run the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud auth application-default login
&lt;/code>&lt;/pre>&lt;p>That will open a tab in your web browser asking to confirm the authentication. Once you have confirmed, any application in your system that needs to access Google Cloud Platform will use those credentials for that access.&lt;/p>
&lt;p>We need also to create a service account for the Dataflow job, with certain permissions. Create the service account with&lt;/p>
&lt;pre tabindex="0">&lt;code>​​gcloud iam service-accounts create dataflow-hop-sa
&lt;/code>&lt;/pre>&lt;p>And now we give permissions to this service account for Dataflow:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud projects add-iam-policy-binding ihr-hop-playground \
--member=&amp;#34;serviceAccount:dataflow-hop-sa@ihr-hop-playground.iam.gserviceaccount.com&amp;#34;\
--role=&amp;#34;roles/dataflow.worker&amp;#34;
&lt;/code>&lt;/pre>&lt;p>We also need to give additional permissions for Google Cloud Storage:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud projects add-iam-policy-binding ihr-hop-playground \
--member=&amp;#34;serviceAccount:dataflow-hop-sa@ihr-hop-playground.iam.gserviceaccount.com&amp;#34;\
--role=&amp;#34;roles/storage.admin&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Make sure that you change the project id &lt;code>ihr-hop-playground&lt;/code> to your own project id.&lt;/p>
&lt;p>Now let&amp;rsquo;s give permissions to our user to impersonate that service account. For that, go to &lt;a href="https://console.cloud.google.com/iam-admin/serviceaccounts">Service Accounts in the Google Cloud Console&lt;/a> in your project, and click on the service account we have just created.&lt;/p>
&lt;p>Click on the &lt;em>Permissions&lt;/em> tab and then in the &lt;em>Grant Access&lt;/em> button:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image21.png"
alt="Apache Hop Permissions">&lt;/p>
&lt;p>Give your user the role &lt;em>Service Account User&lt;/em>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image13.png"
alt="Apache Hop Service Account User">&lt;/p>
&lt;p>You are now all set to be able to run Dataflow with that service account and your user.&lt;/p>
&lt;h2 id="updating-the-pipeline-run-configuration">Updating the Pipeline Run Configuration&lt;/h2>
&lt;p>Before we can run a pipeline in Dataflow, we need to generate the JAR package for the pipeline code. For that, you have to go to the &lt;em>Tools&lt;/em> menu (in the menu bar), and choose the option &lt;em>Generate a Hop fat jar&lt;/em>. Click ok in the dialog, and then select a location and filename for the jar, and click &lt;em>Save&lt;/em>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image5.png"
alt="Apache Hop Tools menu">&lt;/p>
&lt;p>It will take some minutes to generate the file:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image22.png"
alt="Apache Hop generate file">&lt;/p>
&lt;p>We are ready to run the pipeline in Dataflow. Or almost :).&lt;/p>
&lt;p>Go the pipeline editor, click the play button, and select &lt;em>DataFlow&lt;/em> as Pipeline run configuration, and then click the play button on the right side:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image7.png"
alt="Apache Hop pipeline editor">&lt;/p>
&lt;p>That will open the Dataflow Pipeline Run Configuration, where we can change the input variables, and other Dataflow settings.&lt;/p>
&lt;p>Click on the &lt;em>Variables&lt;/em> tab and modify only the &lt;code>DATA_INPUT&lt;/code> and &lt;code>DATA_OUTPUT&lt;/code> variables.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image2.png"
alt="Apache Hop Variables tab">&lt;/p>
&lt;p>Notice that we also need to change the filename.&lt;/p>
&lt;p>Let&amp;rsquo;s go now to the &lt;em>Main&lt;/em> tab, because there are some other options that we need to change there. We need to update:&lt;/p>
&lt;ul>
&lt;li>Project id&lt;/li>
&lt;li>Service account&lt;/li>
&lt;li>Staging location&lt;/li>
&lt;li>Region&lt;/li>
&lt;li>Temp location&lt;/li>
&lt;li>Fat jar file location&lt;/li>
&lt;/ul>
&lt;p>For project id, set your project id (the same one you see when you run &lt;code>gcloud config list&lt;/code>).&lt;/p>
&lt;p>For service account, use the address of the Service Account we have created. If you don&amp;rsquo;t remember, you can find it under S&lt;a href="https://console.cloud.google.com/iam-admin/serviceaccounts">ervice Accounts in the Google Cloud Console&lt;/a>.&lt;/p>
&lt;p>For staging and temp locations, use the same bucket that we have just created. Change the bucket address in the paths, and leave the same &amp;ldquo;binaries&amp;rdquo; and &amp;ldquo;tmp&amp;rdquo; locations that are already set in the configuration.&lt;/p>
&lt;p>For region, in this example we are using &lt;code>europe-west1&lt;/code>.&lt;/p>
&lt;p>Also, depending on your network configuration, you may want to check the box of &amp;ldquo;Use Public IPs?&amp;rdquo;, or alternatively leave it unchecked but enable Google Private Access in the regional subnetwork for europe-west1 in your project (for more details, please see &lt;a href="https://cloud.google.com/vpc/docs/configure-private-google-access#enabling-pga">Configuring Private Google Access | VPC&lt;/a>). In this example, I will check the box for simplicity.&lt;/p>
&lt;p>For the fat jar location, use the _Browse _button on the right side, and locate the JAR that we generated above. In summary, my &lt;em>Main&lt;/em> options look like these (your project id and locations will be different):&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image27.png"
alt="Apache Hop variables">&lt;/p>
&lt;p>You may, of course, change any other option, depending on the specific settings that might be required for your project.&lt;/p>
&lt;p>When you are ready, click on the _Ok _button and then &lt;em>Launch&lt;/em> to trigger the pipeline.&lt;/p>
&lt;p>In the logging window, you should see a line like the following:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image16.png"
alt="Apache Hop logging window">&lt;/p>
&lt;h2 id="checking-the-job-in-dataflow">Checking the job in Dataflow&lt;/h2>
&lt;p>If everything has gone well, you should now see a job running at &lt;a href="https://console.cloud.google.com/dataflow/jobs">https://console.cloud.google.com/dataflow/jobs&lt;/a>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image1.png"
alt="Dataflow job list">&lt;/p>
&lt;p>If for some reason the job has failed, open the failed job page, check the _Logs _at the bottom, and click the error icon to find why the pipeline has failed. It is normally because we have set some wrong option in your configuration:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image25.png"
alt="Dataflow Logs">&lt;/p>
&lt;p>When the pipeline starts running, you should see the graph of the pipeline in the job page:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image23.png"
alt="Dataflow pipeline graph">&lt;/p>
&lt;p>When the job finishes, there should be a file in the output location. You can check it out with &lt;code>gsutil&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>% gsutil ls gs://ihr-apache-hop-blog/output
gs://ihr-apache-hop-blog/output/input-process-output-00000-of-00003.csv
gs://ihr-apache-hop-blog/output/input-process-output-00001-of-00003.csv
gs://ihr-apache-hop-blog/output/input-process-output-00002-of-00003.csv
&lt;/code>&lt;/pre>&lt;p>In my case, the job has generated three files, but the actual number will vary from run to run.&lt;/p>
&lt;p>Let&amp;rsquo;s explore the first lines of those files:&lt;/p>
&lt;pre tabindex="0">&lt;code>gsutil cat &amp;#34;gs://ihr-apache-hop-blog/output/*csv&amp;#34;| head
12,wha-firstname,vnaov-name,egm-city,CALIFORNIA
25,ayl-firstname,bwkoe-name,rtw-city,CALIFORNIA
26,zio-firstname,rezku-name,nvt-city,CALIFORNIA
44,rgh-firstname,wzkjq-name,hkm-city,CALIFORNIA
135,ttv-firstname,eqley-name,trs-city,CALIFORNIA
177,ahc-firstname,nltvw-name,uxf-city,CALIFORNIA
181,kxv-firstname,bxerk-name,sek-city,CALIFORNIA
272,wpy-firstname,qxjcn-name,rew-city,CALIFORNIA
304,skq-firstname,cqapx-name,akw-city,CALIFORNIA
308,sfu-firstname,ibfdt-name,kqf-city,CALIFORNIA
&lt;/code>&lt;/pre>&lt;p>We can see that all the rows have CALIFORNIA as the state, that the output contains only the columns that we selected, and that the user id is the first column. The actual output you get will probably be different, as the order in which data is processed will not be the same in each run.&lt;/p>
&lt;p>We have run this job with a small data sample, but we could have run the same job with an arbitrarily large input CSV. Dataflow would parallelize and process the data in chunks.&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>Apache Hop is a visual development environment for Beam pipelines, that allows us to run the pipelines locally, inspect the data, debug, unit test and many other capabilities. Once we are happy with a pipeline that has run locally, we can deploy the same visual pipeline in the cloud by just setting the necessary parameters for using Dataflow.&lt;/p>
&lt;p>If you want to know more about Apache Hop, don&amp;rsquo;t miss &lt;a href="https://www.youtube.com/watch?v=sZSIbcPtebI">the Beam Summit talk delivered by the author of Hop&lt;/a>, and don&amp;rsquo;t forget to check out the &lt;a href="https://hop.apache.org/manual/latest/getting-started/index.html">getting started guide&lt;/a>.&lt;/p></description></item><item><title>Blog: Apache Beam 2.38.0</title><link>/blog/beam-2.38.0/</link><pubDate>Wed, 20 Apr 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.38.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.38.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2380-2022-04-20">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.38.0 check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12351169">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Introduce projection pushdown optimizer to the Java SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12976">BEAM-12976&lt;/a>). The optimizer currently only works on the &lt;a href="/documentation/io/built-in/google-bigquery/#storage-api">BigQuery Storage API&lt;/a>, but more I/Os will be added in future releases. If you encounter a bug with the optimizer, please file a JIRA and disable the optimizer using pipeline option &lt;code>--experiments=disable_projection_pushdown&lt;/code>.&lt;/li>
&lt;li>A new IO for Neo4j graph databases was added. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-1857">BEAM-1857&lt;/a>) It has the ability to update nodes and relationships using UNWIND statements and to read data using cypher statements with parameters.&lt;/li>
&lt;li>&lt;code>amazon-web-services2&lt;/code> has reached feature parity and is finally recommended over the earlier &lt;code>amazon-web-services&lt;/code> and &lt;code>kinesis&lt;/code> modules (Java). These will be deprecated in one of the next releases (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13174">BEAM-13174&lt;/a>).
&lt;ul>
&lt;li>Long outstanding write support for &lt;code>Kinesis&lt;/code> was added (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13175">BEAM-13175&lt;/a>).&lt;/li>
&lt;li>Configuration was simplified and made consistent across all IOs, including the usage of &lt;code>AwsOptions&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13563">BEAM-13563&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13663">BEAM-13663&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13587">BEAM-13587&lt;/a>).&lt;/li>
&lt;li>Additionally, there&amp;rsquo;s a long list of recent improvements and fixes to
&lt;code>S3&lt;/code> Filesystem (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13245">BEAM-13245&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13246">BEAM-13246&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13441">BEAM-13441&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13445">BEAM-13445&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-14011">BEAM-14011&lt;/a>),
&lt;code>DynamoDB&lt;/code> IO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13009">BEAM-13209&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13209">BEAM-13209&lt;/a>),
&lt;code>SQS&lt;/code> IO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13631">BEAM-13631&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13510">BEAM-13510&lt;/a>) and others.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Pipeline dependencies supplied through &lt;code>--requirements_file&lt;/code> will now be staged to the runner using binary distributions (wheels) of the PyPI packages for linux_x86_64 platform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-4032">BEAM-4032&lt;/a>). To restore the behavior to use source distributions, set pipeline option &lt;code>--requirements_cache_only_sources&lt;/code>. To skip staging the packages at submission time, set pipeline option &lt;code>--requirements_cache=skip&lt;/code> (Python).&lt;/li>
&lt;li>The Flink runner now supports Flink 1.14.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13106">BEAM-13106&lt;/a>).&lt;/li>
&lt;li>Interactive Beam now supports remotely executing Flink pipelines on Dataproc (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14071">BEAM-14071&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>(Python) Previously &lt;code>DoFn.infer_output_types&lt;/code> was expected to return &lt;code>Iterable[element_type]&lt;/code> where &lt;code>element_type&lt;/code> is the PCollection elemnt type. It is now expected to return &lt;code>element_type&lt;/code>. Take care if you have overriden &lt;code>infer_output_type&lt;/code> in a &lt;code>DoFn&lt;/code> (this is not common). See &lt;a href="https://issues.apache.org/jira/browse/BEAM-13860">BEAM-13860&lt;/a>.&lt;/li>
&lt;li>(&lt;code>amazon-web-services2&lt;/code>) The types of &lt;code>awsRegion&lt;/code> / &lt;code>endpoint&lt;/code> in &lt;code>AwsOptions&lt;/code> changed from String to &lt;code>Region&lt;/code> / &lt;code>URI&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13563">BEAM-13563&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Beam 2.38.0 will be the last minor release to support Flink 1.11.&lt;/li>
&lt;li>(&lt;code>amazon-web-services2&lt;/code>) Client providers (&lt;code>withXYZClientProvider()&lt;/code>) as well as IO specific &lt;code>RetryConfiguration&lt;/code>s are deprecated, instead use &lt;code>withClientConfiguration()&lt;/code> or &lt;code>AwsOptions&lt;/code> to configure AWS IOs / clients.
Custom implementations of client providers shall be replaced with a respective &lt;code>ClientBuilderFactory&lt;/code> and configured through &lt;code>AwsOptions&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13563">BEAM-13563&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fix S3 copy for large objects (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14011">BEAM-14011&lt;/a>)&lt;/li>
&lt;li>Fix quadratic behavior of pipeline canonicalization (Go) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14128">BEAM-14128&lt;/a>)
&lt;ul>
&lt;li>This caused unnecessarily long pre-processing times before job submission for large complex pipelines.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Fix &lt;code>pyarrow&lt;/code> version parsing (Python)(&lt;a href="https://issues.apache.org/jira/browse/BEAM-14235">BEAM-14235&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.38.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.38.0 release. Thank you to all contributors!&lt;/p>
&lt;p>abhijeet-lele
Ahmet Altay
akustov
Alexander
Alexander Zhuravlev
Alexey Romanenko
AlikRodriguez
Anand Inguva
andoni-guzman
andreukus
Andy Ye
Ankur Goenka
ansh0l
Artur Khanin
Aydar Farrakhov
Aydar Zainutdinov
Benjamin Gonzalez
Brian Hulette
brucearctor
bulat safiullin
bullet03
Carl Mastrangelo
Chamikara Jayalath
Chun Yang
Daniela Martín
Daniel Oliveira
Danny McCormick
daria.malkova
David Cavazos
David Huntsperger
dmitryor
Dmytro Sadovnychyi
dpcollins-google
egalpin
Elias Segundo Antonio
emily
Etienne Chauchot
Hengfeng Li
Ismaël Mejía
Israel Herraiz
Jack McCluskey
Jakub Kukul
Janek Bevendorff
Jeff Klukas
Johan Sternby
Kamil Breguła
Kenneth Knowles
Ke Wu
Kiley
Kyle Weaver
laraschmidt
Lara Schmidt
LE QUELLEC Olivier
Luka Kalinovcic
Luke Cwik
Marcin Kuthan
masahitojp
Masato Nakamura
Matt Casters
Melissa Pashniak
Michael Li
Miguel Hernandez
Moritz Mack
mosche
nancyxu123
Nathan J Mehl
Niel Markwick
Ning Kang
Pablo Estrada
paul-tlh
Pavel Avilov
Rahul Iyer
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Skraba
Ryan Thompson
Sam Whittle
Seth Vargo
sp029619
Steven Niemitz
Thiago Nunes
Udi Meiri
Valentyn Tymofieiev
Victor
vitaly.terentyev
Yichi Zhang
Yi Hu
yirutang
Zachary Houfek
Zoe&lt;/p></description></item><item><title>Blog: Apache Beam 2.37.0</title><link>/blog/beam-2.37.0/</link><pubDate>Fri, 04 Mar 2022 08:30:00 -0800</pubDate><guid>/blog/beam-2.37.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.37.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2370-2022-03-04">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.37.0 check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12351168">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Java 17 support for Dataflow (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12240">BEAM-12240&lt;/a>).
&lt;ul>
&lt;li>Users using Dataflow Runner V2 may see issues with state cache due to inaccurate object sizes (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13695">BEAM-13695&lt;/a>).&lt;/li>
&lt;li>ZetaSql is currently unsupported (&lt;a href="https://github.com/google/zetasql/issues/89">issue&lt;/a>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Python 3.9 support in Apache Beam (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12000">BEAM-12000&lt;/a>).
&lt;ul>
&lt;li>Dataflow support for Python 3.9 is expected to be available with 2.37.0,
but may not be fully available yet when the release is announced (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13864">BEAM-13864&lt;/a>).&lt;/li>
&lt;li>Users of Dataflow Runner V2 can run Python 3.9 pipelines with 2.37.0 release right away.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Go SDK now has wrappers for the following Cross Language Transforms from Java, along with automatic expansion service startup for each.
&lt;ul>
&lt;li>JDBCIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13293">BEAM-13293&lt;/a>).&lt;/li>
&lt;li>Debezium (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13761">BEAM-13761&lt;/a>).&lt;/li>
&lt;li>BeamSQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13683">BEAM-13683&lt;/a>).&lt;/li>
&lt;li>BiqQuery (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13732">BEAM-13732&lt;/a>).&lt;/li>
&lt;li>KafkaIO now also has automatic expansion service startup. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13821">BEAM-13821&lt;/a>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>DataFrame API now supports pandas 1.4.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13605">BEAM-13605&lt;/a>).&lt;/li>
&lt;li>Go SDK DoFns can now observe trigger panes directly (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13757">BEAM-13757&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.37.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.37.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aizhamal Nurmamat kyzy
Alexander
Alexander Chermenin
Alexandr Zhuravlev
Alexey Romanenko
Anand Inguva
andoni-guzman
andreukus
Andy Ye
Artur Khanin
Aydar Farrakhov
Aydar Zainutdinov
AydarZaynutdinov
Benjamin Gonzalez
Brian Hulette
Chamikara Jayalath
Daniel Oliveira
Danny McCormick
daria-malkova
daria.malkova
darshan-sj
David Huntsperger
dprieto91
emily
Etienne Chauchot
Fernando Morales
Heejong Lee
Ismaël Mejía
Jack McCluskey
Jan Lukavský
johnjcasey
Kamil Breguła
kellen
Kenneth Knowles
kileys
Kyle Weaver
Luke Cwik
Marcin Kuthan
Marco Robles
Matt Rudary
Miguel Hernandez
Milena Bukal
Moritz Mack
Mostafa Aghajani
Ning Kang
Pablo Estrada
Pavel Avilov
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Sam Whittle
Sandy Chapman
Sergey Kalinin
Thiago Nunes
thorbjorn444
Tim Robertson
Tomo Suzuki
Valentyn Tymofieiev
Victor
Victor Chen
Vitaly Ivanov
Yichi Zhang&lt;/p></description></item><item><title>Blog: Upcoming Events for Beam in 2022</title><link>/blog/upcoming-events-for-beam-in-2022/</link><pubDate>Mon, 28 Feb 2022 00:00:01 -0800</pubDate><guid>/blog/upcoming-events-for-beam-in-2022/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are so excited to announce the upcoming Beam events for this year! We believe that events are an important mechanism to foster the community around Apache Beam as an Open Source Project. Our events are focused on a developer experience by giving spaces for the community to connect, facilitate collaboration, and enable knowledge sharing.&lt;/p>
&lt;p>Here is an overview of some upcoming events and ways for everyone to help foster additional community growth:&lt;/p>
&lt;h2 id="beam-summit">Beam Summit&lt;/h2>
&lt;p>The &lt;strong>&lt;a href="https://2022.beamsummit.org/">Beam Summit 2022&lt;/a>&lt;/strong> is approaching! The event will be in a hybrid in-person and virtual format from Austin, TX on July 18-20, 2022. The conference will include three full days of lightning talks, roadmap updates, use cases, demos, and workshops for Beam users of all levels. This is a great opportunity to collaborate, share ideas, and work together in the improvement of the project.&lt;/p>
&lt;p>Check out talks from prior editions of Beam Summit &lt;strong>&lt;a href="https://www.youtube.com/watch?v=jses0W4Zalc&amp;amp;list=PL4dEBWmGSIU8vLWF56shrSuTsLXvO6Ex3">here&lt;/a>&lt;/strong>!&lt;/p>
&lt;h3 id="the-experience">The Experience&lt;/h3>
&lt;p>We are so excited to see some of you in person again and the rest of the community online! The &lt;strong>&lt;a href="https://2022.beamsummit.org/team/">Beam Summit Steering Committee&lt;/a>&lt;/strong> in partnership with an event production company is working hard to ensure that we provide the community with the best possible experience, no matter which format you choose to attend in.&lt;/p>
&lt;p>If you have any ideas on how we can make this year’s event better, please &lt;strong>&lt;a href="mailto:contact@beamsummit.org">reach out to us&lt;/a>&lt;/strong>!&lt;/p>
&lt;h3 id="ways-to-help--participate">Ways to Help &amp;amp; Participate&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>&lt;a href="https://sessionize.com/beam-summit-2022">Submit a proposal&lt;/a>&lt;/strong> to talk! The deadline for submissions is &lt;em>March 15th&lt;/em>.&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://2022.beamsummit.org/tickets/">Register&lt;/a>&lt;/strong> to join as an attendee in person or online.&lt;/li>
&lt;li>Consider sponsoring the event. If your company is interested in engaging with members of the community, please check out the &lt;strong>&lt;a href="https://2022.beamsummit.org/sponsors/">sponsoring prospectus&lt;/a>.&lt;/strong>&lt;/li>
&lt;li>Help us get the word out. Please make sure to let your colleagues and friends know about the Beam Summit.&lt;/li>
&lt;/ol>
&lt;p>Don’t forget to follow our Beam Summit &lt;strong>&lt;a href="https://twitter.com/BeamSummit?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">Twitter&lt;/a>&lt;/strong> and &lt;strong>&lt;a href="https://www.linkedin.com/company/beam-summit/?viewAsMember=true">LinkedIn&lt;/a>&lt;/strong> pages to receive event updates!&lt;/p>
&lt;h2 id="beam-college">Beam College&lt;/h2>
&lt;p>&lt;strong>&lt;a href="https://beamcollege.dev/">Beam College 2022&lt;/a>&lt;/strong> is around the corner for the second season of training! The event will be hosted virtually from May 10-13, 2022. The training is focused on providing more hands-on experience around end-to-end code samples in an interactive environment, and helping attendees see the applications of concepts covered in other venues, such as the Beam Summit.&lt;/p>
&lt;p>Check out talks from prior editions of Beam College &lt;strong>&lt;a href="https://www.youtube.com/playlist?list=PLjYq1UNvv2UcrfapfgKrnLXtYpkvHmpIh">here&lt;/a>&lt;/strong>!&lt;/p>
&lt;p>This year, the training will consist of learning modules such as:&lt;/p>
&lt;ul>
&lt;li>The Data movement ecosystem and distributed processing the Beam way&lt;/li>
&lt;li>Scaling, productionalizing, and developing your Beam pipelines&lt;/li>
&lt;li>Use Cases&lt;/li>
&lt;li>Beam ML Use Cases&lt;/li>
&lt;/ul>
&lt;p>Be sure to check out our &lt;strong>&lt;a href="https://beamcollege.dev/">website&lt;/a>&lt;/strong> as we continue updating the schedule and follow our &lt;strong>&lt;a href="https://twitter.com/beam_college?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">Twitter&lt;/a>&lt;/strong> and &lt;strong>&lt;a href="https://www.linkedin.com/showcase/beam-college/">LinkedIn&lt;/a>&lt;/strong> pages to receive event updates!&lt;/p>
&lt;h3 id="ways-to-help--participate-1">Ways to Help &amp;amp; Participate&lt;/h3>
&lt;ol>
&lt;li>Interested in instructing? Submit a &lt;strong>&lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSct6RCrKtgsvxlgngKUGwKoB_iOKihXi1OadKyBQIsi00p3cQ/viewform?usp=sf_link">proposal&lt;/a>&lt;/strong>! The deadline is: &lt;em>February 28th.&lt;/em>&lt;/li>
&lt;li>Enroll in Beam College. Registration is now open on the &lt;strong>&lt;a href="https://beamcollege.dev/step/2022/">registration page&lt;/a>&lt;/strong>.&lt;/li>
&lt;li>Consider partnering with the event. If your company is interested in helping to promote the event and being a part of the branding, please fill out this &lt;strong>form&lt;/strong>.&lt;/li>
&lt;li>Help us get the word out by letting your network know about this exciting opportunity to help users uplevel data processing skills, solve complex data applications, and optimize data pipelines!&lt;/li>
&lt;/ol>
&lt;h2 id="beam-meetups">Beam Meetups&lt;/h2>
&lt;p>In partnership with an event production company, Beam will be hosting an average of one virtual Meetup per month. These Meetups will be relaxed presentations on topics or demos followed by a Q&amp;amp;A session. The objective of our virtual meetups is to give the community an update on the most recent Beam features launched within the past six months. These meetups are free and open to the public.&lt;/p>
&lt;p>Check out recordings from previous Meetups &lt;strong>&lt;a href="https://www.youtube.com/watch?v=8fNEs7SbefM&amp;amp;list=PL4dEBWmGSIU-cQSpYP7R1lSC6e2K_pTf1">here&lt;/a>&lt;/strong>!&lt;/p>
&lt;h3 id="ways-to-help--participate-2">Ways to Help &amp;amp; Participate&lt;/h3>
&lt;ol>
&lt;li>Are you interested in sharing a feature launch or sharing a step-by-step use case for Beam? Submit a &lt;strong>&lt;a href="https://docs.google.com/forms/d/e/1FAIpQLScFg7fmOFc7fTvnJL_dmdhia4HDesW4HYxJsDeulnsHzIzqCg/viewform">talk idea&lt;/a>&lt;/strong>!&lt;/li>
&lt;li>Register for the events. Registration is now open on the &lt;strong>&lt;a href="https://clowder.space/projects/apache-beam/">registration page&lt;/a>&lt;/strong>.&lt;/li>
&lt;li>Help us get the word out by spreading the word throughout the community to enable more knowledge sharing and collaboration!&lt;/li>
&lt;/ol></description></item><item><title>Blog: Apache Beam 2.36.0</title><link>/blog/beam-2.36.0/</link><pubDate>Mon, 07 Feb 2022 10:11:00 -0800</pubDate><guid>/blog/beam-2.36.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.36.0 release of Apache Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2360-2022-02-07">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.36.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12350407">detailed release
notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for stopReadTime on KafkaIO SDF (Java).(&lt;a href="https://issues.apache.org/jira/browse/BEAM-13171">BEAM-13171&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>💻 Support for ARM64 / Mac M1 out of the box. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11703">BEAM-11703&lt;/a>).&lt;/li>
&lt;li>Added support for cloudpickle as a pickling library for Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8123">BEAM-8123&lt;/a>). To use cloudpickle, set pipeline option: &amp;ndash;pickle_library=cloudpickle&lt;/li>
&lt;li>Added option to specify triggering frequency when streaming to BigQuery (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12865">BEAM-12865&lt;/a>).&lt;/li>
&lt;li>Added option to enable caching uploaded artifacts across job runs for Python Dataflow jobs (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13459">BEAM-13459&lt;/a>). To enable, set pipeline option: &amp;ndash;enable_artifact_caching, this will be enabled by default in a future release.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Updated the jedis from 3.x to 4.x to Java RedisIO. If you are using RedisIO and using jedis directly, please refer to &lt;a href="https://github.com/redis/jedis/blob/v4.0.0/docs/3to4.md">this page&lt;/a> to update it. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12092">BEAM-12092&lt;/a>).&lt;/li>
&lt;li>Datatype of timestamp fields in &lt;code>SqsMessage&lt;/code> for AWS IOs for SDK v2 was changed from &lt;code>String&lt;/code> to &lt;code>long&lt;/code>, visibility of all fields was fixed from &lt;code>package private&lt;/code> to &lt;code>public&lt;/code> &lt;a href="https://issues.apache.org/jira/browse/BEAM-13638">BEAM-13638&lt;/a>.&lt;/li>
&lt;li>Properly check output timestamps on elements output from DoFns, timers, and onWindowExpiration in Java &lt;a href="https://issues.apache.org/jira/browse/BEAM-12931">BEAM-12931&lt;/a>.&lt;/li>
&lt;li>Fixed a bug with DeferredDataFrame.xs when used with a non-tuple key
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-13421%5D">BEAM-13421&lt;/a>).&lt;/li>
&lt;li>Beam Python now requires &lt;code>google-cloud-pubsub&amp;gt;=2.1.0&lt;/code>. The API surface for &lt;code>apache_beam.io.gcp.pubsub&lt;/code> has not changed, but code that uses the PubSub client directly may need to be updated.&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Users may encounter an unexpected java.lang.ArithmeticException when outputting a timestamp
for an element further than allowedSkew from an allowed DoFN skew set to a value more than
Integer.MAX_VALUE.&lt;/li>
&lt;li>S3 object metadata retrieval broken in Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13980">BEAM-13980&lt;/a>)&lt;/li>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.36.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.36.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ada Wong
Ahmet Altay
Alexander
Alexander Dahl
Alexandr Zhuravlev
Alexey Romanenko
AlikRodriguez
Anand Inguva
Andrew Pilloud
Andy Ye
Arkadiusz Gasiński
Artur Khanin
Arun Pandian
Aydar Farrakhov
Aydar Zainutdinov
AydarZaynutdinov
Benjamin Gonzalez
Brian Hulette
Chamikara Jayalath
Daniel Collins
Daniel Oliveira
Daniel Thevessen
Daniela Martín
David Hinkes
David Huntsperger
Emily Ye
Etienne Chauchot
Evan Galpin
Heejong Lee
Ilya
Ilya Kozyrev
In-Ho Yi
Jack McCluskey
Janek Bevendorff
Jarek Potiuk
Ke Wu
KevinGG
Kyle Hersey
Kyle Weaver
Luís Bianchin
Luke Cwik
Masato Nakamura
Matthias Baetens
Mehdi Drissi
Melissa Pashniak
Michel Davit
Miguel Hernandez
MiguelAnzoWizeline
Milena Bukal
Moritz Mack
Mostafa Aghajani
Nathan J Mehl
Niel Markwick
Ning Kang
Pablo Estrada
Pavel Avilov
Quentin Sommer
Reuben van Ammers
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Sayat
Sergei Lebedev
Sergey Kalinin
Steve Niemitz
Talat Uyarer
Thiago Nunes
Tianyang Hu
Tim Robertson
Valentyn Tymofieiev
Vitaly Ivanov
Yichi Zhang
Yiru Tang
Yu Feng
Yu ISHIKAWA
Zachary Houfek
blais
daria-malkova
daria.malkova
darshan-sj
dpcollins-google
emily
ewianda
johnjcasey
kileys
lam206
laraschmidt
mosche
&lt;a href="mailto:msbukal@google.com">msbukal@google.com&lt;/a>
tvalentyn&lt;/p></description></item><item><title>Blog: Apache Beam 2.35.0</title><link>/blog/beam-2.35.0/</link><pubDate>Wed, 29 Dec 2021 10:11:00 -0800</pubDate><guid>/blog/beam-2.35.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.35.0 release of Apache Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2350-2021-12-29">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.35.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12350406">detailed release
notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>MultiMap side inputs are now supported by the Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-3293">BEAM-3293&lt;/a>).&lt;/li>
&lt;li>Side inputs are supported within Splittable DoFns for Dataflow Runner V1 and Dataflow Runner V2. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12522">BEAM-12522&lt;/a>).&lt;/li>
&lt;li>Upgrades Log4j version used in test suites (Apache Beam testing environment only, not for end user consumption) to 2.17.0(&lt;a href="https://issues.apache.org/jira/browse/BEAM-13434">BEAM-13434&lt;/a>).
Note that Apache Beam versions do not depend on the Log4j 2 dependency (log4j-core) impacted by &lt;a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228">CVE-2021-44228&lt;/a>.
However we urge users to update direct and indirect dependencies (if any) on Log4j 2 to the latest version by updating their build configuration and redeploying impacted pipelines.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>We changed the data type for ranges in &lt;code>JdbcIO.readWithPartitions&lt;/code> from &lt;code>int&lt;/code> to &lt;code>long&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13149">BEAM-13149&lt;/a>).
This is a relatively minor breaking change, which we&amp;rsquo;re implementing to improve the usability of the transform without increasing cruft.
This transform is relatively new, so we may implement other breaking changes in the future to improve its usability.&lt;/li>
&lt;li>Side inputs are supported within Splittable DoFns for Dataflow Runner V1 and Dataflow Runner V2. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12522">BEAM-12522&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added custom delimiters to Python TextIO reads (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12730">BEAM-12730&lt;/a>).&lt;/li>
&lt;li>Added escapechar parameter to Python TextIO reads (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13189">BEAM-13189&lt;/a>).&lt;/li>
&lt;li>Splittable reading is enabled by default while reading data with ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12070">BEAM-12070&lt;/a>).&lt;/li>
&lt;li>DoFn Execution Time metrics added to Go (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13001">BEAM-13001&lt;/a>).&lt;/li>
&lt;li>Cross-bundle side input caching is now available in the Go SDK for runners that support the feature by setting the EnableSideInputCache hook (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11097">BEAM-11097&lt;/a>).&lt;/li>
&lt;li>Upgraded the GCP Libraries BOM version to 24.0.0 and associated dependencies (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11205">BEAM-11205&lt;/a>). For Google Cloud client library versions set by this BOM,
see &lt;a href="https://storage.googleapis.com/cloud-opensource-java-dashboard/com.google.cloud/libraries-bom/24.0.0/artifact_details.html">this table&lt;/a>.&lt;/li>
&lt;li>Removed avro-python3 dependency in AvroIO. Fastavro has already been our Avro library of choice on Python 3. Boolean use_fastavro is left for api compatibility, but will have no effect.(&lt;a href="https://github.com/apache/beam/pull/15900">BEAM-13016&lt;/a>).&lt;/li>
&lt;li>MultiMap side inputs are now supported by the Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-3293">BEAM-3293&lt;/a>).&lt;/li>
&lt;li>Remote packages can now be downloaded from locations supported by apache_beam.io.filesystems. The files will be downloaded on Stager and uploaded to staging location. For more information, see &lt;a href="https://issues.apache.org/jira/browse/BEAM-11275">BEAM-11275&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>A new URN convention was adopted for cross-language transforms and existing URNs were updated. This may break advanced use-cases, for example, if a custom expansion service is used to connect diffrent Beam Java and Python versions. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12047">BEAM-12047&lt;/a>).&lt;/li>
&lt;li>The upgrade to Calcite 1.28.0 introduces a breaking change in the SUBSTRING function in SqlTransform, when used with the Calcite dialect (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13099">BEAM-13099&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/CALCITE-4427">CALCITE-4427&lt;/a>).&lt;/li>
&lt;li>ListShards (with DescribeStreamSummary) is used instead of DescribeStream to list shards in Kinesis streams (AWS SDK v2). Due to this change, as mentioned in &lt;a href="https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html">AWS documentation&lt;/a>, for fine-grained IAM policies it is required to update them to allow calls to ListShards and DescribeStreamSummary APIs. For more information, see &lt;a href="https://docs.aws.amazon.com/streams/latest/dev/controlling-access.html">Controlling Access to Amazon Kinesis Data Streams&lt;/a> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13233">BEAM-13233&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Non-splittable reading is deprecated while reading data with ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12070">BEAM-12070&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Properly map main input windows to side input windows by default (Go)
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-11087">BEAM-11087&lt;/a>).&lt;/li>
&lt;li>Fixed data loss when writing to DynamoDB without setting deduplication key names (Java)
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-13009">BEAM-13009&lt;/a>).&lt;/li>
&lt;li>Go SDK Examples now have types and functions registered. (Go) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5378">BEAM-5378&lt;/a>)&lt;/li>
&lt;li>Fixed data loss when using Python WriteToFiles in streaming pipeline (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12950">BEAM-12950&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Users of beam-sdks-java-io-hcatalog (and beam-sdks-java-extensions-sql-hcatalog) must take care to override the transitive log4j dependency when they add a hive dependency (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13499">BEAM-13499&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.35.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay
Alexandr Zhuravlev
Alexey Romanenko
AlikRodriguez
Anand Inguva
Andrew Pilloud
Ankur Goenka
Anthony Sottile
Artur Khanin
Aydar Farrakhov
Aydar Zainutdinov
Benjamin Gonzalez
brachipa
Brian Hulette
Calvin Leung
Chamikara Jayalath
Chris Gray
Damon Douglas
Daniel Collins
Daniel Oliveira
daria.malkova
darshan-sj
David Huntsperger
David Prieto Rivera
Dmitrii Kuzin
dpcollins-google
dprieto
egalpin
Etienne Chauchot
Eugene Nikolaiev
Fernando Morales
Hector Lagos
Heejong Lee
Ilya Kozyrev
Iñigo San Jose Visiers
Jack McCluskey
Jiayang Wu
jrhy
Kenneth Knowles
KevinGG
kileys
klmilam
Kyle Weaver
Luís Bianchin
Luke Cwik
Melissa Pashniak
Michael Luckey
Miguel Hernandez
Milena Bukal
Minbo Bae
minherz
Moritz Mack
mosche
Natalie
Ning Kang
Pablo Estrada
Pavel Avilov
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Rogan Morrow
Ruslan Altynnikov
Sam Whittle
Sergey Kalinin
Slava Chernyak
Svetak Sundhar
Tianyang Hu
Tim Robertson
Tomo Suzuki
tuorhador
Udi Meiri
vachan-shetty
Valentyn Tymofieiev
Yichi Zhang
zhoufek&lt;/p></description></item><item><title>Blog: Apache Beam 2.34.0</title><link>/blog/beam-2.34.0/</link><pubDate>Thu, 11 Nov 2021 00:11:00 -0800</pubDate><guid>/blog/beam-2.34.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.34.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2340-2021-11-11">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.34.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12350405">detailed release
notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>The Beam Java API for Calcite SqlTransform is no longer experimental (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12680">BEAM-12680&lt;/a>).&lt;/li>
&lt;li>Python&amp;rsquo;s ParDo (Map, FlatMap, etc.) transforms now suport a &lt;code>with_exception_handling&lt;/code> option for easily ignoring bad records and implementing the dead letter pattern.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>&lt;code>ReadFromBigQuery&lt;/code> and &lt;code>ReadAllFromBigQuery&lt;/code> now run queries with BATCH priority by default. The &lt;code>query_priority&lt;/code> parameter is introduced to the same transforms to allow configuring the query priority (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12913">BEAM-12913&lt;/a>).&lt;/li>
&lt;li>[EXPERIMENTAL] Support for &lt;a href="https://cloud.google.com/bigquery/docs/reference/storage">BigQuery Storage Read API&lt;/a> added to &lt;code>ReadFromBigQuery&lt;/code>. The newly introduced &lt;code>method&lt;/code> parameter can be set as &lt;code>DIRECT_READ&lt;/code> to use the Storage Read API. The default is &lt;code>EXPORT&lt;/code> which invokes a BigQuery export request. (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10917">BEAM-10917&lt;/a>).&lt;/li>
&lt;li>[EXPERIMENTAL] Added &lt;code>use_native_datetime&lt;/code> parameter to &lt;code>ReadFromBigQuery&lt;/code> to configure the return type of &lt;a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#datetime_type">DATETIME&lt;/a> fields when using &lt;code>ReadFromBigQuery&lt;/code>. This parameter can &lt;em>only&lt;/em> be used when &lt;code>method = DIRECT_READ&lt;/code>(Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10917">BEAM-10917&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Upgrade to Calcite 1.26.0 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9379">BEAM-9379&lt;/a>).&lt;/li>
&lt;li>Added a new &lt;code>dataframe&lt;/code> extra to the Python SDK that tracks &lt;code>pandas&lt;/code> versions
we&amp;rsquo;ve verified compatibility with. We now recommend installing Beam with &lt;code>pip install apache-beam[dataframe]&lt;/code> when you intend to use the DataFrame API
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-12906">BEAM-12906&lt;/a>).&lt;/li>
&lt;li>Add an &lt;a href="https://github.com/cometta/python-apache-beam-spark">example&lt;/a> of deploying Python Apache Beam job with Spark Cluster&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>SQL Rows are no longer flattened (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5505">BEAM-5505&lt;/a>).&lt;/li>
&lt;li>[Go SDK] beam.TryCrossLanguage&amp;rsquo;s signature now matches beam.CrossLanguage. Like other Try functions it returns an error instead of panicking. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9918">BEAM-9918&lt;/a>).&lt;/li>
&lt;li>&lt;a href="https://jira.apache.org/jira/browse/BEAM-12925">BEAM-12925&lt;/a> was fixed. It used to silently pass incorrect null data read from JdbcIO. Pipelines affected by this will now start throwing failures instead of silently passing incorrect data.&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed error while writing multiple DeferredFrames to csv (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12701">BEAM-12701&lt;/a>).&lt;/li>
&lt;li>Fixed error when importing the DataFrame API with pandas 1.0.x installed (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12945">BEAM-12945&lt;/a>).&lt;/li>
&lt;li>Fixed top.SmallestPerKey implementation in the Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12946">BEAM-12946&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Large Java BigQueryIO writes with the FILE_LOADS method will fail in batch mode (specifically, when copy jobs are used).
This results in the error message: &lt;code>IllegalArgumentException: Attempting to access unknown side input&lt;/code>.
Please upgrade to a newer version (&amp;gt; 2.34.0) or use another write method (e.g. &lt;code>STORAGE_WRITE_API&lt;/code>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.34.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay,
Aizhamal Nurmamat kyzy,
Alex Amato,
Alexander Chermenin,
Alexey Romanenko,
AlikRodriguez,
Andrew Pilloud,
Andy Xu,
Ankur Goenka,
Aydar Farrakhov,
Aydar Zainutdinov,
Aydar Zaynutdinov,
AydarZaynutdinov,
Benjamin Gonzalez,
BenWhitehead,
Brachi Packter,
Brian Hulette,
Bu Sun Kim,
Chamikara Jayalath,
Chris Gray,
Chuck Yang,
Chun Yang,
Claire McGinty,
comet,
Daniel Collins,
Daniel Oliveira,
Daniel Thevessen,
daria.malkova,
David Cavazos,
David Huntsperger,
Dmytro Kozhevin,
dpcollins-google,
Eduardo Sánchez López,
Elias Djurfeldt,
emily,
Emily Ye,
Enis Sert,
Etienne Chauchot,
Fernando Morales,
Heejong Lee,
Ihor Indyk,
Ismaël Mejía,
Israel Herraiz,
Jack McCluskey,
Jonathan Hourany,
Judah Rand,
Kenneth Knowles,
KevinGG,
Ke Wu,
kileys,
Kyle Weaver,
Luke Cwik,
masahitojp,
MiguelAnzoWizeline,
Minbo Bae,
Niels Basjes,
Ning Kang,
Pablo Estrada,
pareshsarafmdb,
Paul Féraud,
Piotr Szczepanik,
Reuven Lax,
Ritesh Ghorse,
R. Miles McCain,
Robert Bradshaw,
Robert Burke,
Rogan Morrow,
Ruwan Lambrichts,
rvballada,
Ryan Thompson,
Sam Rohde,
Sam Whittle,
Ștefan Istrate,
Steve Niemitz,
Thomas Li Fredriksen,
Tomo Suzuki,
tvalentyn,
Udi Meiri,
Vachan,
Valentyn Tymofieiev,
Vincent Marquez,
WinsonT,
Yichi Zhang,
Yifan Mai,
Yilei &amp;ldquo;Dolee&amp;rdquo; Yang,
zhoufek&lt;/p></description></item><item><title>Blog: Go SDK Exits Experimental in Apache Beam 2.33.0</title><link>/blog/go-sdk-release/</link><pubDate>Thu, 04 Nov 2021 00:00:01 -0800</pubDate><guid>/blog/go-sdk-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Apache Beam’s latest release, version &lt;a href="/get-started/downloads/">2.33.0&lt;/a>, is the first official release of the long experimental Go SDK.
Built with the &lt;a href="https://golang.org/">Go Programming Language&lt;/a>, the Go SDK joins the Java and Python SDKs as the third implementation of the Beam programming model.&lt;/p>
&lt;h2 id="using-the-new-go-sdk">Using the new Go SDK.&lt;/h2>
&lt;p>New users of the Go SDK can start using it in their Go programs by importing the main beam package:&lt;/p>
&lt;pre tabindex="0">&lt;code>import &amp;#34;github.com/apache/beam/sdks/v2/go/pkg/beam&amp;#34;
&lt;/code>&lt;/pre>&lt;p>The next run of &lt;code>go mod tidy&lt;/code> will fetch the latest stable version of the module.
Alternatively executing &lt;code>go get github.com/apache/beam/sdks/v2/go/pkg/beam&lt;/code> will download it to the local module cache immeadiately, and add it to your &lt;code>go.mod&lt;/code> file.&lt;/p>
&lt;p>Existing users of the experimental Go SDK need to update to new &lt;code>v2&lt;/code> import paths to start using the latest versions of the SDK.
This can be done by adding &lt;code>v2&lt;/code> to the import paths, changing &lt;code>github.com/apache/beam/sdks/go/&lt;/code>&amp;hellip; to &lt;code>github.com/apache/beam/sdks/v2/go/&lt;/code>&amp;hellip; where applicable, and then running &lt;code>go mod tidy&lt;/code>.&lt;/p>
&lt;p>Further documentation on using the SDK is available in the &lt;a href="/documentation/programming-guide/">Beam Programming Guide&lt;/a>, and in the package &lt;a href="https://pkg.go.dev/github.com/apache/beam/sdks/v2/go/pkg/beam">Go Doc&lt;/a>.&lt;/p>
&lt;h2 id="feature-support">Feature Support&lt;/h2>
&lt;p>At time of writing, the Go SDK is currently &amp;ldquo;Batteries Not Included&amp;rdquo;.
This means that there are gaps or edge cases in supported IOs and transforms.
That said, the core of the SDK enables a great deal of the Beam Model for
custom user use, supporting the following features:&lt;/p>
&lt;ul>
&lt;li>PTransforms
&lt;ul>
&lt;li>Impulse&lt;/li>
&lt;li>Create&lt;/li>
&lt;li>ParDo with user DoFns
&lt;ul>
&lt;li>Iterable side inputs&lt;/li>
&lt;li>Multiple output emitters&lt;/li>
&lt;li>Receive and return key-value pairs&lt;/li>
&lt;li>SplittableDoFns&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>GroupByKey and CoGroupByKey&lt;/li>
&lt;li>Combine and CombinePerKey with user CombineFns&lt;/li>
&lt;li>Flatten&lt;/li>
&lt;li>Partition&lt;/li>
&lt;li>Composite transforms&lt;/li>
&lt;li>Cross language transforms&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Event time windowing
&lt;ul>
&lt;li>Global, Interval, Sliding, and Session windows&lt;/li>
&lt;li>Aggregating over windowed PCollections with GroupByKeys or Combines&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Coders
&lt;ul>
&lt;li>Primitive Go types (ints, string, []bytes, and more)&lt;/li>
&lt;li>Beam Schemas for Go struct types (including struct, slice, and map fields)&lt;/li>
&lt;li>Registering custom coders&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Metrics
&lt;ul>
&lt;li>PCollection metrics (element counts, size estimates)&lt;/li>
&lt;li>Custom user metrics&lt;/li>
&lt;li>Post job user metrics querying (coming in 2.34.0)&lt;/li>
&lt;li>DoFn profiling metrics (coming in 2.35.0)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Built-in transforms
&lt;ul>
&lt;li>Sum, count, min, max, top, filter&lt;/li>
&lt;li>Scalable TextIO reading&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Upcoming feature roadmap, and known issues are discussed below.
In particular, we plan to support a much richer set of IO connectors via Beam&amp;rsquo;s cross-language capabilities.&lt;/p>
&lt;h2 id="releases">Releases&lt;/h2>
&lt;p>With this release, the Go SDK now uses &lt;a href="https://golang.org/ref/mod">Go Modules&lt;/a> for dependency management.
This makes it so users, SDK authors, and the testing infrastructure can all rely on the same versions of dependencies, making builds reproducible.
This also makes &lt;a href="/blog/validate-beam-release/#configuring-a-go-build-to-validate-a-beam-release-candidate">validating Go SDK Release Candidates simple&lt;/a>.&lt;/p>
&lt;p>Versioned SDK worker containers are now built and &lt;a href="https://hub.docker.com/r/apache/beam_go_sdk/tags?page=1&amp;amp;ordering=last_updated">published&lt;/a>, with the SDK using matching tagged versions.
User jobs no longer need to specify a container to use, except when using custom containers.&lt;/p>
&lt;h2 id="compatibility">Compatibility&lt;/h2>
&lt;p>The Go SDK will largely follow suit with the Go notion of compatibility.
Some concessions are made to keep all SDKs together on the same release cycle.&lt;/p>
&lt;h3 id="language-compatibility">Language Compatibility&lt;/h3>
&lt;p>The SDK will be tested at a minimum &lt;a href="https://golang.org/doc/devel/release">Go Programming Language version of 1.16&lt;/a>, and use available language features and standard library packages accordingly.
To maintain a broad compatibility, the Go SDK will not require the latest major version of Go.
We expect to follow the 2nd newest supported release of the language, with a possible exception when Go 1.18 is released, in order to begin experimenting with &lt;a href="https://go.dev/blog/generics-proposal">Go Generics&lt;/a> in the SDK.
Release notes will call out when the minimum version of the language changes.&lt;/p>
&lt;h3 id="package-compatibility">Package Compatibility&lt;/h3>
&lt;p>The primary user packages will avoid changing in backwards incompatible ways for core features.
This is to be inline with Go&amp;rsquo;s notion of the &lt;a href="https://research.swtch.com/vgo-import">&lt;code>import compatibility rule&lt;/code>&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>If an old package and a new package have the same import path,
the new package must be backwards compatible with the old package.&lt;/p>
&lt;/blockquote>
&lt;p>Exceptions to this policy are around newer, experimental, or in development features and are subject to change.
Such features will have a doc comment noting the experimental status.
Major changes will be mentioned in the release notes.
For example, using &lt;code>beam.WindowInto&lt;/code> with Triggers is currently experimental and may have the API changed in a future release.&lt;/p>
&lt;p>Primary user packages include:&lt;/p>
&lt;ul>
&lt;li>The main beam package &lt;code>github.com/apache/beam/sdks/v2/go/pkg/beam&lt;/code>&lt;/li>
&lt;li>Sub packages under &lt;code>.../transforms&lt;/code>, &lt;code>.../io&lt;/code>, &lt;code>.../runners&lt;/code>, and &lt;code>.../testing&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>Generally, packages in the module other than the primary user packages are for framework use and are at risk of changing.&lt;/p>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;h4 id="batteries-not-included">Batteries not included.&lt;/h4>
&lt;ul>
&lt;li>Current native transforms are undertested&lt;/li>
&lt;li>IOs may not be written to scale&lt;/li>
&lt;li>Go Direct Runner is incomplete and is not portable, prefer using the Python Portable runner, or Flink
&lt;ul>
&lt;li>Doesn&amp;rsquo;t support side input windowing. &lt;a href="https://issues.apache.org/jira/browse/BEAM-13075">BEAM-13075&lt;/a>&lt;/li>
&lt;li>Doesn&amp;rsquo;t serialize data, making it unlikely to catch coder issues &lt;a href="https://issues.apache.org/jira/browse/BEAM-6372">BEAM-6372&lt;/a>&lt;/li>
&lt;li>Can use other general improvements, and become portable &lt;a href="https://issues.apache.org/jira/browse/BEAM-11076">BEAM-11076&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Current Trigger API is under iteration and subject to change &lt;a href="https://issues.apache.org/jira/browse/BEAM-3304">BEAM-3304&lt;/a>
&lt;ul>
&lt;li>API has a possible breaking change between 2.33.0 and 2.34.0, and may change again&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Support of the SDK on services, like Google Cloud Dataflow, remains at the service owner&amp;rsquo;s discretion&lt;/li>
&lt;li>Need something?
&lt;ul>
&lt;li>File a ticket in the &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20component%20%3D%20sdk-go">Beam JIRA&lt;/a> and,&lt;/li>
&lt;li>Email the &lt;a href="mailto:dev@beam.apache.org?subject=%5BGo%20SDK%20Feature%5D">dev@beam.apache.org&lt;/a> list!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="fixed-in-2340">Fixed in 2.34.0&lt;/h4>
&lt;ul>
&lt;li>&lt;code>top.SmallestPerKey&lt;/code> was broken &lt;a href="https://issues.apache.org/jira/browse/BEAM-12946">BEAM-12946&lt;/a>&lt;/li>
&lt;li>&lt;code>beam.TryCrossLanguage&lt;/code> API didn&amp;rsquo;t match non-Try version &lt;a href="https://issues.apache.org/jira/browse/BEAM-9918">BEAM-9918&lt;/a>
&lt;ul>
&lt;li>This is a breaking change if one was calling &lt;code>beam.TryCrossLanguage&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="fixed-in-2350">Fixed in 2.35.0&lt;/h4>
&lt;ul>
&lt;li>Non-global window side inputs don&amp;rsquo;t match (correctness bug) &lt;a href="https://issues.apache.org/jira/browse/BEAM-11087">BEAM-11087&lt;/a>
&lt;ul>
&lt;li>Until 2.35.0 it&amp;rsquo;s not recommended to use side inputs that are not using the global window.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>DoFns using side inputs accumulate memory over bundles, causing out of memory issues &lt;a href="https://issues.apache.org/jira/browse/BEAM-13130">BEAM-13130&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="roadmap">Roadmap&lt;/h2>
&lt;p>The &lt;a href="/roadmap/go-sdk/">SDK roadmap&lt;/a> has been updated.
Ongoing focus is to bolster streaming focused features, improve existing connectors, and make connectors easier to implement.&lt;/p>
&lt;p>In the nearer term this comes in the form of improvements to side inputs, and providing wrappers and improving ease-of-use for cross language transforms from Java.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>We hope you find the SDK useful, and it&amp;rsquo;s still early days.
If you make something with the Go SDK, consider &lt;a href="/community/contact-us/">sharing it with us&lt;/a>.
And remember, &lt;a href="/contribute/">contributions&lt;/a> are always welcome.&lt;/p></description></item><item><title>Blog: Apache Beam 2.33.0</title><link>/blog/beam-2.33.0/</link><pubDate>Thu, 07 Oct 2021 00:00:01 -0800</pubDate><guid>/blog/beam-2.33.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.33.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2330-2021-10-07">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.33.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12350404">detailed release
notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Go SDK is no longer experimental, and is officially part of the Beam release process.
&lt;ul>
&lt;li>Matching Go SDK containers are published on release.&lt;/li>
&lt;li>Batch usage is well supported, and tested on Flink, Spark, and the Python Portable Runner.
&lt;ul>
&lt;li>SDK Tests are also run against Google Cloud Dataflow, but this doesn&amp;rsquo;t indicate reciprocal support.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The SDK supports Splittable DoFns, Cross Language transforms, and most Beam Model basics.&lt;/li>
&lt;li>Go Modules are now used for dependency management.
&lt;ul>
&lt;li>This is a breaking change, see Breaking Changes for resolution.&lt;/li>
&lt;li>Easier path to contribute to the Go SDK, no need to set up a GO_PATH.&lt;/li>
&lt;li>Minimum Go version is now Go v1.16&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>See the announcement blogpost for full information once published.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
{$TOPICS e.g.:}
### I/Os
* Support for X source added (Java) ([BEAM-X](https://issues.apache.org/jira/browse/BEAM-X)).
{$TOPICS}
-->
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Projection pushdown in SchemaIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12609">BEAM-12609&lt;/a>).&lt;/li>
&lt;li>Upgrade Flink runner to Flink versions 1.13.2, 1.12.5 and 1.11.4 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10955">BEAM-10955&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Since release 2.30.0, &amp;ldquo;The AvroCoder changes for BEAM-2303 [changed] the reader/writer from the Avro ReflectDatum* classes to the SpecificDatum* classes&amp;rdquo; (Java). This default behavior change has been reverted in this release. Use the &lt;code>useReflectApi&lt;/code> setting to control it (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12628">BEAM-12628&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations">Deprecations&lt;/h3>
&lt;ul>
&lt;li>Python GBK will stop supporting unbounded PCollections that have global windowing and a default trigger in Beam 2.34. This can be overriden with &lt;code>--allow_unsafe_triggers&lt;/code>. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9487">BEAM-9487&lt;/a>).&lt;/li>
&lt;li>Python GBK will start requiring safe triggers or the &lt;code>--allow_unsafe_triggers&lt;/code> flag starting with Beam 2.34. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9487">BEAM-9487&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>UnsupportedOperationException when reading from BigQuery tables and converting
TableRows to Beam Rows (Java)
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-12479">BEAM-12479&lt;/a>).&lt;/li>
&lt;li>SDFBoundedSourceReader behaves much slower compared with the original behavior
of BoundedSource (Python)
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-12781">BEAM-12781&lt;/a>).&lt;/li>
&lt;li>ORDER BY column not in SELECT crashes (ZetaSQL)
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-12759">BEAM-12759&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Spark 2.x users will need to update Spark&amp;rsquo;s Jackson runtime dependencies (&lt;code>spark.jackson.version&lt;/code>) to at least version 2.9.2, due to Beam updating its dependencies.&lt;/li>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.33.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;li>Go SDK jobs may produce &amp;ldquo;Failed to deduce Step from MonitoringInfo&amp;rdquo; messages following successful job execution. The messages are benign and don&amp;rsquo;t indicate job failure. These are due to not yet handling PCollection metrics.&lt;/li>
&lt;li>Large Java BigQueryIO writes with the FILE_LOADS method will fail in batch mode (specifically, when copy jobs are used).
This results in the error message: &lt;code>IllegalArgumentException: Attempting to access unknown side input&lt;/code>.
Please upgrade to a newer version (&amp;gt; 2.34.0) or use another write method (e.g. &lt;code>STORAGE_WRITE_API&lt;/code>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.33.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay,
Alex Amato,
Alexey Romanenko,
Andreas Bergmeier,
Andres Rodriguez,
Andrew Pilloud,
Andy Xu,
Ankur Goenka,
anthonyqzhu,
Benjamin Gonzalez,
Bhupinder Sindhwani,
Chamikara Jayalath,
Claire McGinty,
Daniel Mateus Pires,
Daniel Oliveira,
David Huntsperger,
Dylan Hercher,
emily,
Emily Ye,
Etienne Chauchot,
Eugene Nikolaiev,
Heejong Lee,
iindyk,
Iñigo San Jose Visiers,
Ismaël Mejía,
Jack McCluskey,
Jan Lukavský,
Jeff Ruane,
Jeremy Lewi,
KevinGG,
Ke Wu,
Kyle Weaver,
lostluck,
Luke Cwik,
Marwan Tammam,
masahitojp,
Mehdi Drissi,
Minbo Bae,
Ning Kang,
Pablo Estrada,
Pascal Gillet,
Pawas Chhokra,
Reuven Lax,
Ritesh Ghorse,
Robert Bradshaw,
Robert Burke,
Rodrigo Benenson,
Ryan Thompson,
Saksham Gupta,
Sam Rohde,
Sam Whittle,
Sayat,
Sayat Satybaldiyev,
Siyuan Chen,
Slava Chernyak,
Steve Niemitz,
Steven Niemitz,
tvalentyn,
Tyson Hamilton,
Udi Meiri,
vachan-shetty,
Venkatramani Rajgopal,
Yichi Zhang,
zhoufek&lt;/p></description></item><item><title>Blog: Apache Beam 2.32.0</title><link>/blog/beam-2.32.0/</link><pubDate>Wed, 25 Aug 2021 00:00:01 -0800</pubDate><guid>/blog/beam-2.32.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.32.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2320-2021-08-11">download page&lt;/a> for this release.&lt;/p>
&lt;!-- more -->
&lt;p>For more information on changes in 2.32.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12349992">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>The &lt;a href="/documentation/dsls/dataframes/overview/">Beam DataFrame
API&lt;/a> is no
longer experimental! We&amp;rsquo;ve spent the time since the &lt;a href="/blog/dataframe-api-preview-available/">2.26.0 preview
announcement&lt;/a>
implementing the most frequently used pandas operations
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9547">BEAM-9547&lt;/a>), improving
&lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.dataframe.html">documentation&lt;/a>
and &lt;a href="https://issues.apache.org/jira/browse/BEAM-12028">error messages&lt;/a>,
adding
&lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/dataframe">examples&lt;/a>,
integrating DataFrames with &lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.runners.interactive.interactive_beam.html">interactive
Beam&lt;/a>,
and of course finding and fixing
&lt;a href="https://issues.apache.org/jira/issues/?jql=project%3DBEAM%20AND%20issuetype%3DBug%20AND%20status%3DResolved%20AND%20component%3Ddsl-dataframe">bugs&lt;/a>.
Leaving experimental just means that we now have high confidence in the API
and recommend its use for production workloads. We will continue to improve
the API, guided by your
&lt;a href="/community/contact-us/">feedback&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added ability to use JdbcIO.Write.withResults without statement and preparedStatementSetter. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12511">BEAM-12511&lt;/a>)&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>Added ability to register URI schemes to use the S3 protocol via FileIO. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12435">BEAM-12435&lt;/a>).&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>Respect number of shards set in SnowflakeWrite batch mode. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12715">BEAM-12715&lt;/a>)&lt;/li>
&lt;li>Java SDK: Update Google Cloud Healthcare IO connectors from using v1beta1 to using the GA version.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Add support to convert Beam Schema to Avro Schema for JDBC LogicalTypes:
&lt;code>VARCHAR&lt;/code>, &lt;code>NVARCHAR&lt;/code>, &lt;code>LONGVARCHAR&lt;/code>, &lt;code>LONGNVARCHAR&lt;/code>, &lt;code>DATE&lt;/code>, &lt;code>TIME&lt;/code>
(Java)(&lt;a href="https://issues.apache.org/jira/browse/BEAM-12385">BEAM-12385&lt;/a>).&lt;/li>
&lt;li>Reading from JDBC source by partitions (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12456">BEAM-12456&lt;/a>).&lt;/li>
&lt;li>PubsubIO can now write to a dead-letter topic after a parsing error (Java)(&lt;a href="https://issues.apache.org/jira/browse/BEAM-12474">BEAM-12474&lt;/a>).&lt;/li>
&lt;li>New append-only option for Elasticsearch sink (Java) &lt;a href="https://issues.apache.org/jira/browse/BEAM-12601">BEAM-12601&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>ListShards (with DescribeStreamSummary) is used instead of DescribeStream to list shards in Kinesis streams. Due to this change, as mentioned in &lt;a href="https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html">AWS documentation&lt;/a>, for fine-grained IAM policies it is required to update them to allow calls to ListShards and DescribeStreamSummary APIs. For more information, see &lt;a href="https://docs.aws.amazon.com/streams/latest/dev/controlling-access.html">Controlling Access to Amazon Kinesis Data Streams&lt;/a> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12225">BEAM-12225&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Python GBK will stop supporting unbounded PCollections that have global windowing and a default trigger in Beam 2.33. This can be overriden with &lt;code>--allow_unsafe_triggers&lt;/code>. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9487">BEAM-9487&lt;/a>).&lt;/li>
&lt;li>Python GBK will start requiring safe triggers or the &lt;code>--allow_unsafe_triggers&lt;/code> flag starting with Beam 2.33. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9487">BEAM-9487&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed race condition in RabbitMqIO causing duplicate acks (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6516">BEAM-6516&lt;/a>))&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.32.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Ajo Thomas, Alex Amato, Alexey Romanenko, Alex Koay, allenpradeep, Anant Damle, Andrew Pilloud, Ankur Goenka, Ashwin Ramaswami, Benjamin Gonzalez, BenWhitehead, Blake Williams, Boyuan Zhang, Brian Hulette, Chamikara Jayalath, Daniel Oliveira, Daniel Thevessen, daria-malkova, David Cavazos, David Huntsperger, dennisylyung, Dennis Yung, dmkozh, egalpin, emily, Esun Kim, Gabriel Melo de Paula, Harch Vardhan, Heejong Lee, heidimhurst, hoshimura, Iñigo San Jose Visiers, Ismaël Mejía, Jack McCluskey, Jan Lukavský, Justin King, Kenneth Knowles, KevinGG, Ke Wu, kileys, Kyle Weaver, Luke Cwik, Maksym Skorupskyi, masahitojp, Matthew Ouyang, Matthias Baetens, Matt Rudary, MiguelAnzoWizeline, Miguel Hernandez, Nikita Petunin, Ning Ding, Ning Kang, odidev, Pablo Estrada, Pascal Gillet, rafal.ochyra, raphael.sanamyan, Reuven Lax, Robert Bradshaw, Robert Burke, roger-mike, Ryan McDowell, Sam Rohde, Sam Whittle, Siyuan Chen, Teng Qiu, Tianzi Cai, Tobias Hermann, Tomo Suzuki, tvalentyn, Tyson Hamilton, Udi Meiri, Valentyn Tymofieiev, Vitaly Terentyev, Yichi Zhang, Yifan Mai, yoshiki.obata, Yu Feng, YuqiHuai, yzhang559, Zachary Houfek, zhoufek&lt;/p></description></item><item><title>Blog: Apache Beam 2.31.0</title><link>/blog/beam-2.31.0/</link><pubDate>Thu, 08 Jul 2021 09:00:00 -0700</pubDate><guid>/blog/beam-2.31.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.31.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2310-2021-07-08">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.31.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12349991">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Fixed bug in ReadFromBigQuery when a RuntimeValueProvider is used as value of table argument (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12514">BEAM-12514&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>&lt;code>CREATE FUNCTION&lt;/code> DDL statement added to Calcite SQL syntax. &lt;code>JAR&lt;/code> and &lt;code>AGGREGATE&lt;/code> are now reserved keywords. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12339">BEAM-12339&lt;/a>).&lt;/li>
&lt;li>Flink 1.13 is now supported by the Flink runner (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12277">BEAM-12277&lt;/a>).&lt;/li>
&lt;li>DatastoreIO: Write and delete operations now follow automatic gradual ramp-up,
in line with best practices (Java/Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12260">BEAM-12260&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-12272">BEAM-12272&lt;/a>).&lt;/li>
&lt;li>Python &lt;code>TriggerFn&lt;/code> has a new &lt;code>may_lose_data&lt;/code> method to signal potential data loss. Default behavior assumes safe (necessary for backwards compatibility). See Deprecations for potential impact of overriding this. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9487">BEAM-9487&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Python Row objects are now sensitive to field order. So &lt;code>Row(x=3, y=4)&lt;/code> is no
longer considered equal to &lt;code>Row(y=4, x=3)&lt;/code> (BEAM-11929).&lt;/li>
&lt;li>Kafka Beam SQL tables now ascribe meaning to the LOCATION field; previously
it was ignored if provided.&lt;/li>
&lt;li>&lt;code>TopCombineFn&lt;/code> disallow &lt;code>compare&lt;/code> as its argument (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7372">BEAM-7372&lt;/a>).&lt;/li>
&lt;li>Drop support for Flink 1.10 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12281">BEAM-12281&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations">Deprecations&lt;/h3>
&lt;ul>
&lt;li>Python GBK will stop supporting unbounded PCollections that have global windowing and a default trigger in Beam 2.33. This can be overriden with &lt;code>--allow_unsafe_triggers&lt;/code>. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9487">BEAM-9487&lt;/a>).&lt;/li>
&lt;li>Python GBK will start requiring safe triggers or the &lt;code>--allow_unsafe_triggers&lt;/code> flag starting with Beam 2.33. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9487">BEAM-9487&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.31.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to &lt;code>git shortlog&lt;/code>, the following people contributed to the 2.31.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, ajo thomas, Alan Myrvold, Alex Amato, Alexey Romanenko,
AlikRodriguez, Anant Damle, Andrew Pilloud, Benjamin Gonzalez, Boyuan Zhang,
Brian Hulette, Chamikara Jayalath, Daniel Oliveira, David Cavazos,
David Huntsperger, David Moravek, Dmytro Kozhevin, dpcollins-google, Emily Ye,
Ernesto Valentino, Evan Galpin, Fernando Morales, Heejong Lee, Ismaël Mejía,
Jan Lukavský, Josias Rico, jrynd, Kenneth Knowles, Ke Wu, kileys, Kyle Weaver,
masahitojp, Matthias Baetens, Maximilian Michels, Milena Bukal,
Nathan J. Mehl, Pablo Estrada, Peter Sobot, Reuven Lax, Robert Bradshaw,
Robert Burke, roger-mike, Sam Rohde, Sam Whittle, Stephan Hoyer, Tom Underhill,
tvalentyn, Uday Singh, Udi Meiri, Vitaly Terentyev, Xinyu Liu, Yichi Zhang,
Yifan Mai, yoshiki.obata, zhoufek&lt;/p></description></item><item><title>Blog: Apache Beam 2.30.0</title><link>/blog/beam-2.30.0/</link><pubDate>Wed, 09 Jun 2021 09:00:00 -0700</pubDate><guid>/blog/beam-2.30.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.30.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2300-2021-06-09">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.30.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12349978">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Legacy Read transform (non-SDF based Read) is used by default for non-FnAPI opensource runners. Use &lt;code>use_sdf_read&lt;/code> experimental flag to re-enable SDF based Read transforms (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10670">BEAM-10670&lt;/a>)&lt;/li>
&lt;li>Upgraded vendored gRPC dependency to 1.36.0 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11227">BEAM-11227&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Fixed the issue that WriteToBigQuery with batch file loads does not respect schema update options when there are multiple load jobs (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11277">BEAM-11277&lt;/a>)&lt;/li>
&lt;li>Fixed the issue that the job didn&amp;rsquo;t properly retry since BigQuery sink swallows HttpErrors when performing streaming inserts (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12362">BEAM-12362&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Added capability to declare resource hints in Java and Python SDKs (&lt;a href="https://issues.apache.org/jira/browse/BEAM-2085">BEAM-2085&lt;/a>)&lt;/li>
&lt;li>Added Spanner IO Performance tests for read and write in Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10029">BEAM-10029&lt;/a>)&lt;/li>
&lt;li>Added support for accessing GCP PubSub Message ordering keys, message IDs and message publish timestamp in Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7819">BEAM-7819&lt;/a>)&lt;/li>
&lt;li>DataFrame API: Added support for collecting DataFrame objects in interactive Beam (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11855">BEAM-11855&lt;/a>)&lt;/li>
&lt;li>DataFrame API: Added &lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/dataframe">apache_beam.examples.dataframe&lt;/a> module (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12024">BEAM-12024&lt;/a>)&lt;/li>
&lt;li>Upgraded the GCP Libraries BOM version to 20.0.0 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11205">BEAM-11205&lt;/a>). For Google Cloud client library versions set by this BOM, see &lt;a href="https://storage.googleapis.com/cloud-opensource-java-dashboard/com.google.cloud/libraries-bom/20.0.0/artifact_details.html">this table&lt;/a>&lt;/li>
&lt;li>Added &lt;code>sdkContainerImage&lt;/code> flag to (eventually) replace &lt;code>workerHarnessContainerImage&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12212">BEAM-12212&lt;/a>)&lt;/li>
&lt;li>Added support for Dataflow update when schemas are used (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12198">BEAM-12198&lt;/a>)&lt;/li>
&lt;li>Fixed the issue that &lt;code>ZipFiles.zipDirectory&lt;/code> leaks native JVM memory (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12220">BEAM-12220&lt;/a>)&lt;/li>
&lt;li>Fixed the issue that &lt;code>Reshuffle.withNumBuckets&lt;/code> creates &lt;code>(N*2)-1&lt;/code> buckets (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12361">BEAM-12361&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Drop support for Flink 1.8 and 1.9 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11948">BEAM-11948&lt;/a>)&lt;/li>
&lt;li>MongoDbIO: Read.withFilter() and Read.withProjection() are removed since they are deprecated since Beam 2.12.0 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12217">BEAM-12217&lt;/a>)&lt;/li>
&lt;li>RedisIO.readAll() was removed since it was deprecated since Beam 2.13.0. Please use RedisIO.readKeyPatterns() for the equivalent functionality (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12214">BEAM-12214&lt;/a>)&lt;/li>
&lt;li>MqttIO.create() with clientId constructor removed because it was deprecated since Beam 2.13.0 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12216">BEAM-12216&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.30.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to &lt;code>git shortlog&lt;/code>, the following people contributed to the 2.30.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Amato, Alexey Romanenko, Anant Damle, Andreas Bergmeier, Andrew Pilloud, Ankur Goenka,
Anup D, Artur Khanin, Benjamin Gonzalez, Bipin Upadhyaya, Boyuan Zhang, Brian Hulette, Bulat Shakirzyanov,
Chamikara Jayalath, Chun Yang, Daniel Kulp, Daniel Oliveira, David Cavazos, Elliotte Rusty Harold, Emily Ye,
Eric Roshan-Eisner, Evan Galpin, Fabien Caylus, Fernando Morales, Heejong Lee, Iñigo San Jose Visiers,
Isidro Martínez, Ismaël Mejía, Ke Wu, Kenneth Knowles, KevinGG, Kyle Weaver, Ludovic Post, MATTHEW Ouyang (LCL),
Mackenzie Clark, Masato Nakamura, Matthias Baetens, Max, Nicholas Azar, Ning Kang, Pablo Estrada, Patrick McCaffrey,
Quentin Sommer, Reuven Lax, Robert Bradshaw, Robert Burke, Rui Wang, Sam Rohde, Sam Whittle, Shoaib Zafar,
Siyuan Chen, Sruthi Sree Kumar, Steve Niemitz, Sylvain Veyrié, Tomo Suzuki, Udi Meiri, Valentyn Tymofieiev,
Vitaly Terentyev, Wenbing, Xinyu Liu, Yichi Zhang, Yifan Mai, Yueyang Qiu, Yunqing Zhou, ajo thomas, brucearctor,
dmkozh, dpcollins-google, emily, jordan-moore, kileys, lostluck, masahitojp, roger-mike, sychen, tvalentyn,
vachan-shetty, yoshiki.obata&lt;/p></description></item><item><title>Blog: How to validate a Beam Release</title><link>/blog/validate-beam-release/</link><pubDate>Tue, 08 Jun 2021 00:00:01 -0800</pubDate><guid>/blog/validate-beam-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Performing new releases is a core responsibility of any software project.
It is even more important in the culture of Apache projects. Releases are
the main flow of new code / features among the community of a project.&lt;/p>
&lt;p>Beam is no exception: We aspire to keep a release cadence of about 6 weeks,
and try to work with the community to release useful new features, and to
keep Beam useful.&lt;/p>
&lt;h3 id="configure-a-java-build-to-validate-a-beam-release-candidate">Configure a Java build to validate a Beam release candidate&lt;/h3>
&lt;p>First of all, it would be useful to have a single property in your &lt;code>pom.xml&lt;/code>
where you keep the global Beam version that you&amp;rsquo;re using. Something like this
in your &lt;code>pom.xml&lt;/code>:&lt;/p>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">properties&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">version&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">26&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">0&lt;/span>&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">version&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">properties&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">dependencies&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">dependency&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">groupId&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">org&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">apache&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">beam&lt;/span>&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">groupId&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">artifactId&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">sdks&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">java&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">core&lt;/span>&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">artifactId&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">version&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">$&lt;/span>&lt;span class="o">{&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">version&lt;/span>&lt;span class="o">}&amp;lt;/&lt;/span>&lt;span class="n">version&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">dependency&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">dependencies&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Second, you can add a new profile to your &lt;code>pom.xml&lt;/code> file. In this new profile,
add a new repository with the staging repository for the new Beam release. For
Beam 2.27.0, this was &lt;code>https://repository.apache.org/content/repositories/orgapachebeam-1149/&lt;/code>.&lt;/p>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">profile&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">validaterelease&lt;/span>&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">repositories&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">repository&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">apache&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">newrelease&lt;/span>&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">$&lt;/span>&lt;span class="o">{&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">release&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">repo&lt;/span>&lt;span class="o">}&amp;lt;/&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">repository&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">repositories&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">profile&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Once you have a &lt;code>beam.version&lt;/code> property in your &lt;code>pom.xml&lt;/code>, and a new profile
with the new release, you can run your &lt;code>mvn&lt;/code> command activating the new profile,
and the new Beam version:&lt;/p>
&lt;pre tabindex="0">&lt;code>mvn test -Pvalidaterelease \
-Dbeam.version=2.27.0 \
-Dbeam.release.repo=https://repository.apache.org/content/repositories/orgapachebeam-XXXX/
&lt;/code>&lt;/pre>&lt;p>This should build your project against the new release, and run basic tests.
It will allow you to run basic validations against the new Beam release.
If you find any issues, then you can share them &lt;em>before&lt;/em> the release is
finalized, so your concerns can be addressed by the community.&lt;/p>
&lt;h3 id="configuring-a-python-build-to-validate-a-beam-release-candidate">Configuring a Python build to validate a Beam release candidate&lt;/h3>
&lt;p>For Python SDK releases, you can install SDK from Pypi, by enabling the
installation of pre-release artifacts.&lt;/p>
&lt;p>First, make sure that your &lt;code>requirements.txt&lt;/code> or &lt;code>setup.py&lt;/code> files allow
for Beam versions above the current one. Something like this should install
the latest available version:&lt;/p>
&lt;pre tabindex="0">&lt;code>apache-beam&amp;lt;=3.0.0
&lt;/code>&lt;/pre>&lt;p>With that, you can ask &lt;code>pip&lt;/code> to install pre-release versions of Beam in your
environment:&lt;/p>
&lt;pre tabindex="0">&lt;code>pip install --pre apache-beam
&lt;/code>&lt;/pre>&lt;p>With that, the Beam version in your environment will be the latest release
candidate, and you can go ahead and run your tests to verify that everything
works well.&lt;/p>
&lt;h3 id="configuring-a-go-build-to-validate-a-beam-release-candidate">Configuring a Go build to validate a Beam release candidate&lt;/h3>
&lt;p>For Go SDK releases, you can fetch the Go SDK RC using &lt;a href="https://golang.org/ref/mod#go-get">&lt;code>go get&lt;/code>&lt;/a>,
by requesting the specific pre-release version.&lt;/p>
&lt;p>For example, to request the first release candidate for 2.34.0:&lt;/p>
&lt;pre tabindex="0">&lt;code>go get -d github.com/apache/beam/sdks/v2@v2.34.0-RC1
&lt;/code>&lt;/pre>&lt;p>With that, the Beam version in your &lt;code>go.mod&lt;/code> will be the specified release candidate.
You can go ahead and run your tests to verify that everything works well.&lt;/p>
&lt;p>You may need to also specify the RC&amp;rsquo;s matching container when running a job.
Use the &lt;code>--environment_config&lt;/code> flag to specify the release candidate container:
eg. &lt;code>--environment_config=apache/beam_go_sdk:2.34.0_rc1&lt;/code>&lt;/p></description></item><item><title>Blog: Apache Beam 2.29.0</title><link>/blog/beam-2.29.0/</link><pubDate>Thu, 29 Apr 2021 09:00:00 -0700</pubDate><guid>/blog/beam-2.29.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.29.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2290-2021-04-15">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.29.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12349629">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Spark Classic and Portable runners officially support Spark 3 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7093">BEAM-7093&lt;/a>).&lt;/li>
&lt;li>Official Java 11 support for most runners (Dataflow, Flink, Spark) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-2530">BEAM-2530&lt;/a>).&lt;/li>
&lt;li>DataFrame API now supports GroupBy.apply (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11628">BEAM-11628&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Added support for S3 filesystem on AWS SDK V2 (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7637">BEAM-7637&lt;/a>)&lt;/li>
&lt;li>GCP BigQuery sink (file loads) uses runner determined sharding for unbounded data (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11772">BEAM-11772&lt;/a>)&lt;/li>
&lt;li>KafkaIO now recognizes the &lt;code>partition&lt;/code> property in writing records (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11806">BEAM-11806&lt;/a>)&lt;/li>
&lt;li>Support for Hadoop configuration on ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11913">BEAM-11913&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>DataFrame API now supports pandas 1.2.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11531">BEAM-11531&lt;/a>).&lt;/li>
&lt;li>Multiple DataFrame API bugfixes (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12071">BEAM-12071&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-11929">BEAM-11929&lt;/a>)&lt;/li>
&lt;li>DDL supported in SQL transforms (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11850">BEAM-11850&lt;/a>)&lt;/li>
&lt;li>Upgrade Flink runner to Flink version 1.12.2 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11941">BEAM-11941&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Deterministic coding enforced for GroupByKey and Stateful DoFns. Previously non-deterministic coding was allowed, resulting in keys not properly being grouped in some cases. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11719">BEAM-11719&lt;/a>)
To restore the old behavior, one can register &lt;code>FakeDeterministicFastPrimitivesCoder&lt;/code> with
&lt;code>beam.coders.registry.register_fallback_coder(beam.coders.coders.FakeDeterministicFastPrimitivesCoder())&lt;/code>
or use the &lt;code>allow_non_deterministic_key_coders&lt;/code> pipeline option.&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations">Deprecations&lt;/h3>
&lt;ul>
&lt;li>Support for Flink 1.8 and 1.9 will be removed in the next release (2.30.0) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11948">BEAM-11948&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.29.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to &lt;code>git shortlog&lt;/code>, the following people contributed to the 2.29.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alex Amato, Alexander Chermenin, Alexey Romanenko,
Allen Pradeep Xavier, Amy Wu, Anant Damle, Andreas Bergmeier, Andrei Balici,
Andrew Pilloud, Andy Xu, Ankur Goenka, Bashir Sadjad, Benjamin Gonzalez, Boyuan
Zhang, Brian Hulette, Chamikara Jayalath, Chinmoy Mandayam, Chuck Yang,
dandy10, Daniel Collins, Daniel Oliveira, David Cavazos, David Huntsperger,
David Moravek, Dmytro Kozhevin, Emily Ye, Esun Kim, Evgeniy Belousov, Filip
Popić, Fokko Driesprong, Gris Cuevas, Heejong Lee, Ihor Indyk, Ismaël Mejía,
Jakub-Sadowski, Jan Lukavský, John Edmonds, Juan Sandoval, 谷口恵輔, Kenneth
Jung, Kenneth Knowles, KevinGG, Kiley Sok, Kyle Weaver, MabelYC, Mackenzie
Clark, Masato Nakamura, Milena Bukal, Miltos, Minbo Bae, Miraç Vuslat Başaran,
mynameborat, Nahian-Al Hasan, Nam Bui, Niel Markwick, Niels Basjes, Ning Kang,
Nir Gazit, Pablo Estrada, Ramazan Yapparov, Raphael Sanamyan, Reuven Lax, Rion
Williams, Robert Bradshaw, Robert Burke, Rui Wang, Sam Rohde, Sam Whittle,
Shehzaad Nakhoda, Shehzaad Nakhoda, Siyuan Chen, Sonam Ramchand, Steve Niemitz,
sychen, Sylvain Veyrié, Tim Robertson, Tobias Kaymak, Tomasz Szerszeń, Tomasz
Szerszeń, Tomo Suzuki, Tyson Hamilton, Udi Meiri, Valentyn Tymofieiev, Yichi
Zhang, Yifan Mai, Yixing Zhang, Yoshiki Obata&lt;/p></description></item><item><title>Blog: Apache Beam 2.28.0</title><link>/blog/beam-2.28.0/</link><pubDate>Mon, 22 Feb 2021 12:00:00 -0800</pubDate><guid>/blog/beam-2.28.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.28.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2280-2021-02-22">download page&lt;/a> for this release.
For more information on changes in 2.28.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12349499">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Many improvements related to Parquet support (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11460">BEAM-11460&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-8202">BEAM-8202&lt;/a>, and &lt;a href="https://issues.apache.org/jira/browse/BEAM-11526">BEAM-11526&lt;/a>)&lt;/li>
&lt;li>Hash Functions in BeamSQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10074">BEAM-10074&lt;/a>)&lt;/li>
&lt;li>Hash functions in ZetaSQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11624">BEAM-11624&lt;/a>)&lt;/li>
&lt;li>Create ApproximateDistinct using HLL Impl (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10324">BEAM-10324&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>SpannerIO supports using BigDecimal for Numeric fields (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11643">BEAM-11643&lt;/a>)&lt;/li>
&lt;li>Add Beam schema support to ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11526">BEAM-11526&lt;/a>)&lt;/li>
&lt;li>Support ParquetTable Writer (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8202">BEAM-8202&lt;/a>)&lt;/li>
&lt;li>GCP BigQuery sink (streaming inserts) uses runner determined sharding (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11408">BEAM-11408&lt;/a>)&lt;/li>
&lt;li>PubSub support types: TIMESTAMP, DATE, TIME, DATETIME (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11533">BEAM-11533&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>ParquetIO add methods &lt;em>readGenericRecords&lt;/em> and &lt;em>readFilesGenericRecords&lt;/em> can read files with an unknown schema. See &lt;a href="https://github.com/apache/beam/pull/13554">PR-13554&lt;/a> and (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11460">BEAM-11460&lt;/a>)&lt;/li>
&lt;li>Added support for thrift in KafkaTableProvider (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11482">BEAM-11482&lt;/a>)&lt;/li>
&lt;li>Added support for HadoopFormatIO to skip key/value clone (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11457">BEAM-11457&lt;/a>)&lt;/li>
&lt;li>Support Conversion to GenericRecords in Convert.to transform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11571">BEAM-11571&lt;/a>).&lt;/li>
&lt;li>Support writes for Parquet Tables in Beam SQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8202">BEAM-8202&lt;/a>).&lt;/li>
&lt;li>Support reading Parquet files with unknown schema (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11460">BEAM-11460&lt;/a>)&lt;/li>
&lt;li>Support user configurable Hadoop Configuration flags for ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11527">BEAM-11527&lt;/a>)&lt;/li>
&lt;li>Expose commit_offset_in_finalize and timestamp_policy to ReadFromKafka (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11677">BEAM-11677&lt;/a>)&lt;/li>
&lt;li>S3 options does not provided to boto3 client while using FlinkRunner and Beam worker pool container (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11799">BEAM-11799&lt;/a>)&lt;/li>
&lt;li>HDFS not deduplicating identical configuration paths (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11329">BEAM-11329&lt;/a>)&lt;/li>
&lt;li>Hash Functions in BeamSQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10074">BEAM-10074&lt;/a>)&lt;/li>
&lt;li>Create ApproximateDistinct using HLL Impl (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10324">BEAM-10324&lt;/a>)&lt;/li>
&lt;li>Add Beam schema support to ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11526">BEAM-11526&lt;/a>)&lt;/li>
&lt;li>Add a Deque Encoder (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11538">BEAM-11538&lt;/a>)&lt;/li>
&lt;li>Hash functions in ZetaSQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11624">BEAM-11624&lt;/a>)&lt;/li>
&lt;li>Refactor ParquetTableProvider (&lt;a href="https://issues.apache.org/jira/browse/">&lt;/a>)&lt;/li>
&lt;li>Add JVM properties to JavaJobServer (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8344">BEAM-8344&lt;/a>)&lt;/li>
&lt;li>Single source of truth for supported Flink versions (&lt;a href="https://issues.apache.org/jira/browse/">&lt;/a>)&lt;/li>
&lt;li>Use metric for Python BigQuery streaming insert API latency logging (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11018">BEAM-11018&lt;/a>)&lt;/li>
&lt;li>Use metric for Java BigQuery streaming insert API latency logging (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11032">BEAM-11032&lt;/a>)&lt;/li>
&lt;li>Upgrade Flink runner to Flink versions 1.12.1 and 1.11.3 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11697">BEAM-11697&lt;/a>)&lt;/li>
&lt;li>Upgrade Beam base image to use Tensorflow 2.4.1 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11762">BEAM-11762&lt;/a>)&lt;/li>
&lt;li>Create Beam GCP BOM (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11665">BEAM-11665&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Java artifacts &amp;ldquo;beam-sdks-java-io-kinesis&amp;rdquo;, &amp;ldquo;beam-sdks-java-io-google-cloud-platform&amp;rdquo;, and
&amp;ldquo;beam-sdks-java-extensions-sql-zetasql&amp;rdquo; declare Guava 30.1-jre dependency (It was 25.1-jre in Beam 2.27.0).
This new Guava version may introduce dependency conflicts if your project or dependencies rely
on removed APIs. If affected, ensure to use an appropriate Guava version via &lt;code>dependencyManagement&lt;/code> in Maven and
&lt;code>force&lt;/code> in Gradle.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.28.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Amato, Alexey Romanenko, Allen Pradeep Xavier, Anant Damle, Artur Khanin,
Boyuan Zhang, Brian Hulette, Chamikara Jayalath, Chris Roth, Costi Ciudatu, Damon Douglas,
Daniel Collins, Daniel Oliveira, David Cavazos, David Huntsperger, Elliotte Rusty Harold,
Emily Ye, Etienne Chauchot, Etta Rapp, Evan Palmer, Eyal, Filip Krakowski, Fokko Driesprong,
Heejong Lee, Ismaël Mejía, janeliulwq, Jan Lukavský, John Edmonds, Jozef Vilcek, Kenneth Knowles
Ke Wu, kileys, Kyle Weaver, MabelYC, masahitojp, Masato Nakamura, Milena Bukal, Miraç Vuslat Başaran,
Nelson Osacky, Niel Markwick, Ning Kang, omarismail94, Pablo Estrada, Piotr Szuberski,
ramazan-yapparov, Reuven Lax, Reza Rokni, rHermes, Robert Bradshaw, Robert Burke, Robert Gruener,
Romster, Rui Wang, Sam Whittle, shehzaadn-vd, Siyuan Chen, Sonam Ramchand, Tobiasz Kędzierski,
Tomo Suzuki, tszerszen, tvalentyn, Tyson Hamilton, Udi Meiri, Xinbin Huang, Yichi Zhang,
Yifan Mai, yoshiki.obata, Yueyang Qiu, Yusaku Matsuki&lt;/p></description></item><item><title>Blog: Example to ingest data from Apache Kafka to Google Cloud Pub/Sub</title><link>/blog/kafka-to-pubsub-example/</link><pubDate>Fri, 15 Jan 2021 00:00:01 -0800</pubDate><guid>/blog/kafka-to-pubsub-example/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>In this blog post we present an example that creates a pipeline to read data from a single topic or
multiple topics from &lt;a href="https://kafka.apache.org/">Apache Kafka&lt;/a> and write data into a topic
in &lt;a href="https://cloud.google.com/pubsub">Google Pub/Sub&lt;/a>. The example provides code samples to implement
simple yet powerful pipelines and also provides an out-of-the-box solution that you can just &lt;em>&amp;quot;
plug&amp;rsquo;n&amp;rsquo;play&amp;quot;&lt;/em>.&lt;/p>
&lt;p>This end-to-end example is included
in &lt;a href="/blog/beam-2.27.0/">Apache Beam release 2.27&lt;/a>
and can be downloaded &lt;a href="/get-started/downloads/#2270-2020-12-22">here&lt;/a>.&lt;/p>
&lt;p>We hope you will find this example useful for setting up data pipelines between Kafka and Pub/Sub.&lt;/p>
&lt;h1 id="example-specs">Example specs&lt;/h1>
&lt;p>Supported data formats:&lt;/p>
&lt;ul>
&lt;li>Serializable plain text formats, such as JSON&lt;/li>
&lt;li>&lt;a href="https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage">PubSubMessage&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Supported input source configurations:&lt;/p>
&lt;ul>
&lt;li>Single or multiple Apache Kafka bootstrap servers&lt;/li>
&lt;li>Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection&lt;/li>
&lt;li>Secrets vault service &lt;a href="https://www.vaultproject.io/">HashiCorp Vault&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Supported destination configuration:&lt;/p>
&lt;ul>
&lt;li>Single Google Pub/Sub topic&lt;/li>
&lt;/ul>
&lt;p>In a simple scenario, the example will create an Apache Beam pipeline that will read messages from a
source Kafka server with a source topic, and stream the text messages into specified Pub/Sub
destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed
over plaintext or SSL encrypted connection. The example supports using a single Kafka user account
to authenticate in the provided source Kafka servers and topics. To support SASL authentication over
SSL the example will need an SSL certificate location and access to a secrets vault service with
Kafka username and password, currently supporting HashiCorp Vault.&lt;/p>
&lt;h1 id="where-can-i-run-this-example">Where can I run this example?&lt;/h1>
&lt;p>There are two ways to execute the pipeline.&lt;/p>
&lt;ol>
&lt;li>Locally. This way has many options - run directly from your IntelliJ, or create &lt;code>.jar&lt;/code> file and
run it in the terminal, or use your favourite method of running Beam pipelines.&lt;/li>
&lt;li>In &lt;a href="https://cloud.google.com/">Google Cloud&lt;/a> using Google
Cloud &lt;a href="https://cloud.google.com/dataflow">Dataflow&lt;/a>:
&lt;ul>
&lt;li>With &lt;code>gcloud&lt;/code> command-line tool you can create
a &lt;a href="https://cloud.google.com/dataflow/docs/concepts/dataflow-templates">Flex Template&lt;/a>
out of this Beam example and execute it in Google Cloud Platform. &lt;em>This requires corresponding
modifications of the example to turn it into a template.&lt;/em>&lt;/li>
&lt;li>This example exists as
a &lt;a href="https://github.com/GoogleCloudPlatform/DataflowTemplates/tree/master/v2/kafka-to-pubsub">Flex Template version&lt;/a>
within &lt;a href="https://github.com/GoogleCloudPlatform/DataflowTemplates">Google Cloud Dataflow Template Pipelines&lt;/a>
repository and can be run with no additional code modifications.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h1 id="next-steps">Next Steps&lt;/h1>
&lt;p>Give this &lt;strong>Beam end-to-end example&lt;/strong> a try. If you are new to Beam, we hope this example will give
you more understanding on how pipelines work and look like. If you are already using Beam, we hope
some code samples in it will be useful for your use cases.&lt;/p>
&lt;p>Please
&lt;a href="/community/contact-us/">let us know&lt;/a> if you encounter any issues.&lt;/p></description></item><item><title>Blog: Apache Beam 2.27.0</title><link>/blog/beam-2.27.0/</link><pubDate>Thu, 07 Jan 2021 12:00:00 -0800</pubDate><guid>/blog/beam-2.27.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.27.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2270-2020-12-22">download page&lt;/a> for this release.
For more information on changes in 2.27.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12349380">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Java 11 Containers are now published with all Beam releases.&lt;/li>
&lt;li>There is a new transform &lt;code>ReadAllFromBigQuery&lt;/code> that can receive multiple requests to read data from BigQuery at pipeline runtime. See &lt;a href="https://github.com/apache/beam/pull/13170">PR 13170&lt;/a>, and &lt;a href="https://issues.apache.org/jira/browse/BEAM-9650">BEAM-9650&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>ReadFromMongoDB can now be used with MongoDB Atlas (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11266">BEAM-11266&lt;/a>.)&lt;/li>
&lt;li>ReadFromMongoDB/WriteToMongoDB will mask password in display_data (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11444">BEAM-11444&lt;/a>.)&lt;/li>
&lt;li>There is a new transform &lt;code>ReadAllFromBigQuery&lt;/code> that can receive multiple requests to read data from BigQuery at pipeline runtime. See &lt;a href="https://github.com/apache/beam/pull/13170">PR 13170&lt;/a>, and &lt;a href="https://issues.apache.org/jira/browse/BEAM-9650">BEAM-9650&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Beam modules that depend on Hadoop are now tested for compatibility with Hadoop 3 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8569">BEAM-8569&lt;/a>). (Hive/HCatalog pending)&lt;/li>
&lt;li>Publishing Java 11 SDK container images now supported as part of Apache Beam release process. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8106">BEAM-8106&lt;/a>)&lt;/li>
&lt;li>Added Cloud Bigtable Provider extension to Beam SQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11173">BEAM-11173&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-11373">BEAM-11373&lt;/a>)&lt;/li>
&lt;li>Added a schema provider for thrift data (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11338">BEAM-11338&lt;/a>)&lt;/li>
&lt;li>Added combiner packing pipeline optimization to Dataflow runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10641">BEAM-10641&lt;/a>)&lt;/li>
&lt;li>Added an example to ingest data from Apache Kafka to Google Pub/Sub. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11065">BEAM-11065&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>HBaseIO hbase-shaded-client dependency should be now provided by the users (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9278">BEAM-9278&lt;/a>).&lt;/li>
&lt;li>&lt;code>--region&lt;/code> flag in amazon-web-services2 was replaced by &lt;code>--awsRegion&lt;/code> (&lt;a href="https://issues.apache.org/jira/projects/BEAM/issues/BEAM-11331">BEAM-11331&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.27.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alex Amato, Alexey Romanenko, Aliraza Nagamia, Allen Pradeep Xavier,
Andrew Pilloud, andreyKaparulin, Ashwin Ramaswami, Boyuan Zhang, Brent Worden, Brian Hulette,
Carlos Marin, Chamikara Jayalath, Costi Ciudatu, Damon Douglas, Daniel Collins,
Daniel Oliveira, David Huntsperger, David Lu, David Moravek, David Wrede,
dennis, Dennis Yung, dpcollins-google, Emily Ye, emkornfield,
Esun Kim, Etienne Chauchot, Eugene Nikolaiev, Frank Zhao, Haizhou Zhao,
Hector Acosta, Heejong Lee, Ilya, Iñigo San Jose Visiers, InigoSJ,
Ismaël Mejía, janeliulwq, Jan Lukavský, Kamil Wasilewski, Kenneth Jung,
Kenneth Knowles, Ke Wu, kileys, Kyle Weaver, lostluck,
Matt Casters, Maximilian Michels, Michal Walenia, Mike Dewar, nehsyc,
Nelson Osacky, Niels Basjes, Ning Kang, Pablo Estrada, palmere-google,
Pawel Pasterz, Piotr Szuberski, purbanow, Reuven Lax, rHermes,
Robert Bradshaw, Robert Burke, Rui Wang, Sam Rohde, Sam Whittle,
Siyuan Chen, Tim Robertson, Tobiasz Kędzierski, tszerszen,
Valentyn Tymofieiev, Tyson Hamilton, Udi Meiri, vachan-shetty, Xinyu Liu,
Yichi Zhang, Yifan Mai, yoshiki.obata, Yueyang Qiu&lt;/p></description></item><item><title>Blog: DataFrame API Preview now Available!</title><link>/blog/dataframe-api-preview-available/</link><pubDate>Wed, 16 Dec 2020 09:09:41 -0800</pubDate><guid>/blog/dataframe-api-preview-available/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We&amp;rsquo;re excited to announce that a preview of the Beam Python SDK&amp;rsquo;s new DataFrame
API is now available in &lt;a href="/blog/beam-2.26.0/">Beam
2.26.0&lt;/a>. Much like &lt;code>SqlTransform&lt;/code>
(&lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/extensions/sql/SqlTransform.html">Java&lt;/a>,
&lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.sql.html#apache_beam.transforms.sql.SqlTransform">Python&lt;/a>),
the DataFrame API gives Beam users a way to express complex
relational logic much more concisely than previously possible.&lt;/p>
&lt;h2 id="a-more-expressive-api">A more expressive API&lt;/h2>
&lt;p>Beam&amp;rsquo;s new DataFrame API aims to be compatible with the well known
&lt;a href="https://pandas.pydata.org/pandas-docs/stable/index.html">Pandas&lt;/a>
DataFrame API, with a few caveats detailed below. With this new API a simple
pipeline that reads NYC taxiride data from a CSV, performs a grouped
aggregation, and writes the output to CSV, can be expressed very concisely:&lt;/p>
&lt;pre tabindex="0">&lt;code>from apache_beam.dataframe.io import read_csv
with beam.Pipeline() as p:
df = p | read_csv(&amp;#34;gs://apache-beam-samples/nyc_taxi/2019/*.csv&amp;#34;,
usecols=[&amp;#39;passenger_count&amp;#39; , &amp;#39;DOLocationID&amp;#39;])
# Count the number of passengers dropped off per LocationID
agg = df.groupby(&amp;#39;DOLocationID&amp;#39;).sum()
agg.to_csv(output)
&lt;/code>&lt;/pre>&lt;p>Compare this to the same logic implemented as a conventional Beam python
pipeline with a &lt;code>CombinePerKey&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>with beam.Pipeline() as p:
(p | beam.io.ReadFromText(&amp;#34;gs://apache-beam-samples/nyc_taxi/2019/*.csv&amp;#34;,
skip_header_lines=1)
| beam.Map(lambda line: line.split(&amp;#39;,&amp;#39;))
# Parse CSV, create key - value pairs
| beam.Map(lambda splits: (int(splits[8] or 0), # DOLocationID
int(splits[3] or 0))) # passenger_count
# Sum values per key
| beam.CombinePerKey(sum)
| beam.MapTuple(lambda loc_id, pc: f&amp;#39;{loc_id},{pc}&amp;#39;)
| beam.io.WriteToText(known_args.output))
&lt;/code>&lt;/pre>&lt;p>The DataFrame example is much easier to quickly inspect and understand, as it
allows you to concisely express grouped aggregations without using the low-level
&lt;code>CombinePerKey&lt;/code>.&lt;/p>
&lt;p>In addition to being more expressive, a pipeline written with the DataFrame API
can often be more efficient than a conventional Beam pipeline. This is because
the DataFrame API defers to the very efficient, columnar Pandas implementation
as much as possible.&lt;/p>
&lt;h2 id="dataframes-as-a-dsl">DataFrames as a DSL&lt;/h2>
&lt;p>You may already be aware of &lt;a href="/documentation/dsls/sql/overview/">Beam
SQL&lt;/a>, which is
a Domain-Specific Language (DSL) built with Beam&amp;rsquo;s Java SDK. SQL is
considered a DSL because it&amp;rsquo;s possible to express a full pipeline, including IOs
and complex operations, entirely with SQL. &lt;/p>
&lt;p>Similarly, the DataFrame API is a DSL built with the Python SDK. You can see
that the above example is written without traditional Beam constructs like IOs,
ParDo, or CombinePerKey. In fact the only traditional Beam type is the Pipeline
instance! Otherwise this pipeline is written completely using the DataFrame API.
This is possible because the DataFrame API doesn&amp;rsquo;t just implement Pandas'
computation operations, it also includes IOs based on the Pandas native
implementations (&lt;code>pd.read_{csv,parquet,...}&lt;/code> and &lt;code>pd.DataFrame.to_{csv,parquet,...}&lt;/code>).&lt;/p>
&lt;p>Like SQL, it&amp;rsquo;s also possible to embed the DataFrame API into a larger pipeline
by using
&lt;a href="/documentation/programming-guide/#what-is-a-schema">schemas&lt;/a>.
A schema-aware PCollection can be converted to a DataFrame, processed, and the
result converted back to another schema-aware PCollection. For example, if you
wanted to use traditional Beam IOs rather than one of the DataFrame IOs you
could rewrite the above pipeline like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>from apache_beam.dataframe.convert import to_dataframe
from apache_beam.dataframe.convert import to_pcollection
with beam.Pipeline() as p:
...
schema_pc = (p | beam.ReadFromText(..)
# Use beam.Select to assign a schema
| beam.Select(DOLocationID=lambda line: int(...),
passenger_count=lambda line: int(...)))
df = to_dataframe(schema_pc)
agg = df.groupby(&amp;#39;DOLocationID&amp;#39;).sum()
agg_pc = to_pcollection(pc)
# agg_pc has a schema based on the structure of agg
(agg_pc | beam.Map(lambda row: f&amp;#39;{row.DOLocationID},{row.passenger_count}&amp;#39;)
| beam.WriteToText(..))
&lt;/code>&lt;/pre>&lt;p>It&amp;rsquo;s also possible to use the DataFrame API by passing a function to
&lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.dataframe.transforms.html#apache_beam.dataframe.transforms.DataframeTransform">&lt;code>DataframeTransform&lt;/code>&lt;/a>:&lt;/p>
&lt;pre tabindex="0">&lt;code>from apache_beam.dataframe.transforms import DataframeTransform
with beam.Pipeline() as p:
...
| beam.Select(DOLocationID=lambda line: int(..),
passenger_count=lambda line: int(..))
| DataframeTransform(lambda df: df.groupby(&amp;#39;DOLocationID&amp;#39;).sum())
| beam.Map(lambda row: f&amp;#39;{row.DOLocationID},{row.passenger_count}&amp;#39;)
...
&lt;/code>&lt;/pre>&lt;h2 id="caveats">Caveats&lt;/h2>
&lt;p>As hinted above, there are some differences between Beam&amp;rsquo;s DataFrame API and the
Pandas API. The most significant difference is that the Beam DataFrame API is
&lt;em>deferred&lt;/em>, just like the rest of the Beam API. This means that you can&amp;rsquo;t
&lt;code>print()&lt;/code> a DataFrame instance in order to inspect the data, because we haven&amp;rsquo;t
computed the data yet! The computation doesn&amp;rsquo;t take place until the pipeline is
&lt;code>run()&lt;/code>. Before that, we only know about the shape/schema of the result (i.e.
the names and types of the columns), and not the result itself.&lt;/p>
&lt;p>There are a few common exceptions you will likely see when attempting to use
certain Pandas operations:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>NotImplementedError:&lt;/strong> Indicates this is an operation or argument that we
haven&amp;rsquo;t had time to look at yet. We&amp;rsquo;ve tried to make as many Pandas operations
as possible available in the Preview offering of this new API, but there&amp;rsquo;s
still a long tail of operations to go.&lt;/li>
&lt;li>&lt;strong>WontImplementError:&lt;/strong> Indicates this is an operation or argument we do not
intend to support in the near-term because it&amp;rsquo;s incompatible with the Beam
model. The largest class of operations that raise this error are those that
are order sensitive (e.g. shift, cummax, cummin, head, tail, etc..). These
cannot be trivially mapped to Beam because PCollections, representing
distributed datasets, are unordered. Note that even some of these operations
&lt;em>may&lt;/em> get implemented in the future - we actually have some ideas for how we
might support order sensitive operations - but it&amp;rsquo;s a ways off.&lt;/li>
&lt;/ul>
&lt;p>Finally, it&amp;rsquo;s important to note that this is a preview of a new feature that
will get hardened over the next few Beam releases. We would love for you to try
it out now and give us some feedback, but we do not yet recommend it for use in
production workloads.&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved&lt;/h2>
&lt;p>The easiest way to get involved with this effort is to try out DataFrames and
let us know what you think! You can send questions to &lt;a href="mailto:user@beam.apache.org">user@beam.apache.org&lt;/a>, or
file bug reports and feature requests in &lt;a href="https://issues.apache.org/jira">jira&lt;/a>.
In particular, it would be really helpful to know if there&amp;rsquo;s an operation we
haven&amp;rsquo;t implemented yet that you&amp;rsquo;d find useful, so that we can prioritize it.&lt;/p>
&lt;p>If you&amp;rsquo;d like to learn more about how the DataFrame API works under the hood and
get involved with the development we recommend you take a look at the
&lt;a href="https://s.apache.org/beam-dataframes">design doc&lt;/a>
and our &lt;a href="https://2020.beamsummit.org/sessions/simpler-python-pipelines/">Beam summit
presentation&lt;/a>.
From there the best way to help is to knock out some of those not implemented
operations. We&amp;rsquo;re coordinating that work in
&lt;a href="https://issues.apache.org/jira/browse/BEAM-9547">BEAM-9547&lt;/a>.&lt;/p></description></item><item><title>Blog: Splittable DoFn in Apache Beam is Ready to Use</title><link>/blog/splittable-do-fn-is-available/</link><pubDate>Mon, 14 Dec 2020 00:00:01 -0800</pubDate><guid>/blog/splittable-do-fn-is-available/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are pleased to announce that Splittable DoFn (SDF) is ready for use in the Beam Python, Java,
and Go SDKs for versions 2.25.0 and later.&lt;/p>
&lt;p>In 2017, &lt;a href="/blog/splittable-do-fn/">Splittable DoFn Blog Post&lt;/a> proposed
to build &lt;a href="https://s.apache.org/splittable-do-fn">Splittable DoFn&lt;/a> APIs as the new recommended way of
building I/O connectors. Splittable DoFn is a generalization of &lt;code>DoFn&lt;/code> that gives it the core
capabilities of &lt;code>Source&lt;/code> while retaining &lt;code>DoFn&lt;/code>&amp;rsquo;s syntax, flexibility, modularity, and ease of
coding. Thus, it becomes much easier to develop complex I/O connectors with simpler and reusable
code.&lt;/p>
&lt;p>SDF has three advantages over the existing &lt;code>UnboundedSource&lt;/code> and &lt;code>BoundedSource&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>SDF provides a unified set of APIs to handle both unbounded and bounded cases.&lt;/li>
&lt;li>SDF enables reading from source descriptors dynamically.
&lt;ul>
&lt;li>Taking KafkaIO as an example, within &lt;code>UnboundedSource&lt;/code>/&lt;code>BoundedSource&lt;/code> API, you must specify
the topic and partition you want to read from during pipeline construction time. There is no way
for &lt;code>UnboundedSource&lt;/code>/&lt;code>BoundedSource&lt;/code> to accept topics and partitions as inputs during execution
time. But it&amp;rsquo;s built-in to SDF.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>SDF fits in as any node on a pipeline freely with the ability of splitting.
&lt;ul>
&lt;li>&lt;code>UnboundedSource&lt;/code>/&lt;code>BoundedSource&lt;/code> has to be the root node of the pipeline to gain performance
benefits from splitting strategies, which limits many real-world usages. This is no longer a limit
for an SDF.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>As SDF is now ready to use with all the mentioned improvements, it is the recommended
way to build the new I/O connectors. Try out building your own Splittable DoFn by following the
&lt;a href="/documentation/programming-guide/#splittable-dofns">programming guide&lt;/a>. We
have provided tonnes of common utility classes such as common types of &lt;code>RestrictionTracker&lt;/code> and
&lt;code>WatermarkEstimator&lt;/code> in Beam SDK, which will help you onboard easily. As for the existing I/O
connectors, we have wrapped &lt;code>UnboundedSource&lt;/code> and &lt;code>BoundedSource&lt;/code> implementations into Splittable
DoFns, yet we still encourage developers to convert &lt;code>UnboundedSource&lt;/code>/&lt;code>BoundedSource&lt;/code> into actual
Splittable DoFn implementation to gain more performance benefits.&lt;/p>
&lt;p>Many thanks to every contributor who brought this highly anticipated design into the data processing
world. We are really excited to see that users benefit from SDF.&lt;/p>
&lt;p>Below are some real-world SDF examples for you to explore.&lt;/p>
&lt;h2 id="real-world-splittable-dofn-examples">Real world Splittable DoFn examples&lt;/h2>
&lt;p>&lt;strong>Java Examples&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/571338b0cc96e2e80f23620fe86de5c92dffaccc/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java#L118">Kafka&lt;/a>:
An I/O connector for &lt;a href="https://kafka.apache.org/">Apache Kafka&lt;/a>
(an open-source distributed event streaming platform).&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/571338b0cc96e2e80f23620fe86de5c92dffaccc/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Watch.java#L787">Watch&lt;/a>:
Uses a polling function producing a growing set of outputs for each input until a per-input
termination condition is met.&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/571338b0cc96e2e80f23620fe86de5c92dffaccc/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java#L365">Parquet&lt;/a>:
An I/O connector for &lt;a href="https://parquet.apache.org/">Apache Parquet&lt;/a>
(an open-source columnar storage format).&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/6fdde4f4eab72b49b10a8bb1cb3be263c5c416b5/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/healthcare/HL7v2IO.java#L493">HL7v2&lt;/a>:
An I/O connector for HL7v2 messages (a clinical messaging format that provides data about events
that occur inside an organization) part of
&lt;a href="https://cloud.google.com/healthcare">Google’s Cloud Healthcare API&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/571338b0cc96e2e80f23620fe86de5c92dffaccc/sdks/java/core/src/main/java/org/apache/beam/sdk/io/Read.java#L248">BoundedSource wrapper&lt;/a>:
A wrapper which converts an existing &lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/BoundedSource.html">BoundedSource&lt;/a>
implementation to a splittable DoFn.&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/571338b0cc96e2e80f23620fe86de5c92dffaccc/sdks/java/core/src/main/java/org/apache/beam/sdk/io/Read.java#L432">UnboundedSource wrapper&lt;/a>:
A wrapper which converts an existing &lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/UnboundedSource.html">UnboundedSource&lt;/a>
implementation to a splittable DoFn.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Python Examples&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/571338b0cc96e2e80f23620fe86de5c92dffaccc/sdks/python/apache_beam/io/iobase.py#L1375">BoundedSourceWrapper&lt;/a>:
A wrapper which converts an existing &lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.io.iobase.html#apache_beam.io.iobase.BoundedSource">BoundedSource&lt;/a>
implementation to a splittable DoFn.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Go Examples&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/ce190e11332469ea59b6c9acf16ee7c673ccefdd/sdks/go/pkg/beam/io/textio/sdf.go#L40">textio.ReadSdf&lt;/a> implements reading from text files using a splittable DoFn.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Apache Beam 2.26.0</title><link>/blog/beam-2.26.0/</link><pubDate>Fri, 11 Dec 2020 12:00:00 -0800</pubDate><guid>/blog/beam-2.26.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.26.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2260-2020-12-11">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.26.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12348833">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Splittable DoFn is now the default for executing the Read transform for Java based runners (Spark with bounded pipelines) in addition to existing runners from the 2.25.0 release (Direct, Flink, Jet, Samza, Twister2). The expected output of the Read transform is unchanged. Users can opt-out using &lt;code>--experiments=use_deprecated_read&lt;/code>. The Apache Beam community is looking for feedback for this change as the community is planning to make this change permanent with no opt-out. If you run into an issue requiring the opt-out, please send an e-mail to &lt;a href="mailto:user@beam.apache.org">user@beam.apache.org&lt;/a> specifically referencing BEAM-10670 in the subject line and why you needed to opt-out. (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10670">BEAM-10670&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Java BigQuery streaming inserts now have timeouts enabled by default. Pass &lt;code>--HTTPWriteTimeout=0&lt;/code> to revert to the old behavior. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6103">BEAM-6103&lt;/a>)&lt;/li>
&lt;li>Added support for Contextual Text IO (Java), a version of text IO that provides metadata about the records (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10124">BEAM-10124&lt;/a>). Support for this IO is currently experimental. Specifically, &lt;strong>there are no update-compatibility guarantees&lt;/strong> for streaming jobs with this IO between current future verisons of Apache Beam SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added support for avro payload format in Beam SQL Kafka Table (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10885">BEAM-10885&lt;/a>)&lt;/li>
&lt;li>Added support for json payload format in Beam SQL Kafka Table (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10893">BEAM-10893&lt;/a>)&lt;/li>
&lt;li>Added support for protobuf payload format in Beam SQL Kafka Table (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10892">BEAM-10892&lt;/a>)&lt;/li>
&lt;li>Added support for avro payload format in Beam SQL Pubsub Table (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5504">BEAM-5504&lt;/a>)&lt;/li>
&lt;li>Added option to disable unnecessary copying between operators in Flink Runner (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11146">BEAM-11146&lt;/a>)&lt;/li>
&lt;li>Added CombineFn.setup and CombineFn.teardown to Python SDK. These methods let you initialize the CombineFn&amp;rsquo;s state before any of the other methods of the CombineFn is executed and clean that state up later on. If you are using Dataflow, you need to enable Dataflow Runner V2 by passing &lt;code>--experiments=use_runner_v2&lt;/code> before using this feature. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-3736">BEAM-3736&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>BigQuery&amp;rsquo;s DATETIME type now maps to Beam logical type org.apache.beam.sdk.schemas.logicaltypes.SqlTypes.DATETIME&lt;/li>
&lt;li>Pandas 1.x is now required for dataframe operations.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.26.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abhishek Yadav, AbhiY98, Ahmet Altay, Alan Myrvold, Alex Amato, Alexey Romanenko,
Andrew Pilloud, Ankur Goenka, Boyuan Zhang, Brian Hulette, Chad Dombrova,
Chamikara Jayalath, Curtis &amp;ldquo;Fjord&amp;rdquo; Hawthorne, Damon Douglas, dandy10, Daniel Oliveira,
David Cavazos, dennis, Derrick Qin, dpcollins-google, Dylan Hercher, emily, Esun Kim,
Gleb Kanterov, Heejong Lee, Ismaël Mejía, Jan Lukavský, Jean-Baptiste Onofré, Jing,
Jozef Vilcek, Justin White, Kamil Wasilewski, Kenneth Knowles, kileys, Kyle Weaver,
lostluck, Luke Cwik, Mark, Maximilian Michels, Milan Cermak, Mohammad Hossein Sekhavat,
Nelson Osacky, Neville Li, Ning Kang, pabloem, Pablo Estrada, pawelpasterz,
Pawel Pasterz, Piotr Szuberski, PoojaChandak, purbanow, rarokni, Ravi Magham,
Reuben van Ammers, Reuven Lax, Reza Rokni, Robert Bradshaw, Robert Burke,
Romain Manni-Bucau, Rui Wang, rworley-monster, Sam Rohde, Sam Whittle, shollyman,
Simone Primarosa, Siyuan Chen, Steve Niemitz, Steven van Rossum, sychen, Teodor Spæren,
Tim Clemons, Tim Robertson, Tobiasz Kędzierski, tszerszen, Tudor Marian, tvalentyn,
Tyson Hamilton, Udi Meiri, Vasu Gupta, xasm83, Yichi Zhang, yichuan66, Yifan Mai,
yoshiki.obata, Yueyang Qiu, yukihira1992&lt;/p></description></item><item><title>Blog: Apache Beam 2.25.0</title><link>/blog/beam-2.25.0/</link><pubDate>Fri, 23 Oct 2020 14:00:00 -0800</pubDate><guid>/blog/beam-2.25.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.25.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2250-2020-10-23">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.25.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347147">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Splittable DoFn is now the default for executing the Read transform for Java based runners (Direct, Flink, Jet, Samza, Twister2). The expected output of the Read transform is unchanged. Users can opt-out using &lt;code>--experiments=use_deprecated_read&lt;/code>. The Apache Beam community is looking for feedback for this change as the community is planning to make this change permanent with no opt-out. If you run into an issue requiring the opt-out, please send an e-mail to &lt;a href="mailto:user@beam.apache.org">user@beam.apache.org&lt;/a> specifically referencing BEAM-10670 in the subject line and why you needed to opt-out. (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10670">BEAM-10670&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added cross-language support to Java&amp;rsquo;s KinesisIO, now available in the Python module &lt;code>apache_beam.io.kinesis&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10138">BEAM-10138&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-10137">BEAM-10137&lt;/a>).&lt;/li>
&lt;li>Update Snowflake JDBC dependency for SnowflakeIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10864">BEAM-10864&lt;/a>)&lt;/li>
&lt;li>Added cross-language support to Java&amp;rsquo;s SnowflakeIO.Write, now available in the Python module &lt;code>apache_beam.io.snowflake&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9898">BEAM-9898&lt;/a>).&lt;/li>
&lt;li>Added delete function to Java&amp;rsquo;s &lt;code>ElasticsearchIO#Write&lt;/code>. Now, Java&amp;rsquo;s ElasticsearchIO can be used to selectively delete documents using &lt;code>withIsDeleteFn&lt;/code> function (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5757">BEAM-5757&lt;/a>).&lt;/li>
&lt;li>Java SDK: Added new IO connector for InfluxDB - InfluxDbIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-2546">BEAM-2546&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Support for repeatable fields in JSON decoder for &lt;code>ReadFromBigQuery&lt;/code> added. (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10524">BEAM-10524&lt;/a>)&lt;/li>
&lt;li>Added an opt-in, performance-driven runtime type checking system for the Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10549">BEAM-10549&lt;/a>).
More details will be in an upcoming &lt;a href="/blog/python-performance-runtime-type-checking/index.html">blog post&lt;/a>.&lt;/li>
&lt;li>Added support for Python 3 type annotations on PTransforms using typed PCollections (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10258">BEAM-10258&lt;/a>).
More details will be in an upcoming &lt;a href="/blog/python-improved-annotations/index.html">blog post&lt;/a>.&lt;/li>
&lt;li>Improved the Interactive Beam API where recording streaming jobs now start a long running background recording job. Running ib.show() or ib.collect() samples from the recording (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10603">BEAM-10603&lt;/a>).&lt;/li>
&lt;li>In Interactive Beam, ib.show() and ib.collect() now have &amp;ldquo;n&amp;rdquo; and &amp;ldquo;duration&amp;rdquo; as parameters. These mean read only up to &amp;ldquo;n&amp;rdquo; elements and up to &amp;ldquo;duration&amp;rdquo; seconds of data read from the recording (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10603">BEAM-10603&lt;/a>).&lt;/li>
&lt;li>Initial preview of &lt;a href="https://s.apache.org/simpler-python-pipelines-2020#slide=id.g905ac9257b_1_21">Dataframes&lt;/a> support.
See also example at apache_beam/examples/wordcount_dataframe.py&lt;/li>
&lt;li>Fixed support for type hints on &lt;code>@ptransform_fn&lt;/code> decorators in the Python SDK.
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-4091">BEAM-4091&lt;/a>)
This has not enabled by default to preserve backwards compatibility; use the
&lt;code>--type_check_additional=ptransform_fn&lt;/code> flag to enable. It may be enabled by
default in future versions of Beam.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Python 2 and Python 3.5 support dropped (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10644">BEAM-10644&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-9372">BEAM-9372&lt;/a>).&lt;/li>
&lt;li>Pandas 1.x allowed. Older version of Pandas may still be used, but may not be as well tested.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Python transform ReadFromSnowflake has been moved from &lt;code>apache_beam.io.external.snowflake&lt;/code> to &lt;code>apache_beam.io.snowflake&lt;/code>. The previous path will be removed in the future versions.&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Dataflow streaming timers once against not strictly time ordered when set earlier mid-bundle, as the fix for &lt;a href="https://issues.apache.org/jira/browse/BEAM-8543">BEAM-8543&lt;/a> introduced more severe bugs and has been rolled back.&lt;/li>
&lt;li>Default compressor change breaks dataflow python streaming job update compatibility. Please use python SDK version &amp;lt;= 2.23.0 or &amp;gt; 2.25.0 if job update is critical.(&lt;a href="https://issues.apache.org/jira/browse/BEAM-11113">BEAM-11113&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.25.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Aldair Coronel Ruiz, Alexey Romanenko, Andrew Pilloud, Ankur Goenka,
Ayoub ENNASSIRI, Bipin Upadhyaya, Boyuan Zhang, Brian Hulette, Brian Michalski, Chad Dombrova,
Chamikara Jayalath, Damon Douglas, Daniel Oliveira, David Cavazos, David Janicek, Doug Roeper, Eric
Roshan-Eisner, Etta Rapp, Eugene Kirpichov, Filipe Regadas, Heejong Lee, Ihor Indyk, Irvi Firqotul
Aini, Ismaël Mejía, Jan Lukavský, Jayendra, Jiadai Xia, Jithin Sukumar, Jozsef Bartok, Kamil
Gałuszka, Kamil Wasilewski, Kasia Kucharczyk, Kenneth Jung, Kenneth Knowles, Kevin Puthusseri, Kevin
Sijo Puthusseri, KevinGG, Kyle Weaver, Leiyi Zhang, Lourens Naudé, Luke Cwik, Matthew Ouyang,
Maximilian Michels, Michal Walenia, Milan Cermak, Monica Song, Nelson Osacky, Neville Li, Ning Kang,
Pablo Estrada, Piotr Szuberski, Qihang, Rehman, Reuven Lax, Robert Bradshaw, Robert Burke, Rui Wang,
Saavan Nanavati, Sam Bourne, Sam Rohde, Sam Whittle, Sergiy Kolesnikov, Sindy Li, Siyuan Chen, Steve
Niemitz, Terry Xian, Thomas Weise, Tobiasz Kędzierski, Truc Le, Tyson Hamilton, Udi Meiri, Valentyn
Tymofieiev, Yichi Zhang, Yifan Mai, Yueyang Qiu, annaqin418, danielxjd, dennis, dp, fuyuwei,
lostluck, nehsyc, odeshpande, odidev, pulasthi, purbanow, rworley-monster, sclukas77, terryxian78,
tvalentyn, yoshiki.obata&lt;/p></description></item><item><title>Blog: Apache Beam 2.24.0</title><link>/blog/beam-2.24.0/</link><pubDate>Fri, 18 Sep 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.24.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.24.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2240-2020-09-18">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.24.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347146">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Apache Beam 2.24.0 is the last release with Python 2 and Python 3.5
support.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>New overloads for BigtableIO.Read.withKeyRange() and BigtableIO.Read.withRowFilter()
methods that take ValueProvider as a parameter (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10283">BEAM-10283&lt;/a>).&lt;/li>
&lt;li>The WriteToBigQuery transform (Python) in Dataflow Batch no longer relies on BigQuerySink by default. It relies on
a new, fully-featured transform based on file loads into BigQuery. To revert the behavior to the old implementation,
you may use &lt;code>--experiments=use_legacy_bq_sink&lt;/code>.&lt;/li>
&lt;li>Add cross-language support to Java&amp;rsquo;s JdbcIO, now available in the Python module &lt;code>apache_beam.io.jdbc&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10135">BEAM-10135&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-10136">BEAM-10136&lt;/a>).&lt;/li>
&lt;li>Add support of AWS SDK v2 for KinesisIO.Read (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9702">BEAM-9702&lt;/a>).&lt;/li>
&lt;li>Add streaming support to SnowflakeIO in Java SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9896">BEAM-9896&lt;/a>)&lt;/li>
&lt;li>Support reading and writing to Google Healthcare DICOM APIs in Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10601">BEAM-10601&lt;/a>)&lt;/li>
&lt;li>Add dispositions for SnowflakeIO.write (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10343">BEAM-10343&lt;/a>)&lt;/li>
&lt;li>Add cross-language support to SnowflakeIO.Read now available in the Python module &lt;code>apache_beam.io.external.snowflake&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9897">BEAM-9897&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Shared library for simplifying management of large shared objects added to Python SDK. Example use case is sharing a large TF model object across threads (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10417">BEAM-10417&lt;/a>).&lt;/li>
&lt;li>Dataflow streaming timers are not strictly time ordered when set earlier mid-bundle (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8543">BEAM-8543&lt;/a>).&lt;/li>
&lt;li>OnTimerContext should not create a new one when processing each element/timer in FnApiDoFnRunner (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9839">BEAM-9839&lt;/a>)&lt;/li>
&lt;li>Key should be available in @OnTimer methods (Spark Runner) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9850">BEAM-9850&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>WriteToBigQuery transforms now require a GCS location to be provided through either
custom_gcs_temp_location in the constructor of WriteToBigQuery or the fallback option
&amp;ndash;temp_location, or pass method=&amp;ldquo;STREAMING_INSERTS&amp;rdquo; to WriteToBigQuery (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6928">BEAM-6928&lt;/a>).&lt;/li>
&lt;li>Python SDK now understands &lt;code>typing.FrozenSet&lt;/code> type hints, which are not interchangeable with &lt;code>typing.Set&lt;/code>. You may need to update your pipelines if type checking fails. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10197">BEAM-10197&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Default compressor change breaks dataflow python streaming job update compatibility. Please use python SDK version &amp;lt;= 2.23.0 or &amp;gt; 2.25.0 if job update is critical.(&lt;a href="https://issues.apache.org/jira/browse/BEAM-11113">BEAM-11113&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.24.0 release. Thank you to all contributors!&lt;/p>
&lt;p>adesormi, Ahmet Altay, Alex Amato, Alexey Romanenko, Andrew Pilloud, Ashwin Ramaswami, Borzoo,
Boyuan Zhang, Brian Hulette, Brian M, Bu Sun Kim, Chamikara Jayalath, Colm O hEigeartaigh,
Corvin Deboeser, Damian Gadomski, Damon Douglas, Daniel Oliveira, Dariusz Aniszewski,
davidak09, David Cavazos, David Moravek, David Yan, dhodun, Doug Roeper, Emil Hessman, Emily Ye,
Etienne Chauchot, Etta Rapp, Eugene Kirpichov, fuyuwei, Gleb Kanterov,
Harrison Green, Heejong Lee, Henry Suryawirawan, InigoSJ, Ismaël Mejía, Israel Herraiz,
Jacob Ferriero, Jan Lukavský, Jayendra, jfarr, jhnmora000, Jiadai Xia, JIahao wu, Jie Fan,
Jiyong Jung, Julius Almeida, Kamil Gałuszka, Kamil Wasilewski, Kasia Kucharczyk, Kenneth Knowles,
Kevin Puthusseri, Kyle Weaver, Łukasz Gajowy, Luke Cwik, Mark-Zeng, Maximilian Michels,
Michal Walenia, Niel Markwick, Ning Kang, Pablo Estrada, pawel.urbanowicz, Piotr Szuberski,
Rafi Kamal, rarokni, Rehman Murad Ali, Reuben van Ammers, Reuven Lax, Ricardo Bordon,
Robert Bradshaw, Robert Burke, Robin Qiu, Rui Wang, Saavan Nanavati, sabhyankar, Sam Rohde,
Scott Lukas, Siddhartha Thota, Simone Primarosa, Sławomir Andrian,
Steve Niemitz, Tobiasz Kędzierski, Tomo Suzuki, Tyson Hamilton, Udi Meiri,
Valentyn Tymofieiev, viktorjonsson, Xinyu Liu, Yichi Zhang, Yixing Zhang, yoshiki.obata,
Yueyang Qiu, zijiesong&lt;/p></description></item><item><title>Blog: Pattern Matching with Beam SQL</title><link>/blog/pattern-match-beam-sql/</link><pubDate>Thu, 27 Aug 2020 00:00:01 +0800</pubDate><guid>/blog/pattern-match-beam-sql/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>SQL is becoming increasingly powerful and useful in the field of data analysis. MATCH_RECOGNIZE,
a new SQL component introduced in 2016, brings extra analytical functionality. This project,
as part of Google Summer of Code, aims to support basic MATCH_RECOGNIZE functionality. A basic MATCH_RECOGNIZE
query would be something like this:
&lt;div class='language-sql snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">aid&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">bid&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">cid&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">MyTable&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">MATCH_RECOGNIZE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">PARTITION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">userid&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">ORDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">proctime&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">MEASURES&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">aid&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">bid&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">C&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">cid&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">PATTERN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">C&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">DEFINE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">C&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;c&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">T&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;/p>
&lt;p>The above query finds out ordered sets of events that have names &amp;lsquo;a&amp;rsquo;, &amp;lsquo;b&amp;rsquo; and &amp;lsquo;c&amp;rsquo;. Apart from this basic usage of
MATCH_RECOGNIZE, I supported a few of other crucial features such as quantifiers and row pattern navigation. I will spell out
the details in later sections.&lt;/p>
&lt;h2 id="approach--discussion">Approach &amp;amp; Discussion&lt;/h2>
&lt;p>The implementation is strongly based on BEAM core transforms. Specifically, one MATCH_RECOGNIZE execution composes the
following series of transforms:&lt;/p>
&lt;ol>
&lt;li>A &lt;code>ParDo&lt;/code> transform and then a &lt;code>GroupByKey&lt;/code> transform that build up the partitions (PARTITION BY).&lt;/li>
&lt;li>A &lt;code>ParDo&lt;/code> transform that sorts within each partition (ORDER BY).&lt;/li>
&lt;li>A &lt;code>ParDo&lt;/code> transform that applies pattern-match in each sorted partition.&lt;/li>
&lt;/ol>
&lt;p>A pattern-match operation was first done with the java regex library. That is, I first transform rows within a partition into
a string and then apply regex pattern-match routines. If a row satisfies a condition, then I output the corresponding pattern variable.
This is ok under the assumption that the pattern definitions are mutually exclusive. That is, a pattern definition like &lt;code>A AS A.price &amp;gt; 0, B AS b.price &amp;lt; 0&lt;/code> is allowed while
a pattern definition like &lt;code>A AS A.price &amp;gt; 0, B AS B.proctime &amp;gt; 0&lt;/code> might results in an incomplete match. For the latter case,
an event can satisfy the conditions A and B at the same time. Mutually exclusive conditions gives deterministic pattern-match:
each event can only belong to at most one pattern class.&lt;/p>
&lt;p>As specified in the SQL 2016 document, MATCH_RECOGNIZE defines a richer set of expression than regular expression. Specifically,
it introduces &lt;em>Row Pattern Navigation Operations&lt;/em> such as &lt;code>PREV&lt;/code> and &lt;code>NEXT&lt;/code>. This is perhaps one of the most intriguing feature of
MATCH_RECOGNIZE. A regex library would no longer suffice the need since the pattern definition could be back-referencing (&lt;code>PREV&lt;/code>) or
forward-referencing (&lt;code>NEXT&lt;/code>). So for the second version of implementation, we chose to use an NFA regex engine. An NFA brings more flexibility
in terms of non-determinism (see Chapter 6 of SQL 2016 Part 5 for a more thorough discussion). My proposed NFA is based on a paper of UMASS.&lt;/p>
&lt;p>This is a working project. Many of the components are still not supported. I will list some unimplemented work in the section
of future work.&lt;/p>
&lt;h2 id="usages">Usages&lt;/h2>
&lt;p>For now, the components I supported are:&lt;/p>
&lt;ul>
&lt;li>PARTITION BY&lt;/li>
&lt;li>ORDER BY&lt;/li>
&lt;li>MEASURES
&lt;ol>
&lt;li>LAST&lt;/li>
&lt;li>FIRST&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>ONE ROW PER MATCH/ALL ROWS PER MATCH&lt;/li>
&lt;li>DEFINE
&lt;ol>
&lt;li>Left side of the condition
&lt;ol>
&lt;li>LAST&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Right side of the condition
&lt;ol>
&lt;li>PREV&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Quantifier
&lt;ol>
&lt;li>Kleene plus&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;p>The pattern definition evaluation is hard coded. To be more specific, it expects the column reference of the incoming row
to be on the left side of a comparator. Additionally, PREV function can only appear on the right side of the comparator.&lt;/p>
&lt;p>With these limited tools, we could already write some slightly more complicated queries. Imagine we have the following
table:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">transTime&lt;/th>
&lt;th style="text-align:center">price&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">4&lt;/td>
&lt;td style="text-align:center">5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">5&lt;/td>
&lt;td style="text-align:center">6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>This table reflects the price changes of a product with respect to the transaction time. We could write the following
query:&lt;/p>
&lt;div class='language-sql snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">MyTable&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">MATCH_RECOGNIZE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">ORDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">transTime&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">MEASURES&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">LAST&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">beforePrice&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">FIRST&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">afterPrice&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">PATTERN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">DEFINE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PREV&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PREV&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">T&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>This will find the local minimum price and the price after it. For the example dataset, the first 3 rows will be
mapped to A and the rest of the rows will be mapped to B. Thus, we will have (1, 5) as the result.&lt;/p>
&lt;blockquote>
&lt;p>Very important: For my NFA implementation, it slightly breaks the rule in the SQL standard. Since the buffered NFA
only stores an event to the buffer if the event is a match to some pattern class, There would be no way to get the
previous event back if the previous row is discarded. So the first row would always be a match (different from the standard)
if PREV is used.&lt;/p>
&lt;/blockquote>
&lt;h2 id="progress">Progress&lt;/h2>
&lt;ol>
&lt;li>PRs
&lt;ol>
&lt;li>&lt;a href="https://github.com/apache/beam/pull/12232">Support MATCH_RECOGNIZE using regex library&lt;/a> (merged)&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/pull/12532">Support MATCH_RECOGNIZE using NFA&lt;/a> (pending)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Commits
&lt;ol>
&lt;li>partition by: &lt;a href="https://github.com/apache/beam/pull/12232/commits/064ada7257970bcb1d35530be1b88cb3830f242b">commit 064ada7&lt;/a>&lt;/li>
&lt;li>order by: &lt;a href="https://github.com/apache/beam/pull/12232/commits/9cd1a82bec7b2f7c44aacfbd72f5f775bb58b650">commit 9cd1a82&lt;/a>&lt;/li>
&lt;li>regex pattern match: &lt;a href="https://github.com/apache/beam/pull/12232/commits/8d6ffcc213e30999fc495c119b68da4f62fad258">commit 8d6ffcc&lt;/a>&lt;/li>
&lt;li>support quantifiers: &lt;a href="https://github.com/apache/beam/pull/12232/commits/f529b876a2c2e43d012c71b3a83ebd55eb16f4ff">commit f529b87&lt;/a>&lt;/li>
&lt;li>measures: &lt;a href="https://github.com/apache/beam/pull/12232/commits/87935746647611aa139d664ebed10c8e638bb024">commit 8793574&lt;/a>&lt;/li>
&lt;li>added NFA implementation: &lt;a href="https://github.com/apache/beam/pull/12532/commits/fc731f2b0699d11853e7b76da86456427d434a2a">commit fc731f2&lt;/a>&lt;/li>
&lt;li>implemented functions PREV and LAST: &lt;a href="https://github.com/apache/beam/pull/12532/commits/fc731f2b0699d11853e7b76da86456427d434a2a">commit 35323da&lt;/a>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h2 id="future-work">Future Work&lt;/h2>
&lt;ul>
&lt;li>Support FINAL/RUNNING keywords.&lt;/li>
&lt;li>Support more quantifiers.&lt;/li>
&lt;li>Add optimization to the NFA.&lt;/li>
&lt;li>A better way to realize MATCH_RECOGNIZE might be having a Complex Event Processing library at BEAM core (rather than using BEAM transforms).&lt;/li>
&lt;/ul>
&lt;!-- Related Documents:
- proposal
- design doc
- SQL 2016 standard
- UMASS NFA^b paper
-->
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://drive.google.com/file/d/1ZuFZV4dCFVPZW_-RiqbU0w-vShaZh_jX/view?usp=sharing">Project Proposal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://s.apache.org/beam-sql-pattern-recognization">Design Documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.iso.org/standard/65143.html">SQL 2016 documentation Part 5&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://dl.acm.org/doi/10.1145/1376616.1376634">UMASS paper on NFA with shared buffer&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Improved Annotation Support for the Python SDK</title><link>/blog/python-improved-annotations/</link><pubDate>Fri, 21 Aug 2020 00:00:01 -0800</pubDate><guid>/blog/python-improved-annotations/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The importance of static type checking in a dynamically
typed language like Python is not up for debate. Type hints
allow developers to leverage a strong typing system to:&lt;/p>
&lt;ul>
&lt;li>write better code,&lt;/li>
&lt;li>self-document ambiguous programming logic, and&lt;/li>
&lt;li>inform intelligent code completion in IDEs like PyCharm.&lt;/li>
&lt;/ul>
&lt;p>This is why we&amp;rsquo;re excited to announce upcoming improvements to
the &lt;code>typehints&lt;/code> module of Beam&amp;rsquo;s Python SDK, including support
for typed PCollections and Python 3 style annotations on PTransforms.&lt;/p>
&lt;h1 id="improved-annotations">Improved Annotations&lt;/h1>
&lt;p>Today, you have the option to declare type hints on PTransforms using either
class decorators or inline functions.&lt;/p>
&lt;p>For instance, a PTransform with decorated type hints might look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>@beam.typehints.with_input_types(int)
@beam.typehints.with_output_types(str)
class IntToStr(beam.PTransform):
def expand(self, pcoll):
return pcoll | beam.Map(lambda num: str(num))
strings = numbers | beam.ParDo(IntToStr())
&lt;/code>&lt;/pre>&lt;p>Using inline functions instead, the same transform would look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>class IntToStr(beam.PTransform):
def expand(self, pcoll):
return pcoll | beam.Map(lambda num: str(num))
strings = numbers | beam.ParDo(IntToStr()).with_input_types(int).with_output_types(str)
&lt;/code>&lt;/pre>&lt;p>Both methods have problems. Class decorators are syntax-heavy,
requiring two additional lines of code, whereas inline functions provide type hints
that aren&amp;rsquo;t reusable across other instances of the same transform. Additionally, both
methods are incompatible with static type checkers like MyPy.&lt;/p>
&lt;p>With Python 3 annotations however, we can subvert these problems to provide a
clean and reusable type hint experience. Our previous transform now looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>class IntToStr(beam.PTransform):
def expand(self, pcoll: PCollection[int]) -&amp;gt; PCollection[str]:
return pcoll | beam.Map(lambda num: str(num))
strings = numbers | beam.ParDo(IntToStr())
&lt;/code>&lt;/pre>&lt;p>These type hints will actively hook into the internal Beam typing system to
play a role in pipeline type checking, and runtime type checking.&lt;/p>
&lt;p>So how does this work?&lt;/p>
&lt;h2 id="typed-pcollections">Typed PCollections&lt;/h2>
&lt;p>You guessed it! The PCollection class inherits from &lt;code>typing.Generic&lt;/code>, allowing it to be
parameterized with either zero types (denoted &lt;code>PCollection&lt;/code>) or one type (denoted &lt;code>PCollection[T]&lt;/code>).&lt;/p>
&lt;ul>
&lt;li>A PCollection with zero types is implicitly converted to &lt;code>PCollection[Any]&lt;/code>.&lt;/li>
&lt;li>A PCollection with one type can have any nested type (e.g. &lt;code>Union[int, str]&lt;/code>).&lt;/li>
&lt;/ul>
&lt;p>Internally, Beam&amp;rsquo;s typing system makes these annotations compatible with other
type hints by removing the outer PCollection container.&lt;/p>
&lt;h2 id="pbegin-pdone-none">PBegin, PDone, None&lt;/h2>
&lt;p>Finally, besides PCollection, a valid annotation on the &lt;code>expand(...)&lt;/code> method of a PTransform is
&lt;code>PBegin&lt;/code> or &lt;code>None&lt;/code>. These are generally used for PTransforms that begin or end with an I/O operation.&lt;/p>
&lt;p>For instance, when saving data, your transform&amp;rsquo;s output type should be &lt;code>None&lt;/code>.&lt;/p>
&lt;pre tabindex="0">&lt;code>class SaveResults(beam.PTransform):
def expand(self, pcoll: PCollection[str]) -&amp;gt; None:
return pcoll | beam.io.WriteToBigQuery(...)
&lt;/code>&lt;/pre>&lt;h1 id="next-steps">Next Steps&lt;/h1>
&lt;p>What are you waiting for.. start using annotations on your transforms!&lt;/p>
&lt;p>For more background on type hints in Python, see:
&lt;a href="/documentation/sdks/python-type-safety/">Ensuring Python Type Safety&lt;/a>.&lt;/p>
&lt;p>Finally, please
&lt;a href="/community/contact-us/">let us know&lt;/a>
if you encounter any issues.&lt;/p></description></item><item><title>Blog: Performance-Driven Runtime Type Checking for the Python SDK</title><link>/blog/python-performance-runtime-type-checking/</link><pubDate>Fri, 21 Aug 2020 00:00:01 -0800</pubDate><guid>/blog/python-performance-runtime-type-checking/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>In this blog post, we&amp;rsquo;re announcing the upcoming release of a new, opt-in
runtime type checking system for Beam&amp;rsquo;s Python SDK that&amp;rsquo;s optimized for performance
in both development and production environments.&lt;/p>
&lt;p>But let&amp;rsquo;s take a step back - why do we even care about runtime type checking
in the first place? Let&amp;rsquo;s look at an example.&lt;/p>
&lt;pre tabindex="0">&lt;code>class MultiplyNumberByTwo(beam.DoFn):
def process(self, element: int):
return element * 2
p = Pipeline()
p | beam.Create([&amp;#39;1&amp;#39;, &amp;#39;2&amp;#39;] | beam.ParDo(MultiplyNumberByTwo())
&lt;/code>&lt;/pre>&lt;p>In this code, we passed a list of strings to a DoFn that&amp;rsquo;s clearly intended for use with
integers. Luckily, this code will throw an error during pipeline construction because
the inferred output type of &lt;code>beam.Create(['1', '2'])&lt;/code> is &lt;code>str&lt;/code> which is incompatible with
the declared input type of &lt;code>MultiplyNumberByTwo.process&lt;/code> which is &lt;code>int&lt;/code>.&lt;/p>
&lt;p>However, what if we turned pipeline type checking off using the &lt;code>no_pipeline_type_check&lt;/code>
flag? Or more realistically, what if the input PCollection to &lt;code>MultiplyNumberByTwo&lt;/code> arrived
from a database, meaning that the output data type can only be known at runtime?&lt;/p>
&lt;p>In either case, no error would be thrown during pipeline construction.
And even at runtime, this code works. Each string would be multiplied by 2,
yielding a result of &lt;code>['11', '22']&lt;/code>, but that&amp;rsquo;s certainly not the outcome we want.&lt;/p>
&lt;p>So how do you debug this breed of &amp;ldquo;hidden&amp;rdquo; errors? More broadly speaking, how do you debug
any typing or serialization error in Beam?&lt;/p>
&lt;p>The answer is to use runtime type checking.&lt;/p>
&lt;h1 id="runtime-type-checking-rtc">Runtime Type Checking (RTC)&lt;/h1>
&lt;p>This feature works by checking that actual input and output values satisfy the declared
type constraints during pipeline execution. If you ran the code from before with
&lt;code>runtime_type_check&lt;/code> on, you would receive the following error message:&lt;/p>
&lt;pre tabindex="0">&lt;code>Type hint violation for &amp;#39;ParDo(MultiplyByTwo)&amp;#39;: requires &amp;lt;class &amp;#39;int&amp;#39;&amp;gt; but got &amp;lt;class &amp;#39;str&amp;#39;&amp;gt; for element
&lt;/code>&lt;/pre>&lt;p>This is an actionable error message - it tells you that either your code has a bug
or that your declared type hints are incorrect. Sounds simple enough, so what&amp;rsquo;s the catch?&lt;/p>
&lt;p>&lt;em>It is soooo slowwwwww.&lt;/em> See for yourself.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Element Size&lt;/th>
&lt;th>Normal Pipeline&lt;/th>
&lt;th>Runtime Type Checking Pipeline&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>5.3 sec&lt;/td>
&lt;td>5.6 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2,001&lt;/td>
&lt;td>9.4 sec&lt;/td>
&lt;td>57.2 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10,001&lt;/td>
&lt;td>24.5 sec&lt;/td>
&lt;td>259.8 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18,001&lt;/td>
&lt;td>38.7 sec&lt;/td>
&lt;td>450.5 sec&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>In this micro-benchmark, the pipeline with runtime type checking was over 10x slower,
with the gap only increasing as our input PCollection increased in size.&lt;/p>
&lt;p>So, is there any production-friendly alternative?&lt;/p>
&lt;h1 id="performance-runtime-type-check">Performance Runtime Type Check&lt;/h1>
&lt;p>There is! We developed a new flag called &lt;code>performance_runtime_type_check&lt;/code> that
minimizes its footprint on the pipeline&amp;rsquo;s time complexity using a combination of&lt;/p>
&lt;ul>
&lt;li>efficient Cython code,&lt;/li>
&lt;li>smart sampling techniques, and&lt;/li>
&lt;li>optimized mega type-hints.&lt;/li>
&lt;/ul>
&lt;p>So what do the new numbers look like?&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Element Size&lt;/th>
&lt;th>Normal&lt;/th>
&lt;th>RTC&lt;/th>
&lt;th>Performance RTC&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>5.3 sec&lt;/td>
&lt;td>5.6 sec&lt;/td>
&lt;td>5.4 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2,001&lt;/td>
&lt;td>9.4 sec&lt;/td>
&lt;td>57.2 sec&lt;/td>
&lt;td>11.2 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10,001&lt;/td>
&lt;td>24.5 sec&lt;/td>
&lt;td>259.8 sec&lt;/td>
&lt;td>25.5 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18,001&lt;/td>
&lt;td>38.7 sec&lt;/td>
&lt;td>450.5 sec&lt;/td>
&lt;td>39.4 sec&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>On average, the new Performance RTC is 4.4% slower than a normal pipeline whereas the old RTC
is over 900% slower! Additionally, as the size of the input PCollection increases, the fixed cost
of setting up the Performance RTC system is spread across each element, decreasing the relative
impact on the overall pipeline. With 18,001 elements, the difference is less than 1 second.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>There are three key factors responsible for this upgrade in performance.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Instead of type checking all values, we only type check a subset of values, known as
a sample in statistics. Initially, we sample a substantial number of elements, but as our
confidence that the element type won&amp;rsquo;t change over time increases, we reduce our
sampling rate (up to a fixed minimum).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Whereas the old RTC system used heavy wrappers to perform the type check, the new RTC system
moves the type check to a Cython-optimized, non-decorated portion of the codebase. For reference,
Cython is a programming language that gives C-like performance to Python code.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Finally, we use a single mega type hint to type-check only the output values of transforms
instead of type-checking both the input and output values separately. This mega typehint is composed of
the original transform&amp;rsquo;s output type constraints along with all consumer transforms&amp;rsquo; input type
constraints. Using this mega type hint allows us to reduce overhead while simultaneously allowing
us to throw &lt;em>more actionable errors&lt;/em>. For instance, consider the following error (which was
generated from the old RTC system):&lt;/p>
&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>Runtime type violation detected within ParDo(DownstreamDoFn): Type-hint for argument: &amp;#39;element&amp;#39; violated. Expected an instance of &amp;lt;class ‘str’&amp;gt;, instead found 9, an instance of &amp;lt;class ‘int’&amp;gt;.
&lt;/code>&lt;/pre>&lt;p>This error tells us that the &lt;code>DownstreamDoFn&lt;/code> received an &lt;code>int&lt;/code> when it was expecting a &lt;code>str&lt;/code>, but doesn&amp;rsquo;t tell us
who created that &lt;code>int&lt;/code> in the first place. Who is the offending upstream transform that&amp;rsquo;s responsible for
this &lt;code>int&lt;/code>? Presumably, &lt;em>that&lt;/em> transform&amp;rsquo;s output type hints were too expansive (e.g. &lt;code>Any&lt;/code>) or otherwise non-existent because
no error was thrown during the runtime type check of its output.&lt;/p>
&lt;p>The problem here boils down to a lack of context. If we knew who our consumers were when type
checking our output, we could simultaneously type check our output value against our output type
constraints and every consumers&amp;rsquo; input type constraints to know whether there is &lt;em>any&lt;/em> possibility
for a mismatch. This is exactly what the mega type hint does, and it allows us to throw errors
at the point of declaration rather than the point of exception, saving you valuable time
while providing higher quality error messages.&lt;/p>
&lt;p>So what would the same error look like using Performance RTC? It&amp;rsquo;s the exact same string but with one additional line:&lt;/p>
&lt;pre tabindex="0">&lt;code>[while running &amp;#39;ParDo(UpstreamDoFn)&amp;#39;]
&lt;/code>&lt;/pre>&lt;p>And that&amp;rsquo;s much more actionable for an investigation :)&lt;/p>
&lt;h1 id="next-steps">Next Steps&lt;/h1>
&lt;p>Go play with the new &lt;code>performance_runtime_type_check&lt;/code> feature!&lt;/p>
&lt;p>It&amp;rsquo;s in an experimental state so please
&lt;a href="/community/contact-us/">let us know&lt;/a>
if you encounter any issues.&lt;/p></description></item><item><title>Blog: Apache Beam 2.23.0</title><link>/blog/beam-2.23.0/</link><pubDate>Wed, 29 Jul 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.23.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.23.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2230-2020-07-29">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.23.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347145">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Twister2 Runner (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7304">BEAM-7304&lt;/a>).&lt;/li>
&lt;li>Python 3.8 support (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8494">BEAM-8494&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for reading from Snowflake added (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9722">BEAM-9722&lt;/a>).&lt;/li>
&lt;li>Support for writing to Splunk added (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8596">BEAM-8596&lt;/a>).&lt;/li>
&lt;li>Support for assume role added (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10335">BEAM-10335&lt;/a>).&lt;/li>
&lt;li>A new transform to read from BigQuery has been added: &lt;code>apache_beam.io.gcp.bigquery.ReadFromBigQuery&lt;/code>. This transform
is experimental. It reads data from BigQuery by exporting data to Avro files, and reading those files. It also supports
reading data by exporting to JSON files. This has small differences in behavior for Time and Date-related fields. See
Pydoc for more information.&lt;/li>
&lt;li>Add dispositions for SnowflakeIO.write (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10343">BEAM-10343&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Update Snowflake JDBC dependency and add application=beam to connection URL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10383">BEAM-10383&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>&lt;code>RowJson.RowJsonDeserializer&lt;/code>, &lt;code>JsonToRow&lt;/code>, and &lt;code>PubsubJsonTableProvider&lt;/code> now accept &amp;ldquo;implicit
nulls&amp;rdquo; by default when deserializing JSON (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10220">BEAM-10220&lt;/a>).
Previously nulls could only be represented with explicit null values, as in
&lt;code>{&amp;quot;foo&amp;quot;: &amp;quot;bar&amp;quot;, &amp;quot;baz&amp;quot;: null}&lt;/code>, whereas an implicit null like &lt;code>{&amp;quot;foo&amp;quot;: &amp;quot;bar&amp;quot;}&lt;/code> would raise an
exception. Now both JSON strings will yield the same result by default. This behavior can be
overridden with &lt;code>RowJson.RowJsonDeserializer#withNullBehavior&lt;/code>.&lt;/li>
&lt;li>Fixed a bug in &lt;code>GroupIntoBatches&lt;/code> experimental transform in Python to actually group batches by key.
This changes the output type for this transform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6696">BEAM-6696&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Remove Gearpump runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9999">BEAM-9999&lt;/a>)&lt;/li>
&lt;li>Remove Apex runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9999">BEAM-9999&lt;/a>)&lt;/li>
&lt;li>RedisIO.readAll() is deprecated and will be removed in 2 versions, users must use RedisIO.readKeyPatterns() as a replacement (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9747">BEAM-9747&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.23.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aaron, Abhishek Yadav, Ahmet Altay, aiyangar, Aizhamal Nurmamat kyzy, Ajo Thomas, Akshay-Iyangar, Alan Pryor, Alex Amato, Alexey Romanenko, Allen Pradeep Xavier, Andrew Crites, Andrew Pilloud, Ankur Goenka, Anna Qin, Ashwin Ramaswami, bntnam, Borzoo Esmailloo, Boyuan Zhang, Brian Hulette, Brian Michalski, brucearctor, Chamikara Jayalath, chi-chi weng, Chuck Yang, Chun Yang, Colm O hEigeartaigh, Corvin Deboeser, Craig Chambers, Damian Gadomski, Damon Douglas, Daniel Oliveira, Dariusz Aniszewski, darshanj, darshan jani, David Cavazos, David Moravek, David Yan, Esun Kim, Etienne Chauchot, Filipe Regadas, fuyuwei, Graeme Morgan, Hannah-Jiang, Harch Vardhan, Heejong Lee, Henry Suryawirawan, InigoSJ, Ismaël Mejía, Israel Herraiz, Jacob Ferriero, Jan Lukavský, Jie Fan, John Mora, Jozef Vilcek, Julien Phalip, Justine Koa, Kamil Gabryjelski, Kamil Wasilewski, Kasia Kucharczyk, Kenneth Jung, Kenneth Knowles, kevingg, Kevin Sijo Puthusseri, kshivvy, Kyle Weaver, Kyoungha Min, Kyungwon Jo, Luke Cwik, Mark Liu, Mark-Zeng, Matthias Baetens, Maximilian Michels, Michal Walenia, Mikhail Gryzykhin, Nam Bui, Nathan Fisher, Niel Markwick, Ning Kang, Omar Ismail, Pablo Estrada, paul fisher, Pawel Pasterz, perkss, Piotr Szuberski, pulasthi, purbanow, Rahul Patwari, Rajat Mittal, Rehman, Rehman Murad Ali, Reuben van Ammers, Reuven Lax, Reza Rokni, Rion Williams, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, sabhyankar, Sam Rohde, Sam Whittle, sclukas77, Sebastian Graca, Shoaib Zafar, Sruthi Sree Kumar, Stephen O&amp;rsquo;Kennedy, Steve Koonce, Steve Niemitz, Steven van Rossum, Ted Romer, Tesio, Thinh Ha, Thomas Weise, Tobias Kaymak, tobiaslieber-cognitedata, Tobiasz Kędzierski, Tomo Suzuki, Tudor Marian, tvs, Tyson Hamilton, Udi Meiri, Valentyn Tymofieiev, Vasu Nori, xuelianhan, Yichi Zhang, Yifan Zou, Yixing Zhang, yoshiki.obata, Yueyang Qiu, Yu Feng, Yuwei Fu, Zhuo Peng, ZijieSong946.&lt;/p></description></item><item><title>Blog: Apache Beam 2.22.0</title><link>/blog/beam-2.22.0/</link><pubDate>Mon, 08 Jun 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.22.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.22.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2220-2020-06-08">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.22.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347144">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Basic Kafka read/write support for DataflowRunner (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8019">BEAM-8019&lt;/a>).&lt;/li>
&lt;li>Sources and sinks for Google Healthcare APIs (Java)(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9468">BEAM-9468&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>&lt;code>--workerCacheMB&lt;/code> flag is supported in Dataflow streaming pipeline (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9964">BEAM-9964&lt;/a>)&lt;/li>
&lt;li>&lt;code>--direct_num_workers=0&lt;/code> is supported for FnApi runner. It will set the number of threads/subprocesses to number of cores of the machine executing the pipeline (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9443">BEAM-9443&lt;/a>).&lt;/li>
&lt;li>Python SDK now has experimental support for SqlTransform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8603">BEAM-8603&lt;/a>).&lt;/li>
&lt;li>Add OnWindowExpiration method to Stateful DoFn (&lt;a href="https://issues.apache.org/jira/browse/BEAM-1589">BEAM-1589&lt;/a>).&lt;/li>
&lt;li>Added PTransforms for Google Cloud DLP (Data Loss Prevention) services integration (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9723">BEAM-9723&lt;/a>):
&lt;ul>
&lt;li>Inspection of data,&lt;/li>
&lt;li>Deidentification of data,&lt;/li>
&lt;li>Reidentification of data.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Add a more complete I/O support matrix in the documentation site (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9916">BEAM-9916&lt;/a>).&lt;/li>
&lt;li>Upgrade Sphinx to 3.0.3 for building PyDoc.&lt;/li>
&lt;li>Added a PTransform for image annotation using Google Cloud AI image processing service
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9646">BEAM-9646&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Python SDK now requires &lt;code>--job_endpoint&lt;/code> to be set when using &lt;code>--runner=PortableRunner&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9860">BEAM-9860&lt;/a>). Users seeking the old default behavior should set &lt;code>--runner=FlinkRunner&lt;/code> instead.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.22.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, aiyangar, Ajo Thomas, Akshay-Iyangar, Alan Pryor, Alexey Romanenko, Allen Pradeep Xavier, amaliujia, Andrew Pilloud, Ankur Goenka, Ashwin Ramaswami, bntnam, Borzoo Esmailloo, Boyuan Zhang, Brian Hulette, Chamikara Jayalath, Colm O hEigeartaigh, Craig Chambers, Damon Douglas, Daniel Oliveira, David Cavazos, David Moravek, Esun Kim, Etienne Chauchot, Filipe Regadas, Graeme Morgan, Hannah Jiang, Hannah-Jiang, Harch Vardhan, Heejong Lee, Henry Suryawirawan, Ismaël Mejía, Israel Herraiz, Jacob Ferriero, Jan Lukavský, John Mora, Kamil Wasilewski, Kenneth Jung, Kenneth Knowles, kevingg, Kyle Weaver, Kyoungha Min, Kyungwon Jo, Luke Cwik, Mark Liu, Matthias Baetens, Maximilian Michels, Michal Walenia, Mikhail Gryzykhin, Nam Bui, Niel Markwick, Ning Kang, Omar Ismail, omarismail94, Pablo Estrada, paul fisher, pawelpasterz, Pawel Pasterz, Piotr Szuberski, Rahul Patwari, rarokni, Rehman, Rehman Murad Ali, Reuven Lax, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Sam Rohde, Sam Whittle, Sebastian Graca, Shoaib Zafar, Sruthi Sree Kumar, Stephen O&amp;rsquo;Kennedy, Steve Koonce, Steve Niemitz, Steven van Rossum, Tesio, Thomas Weise, tobiaslieber-cognitedata, Tomo Suzuki, Tudor Marian, tvalentyn, Tyson Hamilton, Udi Meiri, Valentyn Tymofieiev, Vasu Nori, xuelianhan, Yichi Zhang, Yifan Zou, yoshiki.obata, Yueyang Qiu, Zhuo Peng&lt;/p></description></item><item><title>Blog: Announcing Beam Katas for Kotlin</title><link>/blog/beam-katas-kotlin-release/</link><pubDate>Mon, 01 Jun 2020 00:00:01 -0800</pubDate><guid>/blog/beam-katas-kotlin-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Today, we are happy to announce a new addition to the Beam Katas family: Kotlin!&lt;/p>
&lt;img src="/images/blog/beam-katas-kotlin-release/beam-and-kotlin.png" alt="Apache Beam and Kotlin Shaking Hands" height="330" width="800" >
&lt;p>You may remember &lt;a href="/blog/beam-kata-release">a post from last year&lt;/a> that informed everyone of the wonderful Beam Katas available on &lt;a href="https://stepik.org">Stepik&lt;/a>
for learning more about writing Apache Beam applications, working with its various APIs and programming model
hands-on, all from the comfort of your favorite IDEs. As of today, you can now work through all of the progressive
exercises to learn about the fundamentals of Beam in Kotlin.&lt;/p>
&lt;p>&lt;a href="https://kotlinlang.org">Kotlin&lt;/a> is a modern, open-source, statically typed language that targets the JVM. It is most commonly used by
Android developers, however it has recently risen in popularity due to its extensive feature set that enables
more concise and cleaner code than Java, without sacrificing performance or type safety. It recently was &lt;a href="https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-languages-loved">ranked
as one of the most beloved programming languages in the annual Stack Overflow Developer Survey&lt;/a>, so don&amp;rsquo;t take
just our word for it.&lt;/p>
&lt;p>The relationship between Apache Beam and Kotlin isn&amp;rsquo;t a new one. You can find examples scattered across the web
of engineering teams embracing the two technologies including &lt;a href="/blog/beam-kotlin/">a series of samples announced on this very blog&lt;/a>.
If you are new to Beam or are an experienced veteran looking for a change of pace, we&amp;rsquo;d encourage you to give
Kotlin a try.&lt;/p>
&lt;p>You can find the Kotlin and the other excellent Beam Katas below (or by just searching for &amp;ldquo;Beam Katas&amp;rdquo; within
&lt;a href="https://www.jetbrains.com/education/download/#section=idea">IntelliJ&lt;/a> or &lt;a href="https://www.jetbrains.com/education/download/#section=pycharm-edu">PyCharm&lt;/a> through &lt;a href="https://plugins.jetbrains.com/plugin/10081-edutools">the EduTools plugin&lt;/a>):&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://stepik.org/course/72488">&lt;strong>Kotlin&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stepik.org/course/54530">&lt;strong>Java&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stepik.org/course/54532">&lt;strong>Python&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stepik.org/course/70387">&lt;strong>Go (in development)&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>I&amp;rsquo;d like to extend a very special thanks to &lt;a href="https://twitter.com/henry_ken">Henry Suryawirawan&lt;/a> for his creation of the original series of Katas
and his support during the review process and making this effort a reality.&lt;/p>
&lt;br />
&lt;img src="/images/blog/beam-katas-kotlin-release/beam-katas-in-edutools.png" alt="Access Beam Katas Kotlin through a JetBrains Educational Product" height="252" width="800" ></description></item><item><title>Blog: Python SDK Typing Changes</title><link>/blog/python-typing/</link><pubDate>Thu, 28 May 2020 00:00:01 -0800</pubDate><guid>/blog/python-typing/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Beam Python has recently increased its support and integration of Python 3 type
annotations for improved code clarity and type correctness checks.
Read on to find out what&amp;rsquo;s new.&lt;/p>
&lt;p>Python supports type annotations on functions (PEP 484). Static type checkers,
such as mypy, are used to verify adherence to these types.
For example:&lt;/p>
&lt;pre tabindex="0">&lt;code>def f(v: int) -&amp;gt; int:
return v[0]
&lt;/code>&lt;/pre>&lt;p>Running mypy on the above code will give the error:
&lt;code>Value of type &amp;quot;int&amp;quot; is not indexable&lt;/code>.&lt;/p>
&lt;p>We&amp;rsquo;ve recently made changes to Beam in 2 areas:&lt;/p>
&lt;p>Adding type annotations throughout Beam. Type annotations make a large and
sophisticated codebase like Beam easier to comprehend and navigate in your
favorite IDE.&lt;/p>
&lt;p>Second, we&amp;rsquo;ve added support for Python 3 type annotations. This allows SDK
users to specify a DoFn&amp;rsquo;s type hints in one place.
We&amp;rsquo;ve also expanded Beam&amp;rsquo;s support of &lt;code>typing&lt;/code> module types.&lt;/p>
&lt;p>For more background see:
&lt;a href="/documentation/sdks/python-type-safety/">Ensuring Python Type Safety&lt;/a>.&lt;/p>
&lt;h1 id="beam-is-typed">Beam Is Typed&lt;/h1>
&lt;p>In tandem with the new type annotation support within DoFns, we&amp;rsquo;ve invested a
great deal of time adding type annotations to the Beam python code itself.
With this in place, we have begun using mypy, a static type
checker, as part of Beam&amp;rsquo;s code review process, which ensures higher quality
contributions and fewer bugs.
The added context and insight that type annotations add throughout Beam is
useful for all Beam developers, contributors and end users alike, but
it is especially beneficial for developers who are new to the project.
If you use an IDE that understands type annotations, it will provide richer
type completions and warnings than before.
You&amp;rsquo;ll also be able to use your IDE to inspect the types of Beam functions and
transforms to better understand how they work, which will ease your own
development.
Finally, once Beam is fully annotated, end users will be able to benefit from
the use of static type analysis on their own pipelines and custom transforms.&lt;/p>
&lt;h1 id="new-ways-to-annotate">New Ways to Annotate&lt;/h1>
&lt;h2 id="python-3-syntax-annotations">Python 3 Syntax Annotations&lt;/h2>
&lt;p>Coming in Beam 2.21 (BEAM-8280), you will be able to use Python annotation
syntax to specify input and output types.&lt;/p>
&lt;p>For example, this new form:&lt;/p>
&lt;pre tabindex="0">&lt;code>class MyDoFn(beam.DoFn):
def process(self, element: int) -&amp;gt; typing.Text:
yield str(element)
&lt;/code>&lt;/pre>&lt;p>is equivalent to this:&lt;/p>
&lt;pre tabindex="0">&lt;code>@apache_beam.typehints.with_input_types(int)
@apache_beam.typehints.with_output_types(typing.Text)
class MyDoFn(beam.DoFn):
def process(self, element):
yield str(element)
&lt;/code>&lt;/pre>&lt;p>One of the advantages of the new form is that you may already be using it
in tandem with a static type checker such as mypy, thus getting additional
runtime type checking for free.&lt;/p>
&lt;p>This feature will be enabled by default, and there will be 2 mechanisms in
place to disable it:&lt;/p>
&lt;ol>
&lt;li>Calling &lt;code>apache_beam.typehints.disable_type_annotations()&lt;/code> before pipeline
construction will disable the new feature completely.&lt;/li>
&lt;li>Decorating a function with &lt;code>@apache_beam.typehints.no_annotations&lt;/code> will
tell Beam to ignore annotations for it.&lt;/li>
&lt;/ol>
&lt;p>Uses of Beam&amp;rsquo;s &lt;code>with_input_type&lt;/code>, &lt;code>with_output_type&lt;/code> methods and decorators will
still work and take precedence over annotations.&lt;/p>
&lt;h3 id="sidebar">Sidebar&lt;/h3>
&lt;p>You might ask: couldn&amp;rsquo;t we use mypy to type check Beam pipelines?
There are several reasons why this is not the case.&lt;/p>
&lt;ul>
&lt;li>Pipelines are constructed at runtime and may depend on information that is
only known at that time, such as a config file or database table schema.&lt;/li>
&lt;li>PCollections don&amp;rsquo;t have the necessary type information, so mypy sees them as
effectively containing any element type.
This may change in the future.&lt;/li>
&lt;li>Transforms using lambdas (ex: &lt;code>beam.Map(lambda x: (1, x)&lt;/code>) cannot be
annotated properly using PEP 484.
However, Beam does a best-effort attempt to analyze the output type
from the bytecode.&lt;/li>
&lt;/ul>
&lt;h2 id="typing-module-support">Typing Module Support&lt;/h2>
&lt;p>Python&amp;rsquo;s &lt;a href="https://docs.python.org/3/library/typing.html">typing&lt;/a> module defines
types used in type annotations. This is what we call &amp;ldquo;native&amp;rdquo; types.
While Beam has its own typing types, it also supports native types.
While both Beam and native types are supported, for new code we encourage using
native typing types. Native types have as these are supported by additional tools.&lt;/p>
&lt;p>While working on Python 3 annotations syntax support, we&amp;rsquo;ve also discovered and
fixed issues with native type support. There may still be bugs and unsupported
native types. Please
&lt;a href="/community/contact-us/">let us know&lt;/a> if you encounter
issues.&lt;/p></description></item><item><title>Blog: Apache Beam 2.21.0</title><link>/blog/beam-2.21.0/</link><pubDate>Wed, 27 May 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.21.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.21.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2210-2020-05-27">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.21.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347143">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Python: Deprecated module &lt;code>apache_beam.io.gcp.datastore.v1&lt;/code> has been removed
as the client it uses is out of date and does not support Python 3
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9529">BEAM-9529&lt;/a>).
Please migrate your code to use
&lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.io.gcp.datastore.v1new.datastoreio.html">apache_beam.io.gcp.datastore.&lt;strong>v1new&lt;/strong>&lt;/a>.
See the updated
&lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/cookbook/datastore_wordcount.py">datastore_wordcount&lt;/a>
for example usage.&lt;/li>
&lt;li>Python SDK: Added integration tests and updated batch write functionality for Google Cloud Spanner transform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8949">BEAM-8949&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Python SDK will now use Python 3 type annotations as pipeline type hints.
(&lt;a href="https://github.com/apache/beam/pull/10717">#10717&lt;/a>)&lt;/p>
&lt;p>If you suspect that this feature is causing your pipeline to fail, calling
&lt;code>apache_beam.typehints.disable_type_annotations()&lt;/code> before pipeline creation
will disable is completely, and decorating specific functions (such as
&lt;code>process()&lt;/code>) with &lt;code>@apache_beam.typehints.no_annotations&lt;/code> will disable it
for that function.&lt;/p>
&lt;p>More details can be found in
&lt;a href="/documentation/sdks/python-type-safety/">Ensuring Python Type Safety&lt;/a>
and the Python SDK Typing Changes
&lt;a href="/blog/python-typing/">blog post&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Java SDK: Introducing the concept of options in Beam Schema’s. These options add extra
context to fields and schemas. This replaces the current Beam metadata that is present
in a FieldType only, options are available in fields and row schemas. Schema options are
fully typed and can contain complex rows. &lt;em>Remark: Schema aware is still experimental.&lt;/em>
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9035">BEAM-9035&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Java SDK: The protobuf extension is fully schema aware and also includes protobuf option
conversion to beam schema options. &lt;em>Remark: Schema aware is still experimental.&lt;/em>
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9044">BEAM-9044&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Added ability to write to BigQuery via Avro file loads (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8841">BEAM-8841&lt;/a>)&lt;/p>
&lt;p>By default, file loads will be done using JSON, but it is possible to
specify the temp_file_format parameter to perform file exports with AVRO.
AVRO-based file loads work by exporting Python types into Avro types, so
to switch to Avro-based loads, you will need to change your data types
from Json-compatible types (string-type dates and timestamp, long numeric
values as strings) into Python native types that are written to Avro
(Python&amp;rsquo;s date, datetime types, decimal, etc). For more information
see &lt;a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#avro_conversions">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#avro_conversions&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Added integration of Java SDK with Google Cloud AI VideoIntelligence service
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9147">BEAM-9147&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Added integration of Java SDK with Google Cloud AI natural language processing API
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9634">BEAM-9634&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>docker-pull-licenses&lt;/code> tag was introduced. Licenses/notices of third party dependencies will be added to the docker images when &lt;code>docker-pull-licenses&lt;/code> was set.
The files are added to &lt;code>/opt/apache/beam/third_party_licenses/&lt;/code>.
By default, no licenses/notices are added to the docker images. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9136">BEAM-9136&lt;/a>)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Dataflow runner now requires the &lt;code>--region&lt;/code> option to be set, unless a default value is set in the environment (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9199">BEAM-9199&lt;/a>). See &lt;a href="https://cloud.google.com/dataflow/docs/concepts/regional-endpoints">here&lt;/a> for more details.&lt;/li>
&lt;li>HBaseIO.ReadAll now requires a PCollection of HBaseIO.Read objects instead of HBaseQuery objects (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9279">BEAM-9279&lt;/a>).&lt;/li>
&lt;li>ProcessContext.updateWatermark has been removed in favor of using a WatermarkEstimator (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9430">BEAM-9430&lt;/a>).&lt;/li>
&lt;li>Coder inference for PCollection of Row objects has been disabled (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9569">BEAM-9569&lt;/a>).&lt;/li>
&lt;li>Go SDK docker images are no longer released until further notice.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Java SDK: Beam Schema FieldType.getMetadata is now deprecated and is replaced by the Beam
Schema Options, it will be removed in version &lt;code>2.23.0&lt;/code>. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9704">BEAM-9704&lt;/a>)&lt;/li>
&lt;li>The &lt;code>--zone&lt;/code> option in the Dataflow runner is now deprecated. Please use &lt;code>--worker_zone&lt;/code> instead. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9716">BEAM-9716&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.21.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aaron Meihm, Adrian Eka, Ahmet Altay, AldairCoronel, Alex Van Boxel, Alexey Romanenko, Andrew Crites, Andrew Pilloud, Ankur Goenka, Badrul (Taki) Chowdhury, Bartok Jozsef, Boyuan Zhang, Brian Hulette, brucearctor, bumblebee-coming, Chad Dombrova, Chamikara Jayalath, Chie Hayashida, Chris Gorgolewski, Chuck Yang, Colm O hEigeartaigh, Curtis &amp;ldquo;Fjord&amp;rdquo; Hawthorne, Daniel Mills, Daniel Oliveira, David Yan, Elias Djurfeldt, Emiliano Capoccia, Etienne Chauchot, Fernando Diaz, Filipe Regadas, Gleb Kanterov, Hai Lu, Hannah Jiang, Harch Vardhan, Heejong Lee, Henry Suryawirawan, Hk-tang, Ismaël Mejía, Jacoby, Jan Lukavský, Jeroen Van Goey, jfarr, Jozef Vilcek, Kai Jiang, Kamil Wasilewski, Kenneth Knowles, KevinGG, Kyle Weaver, Kyoungha Min, Luke Cwik, Maximilian Michels, Michal Walenia, Ning Kang, Pablo Estrada, paul fisher, Piotr Szuberski, Reuven Lax, Robert Bradshaw, Robert Burke, Rose Nguyen, Rui Wang, Sam Rohde, Sam Whittle, Spoorti Kundargi, Steve Koonce, sunjincheng121, Ted Yun, Tesio, Thomas Weise, Tomo Suzuki, Udi Meiri, Valentyn Tymofieiev, Vasu Nori, Yichi Zhang, yoshiki.obata, Yueyang Qiu&lt;/p></description></item></channel></rss>